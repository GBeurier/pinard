{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikeras Keras regressor integration\n",
    "\n",
    "The integration of KerasRegressor in pipeline is limited. To use full keras capabilities, you may need to modify the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard loading and preprocessing code\n",
    "\n",
    "from pinard import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from pinard import preprocessing as pp\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Concatenate,GlobalAveragePooling1D, Conv1D, Activation, SpatialDropout1D,SeparableConv1D,Flatten, Dropout, Input, MaxPooling1D, DepthwiseConv1D, BatchNormalization, AveragePooling1D\n",
    "from typing import Dict, Iterable, Any\n",
    "from tensorflow.keras.optimizers import Adam, Adadelta, SGD\n",
    "import models\n",
    "from tensorflow_addons.optimizers import CyclicalLearningRate, LazyAdam\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "import datetime\n",
    "from pinard.sklearn import FeatureAugmentation\n",
    "\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "# tf.keras.wrappers.scikit_learn.KerasRegressor\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "# Init basic random\n",
    "rd_seed = 45678\n",
    "np.random.seed(rd_seed)\n",
    "tf.random.set_seed(rd_seed)\n",
    "random.seed(rd_seed)\n",
    "\n",
    "preprocessing = [   ('id', pp.IdentityTransformer()),\n",
    "                    ('baseline', pp.StandardNormalVariate()),\n",
    "                    ('savgol', pp.SavitzkyGolay()),\n",
    "                    ('gaussian1', pp.Gaussian(order = 1, sigma = 2)),\n",
    "                    ('gaussian2', pp.Gaussian(order = 2, sigma = 1)),\n",
    "                    ('haar', pp.Wavelet('haar')),\n",
    "                    ('coif3', pp.Wavelet('coif3')),\n",
    "                    ('detrend', pp.Detrend()),\n",
    "                    ('msc', pp.MultiplicativeScatterCorrection(scale=False)),\n",
    "                    ('dv1', pp.Derivate(1,1)),\n",
    "                    ('dv2', pp.Derivate(2,1)),\n",
    "                    ('dv3', pp.Derivate(2,2)),\n",
    "                    \n",
    "                    ('baseline*savgol', Pipeline([('_sg1',pp.StandardNormalVariate()),('_sg2',pp.SavitzkyGolay())])),\n",
    "                    ('baseline*gaussian1', Pipeline([('_sg1',pp.StandardNormalVariate()),('g2', pp.Gaussian(order = 1, sigma = 2) )])),\n",
    "                    ('baseline*gaussian2', Pipeline([('_sg1',pp.StandardNormalVariate()),('g2', pp.Gaussian(order = 2, sigma = 1) )])),\n",
    "                    ('baseline*haar', Pipeline([('_sg1',pp.StandardNormalVariate()),('_sg2',pp.Wavelet('haar'))])),\n",
    "                    \n",
    "                    # ('savgol*savgol', Pipeline([('_sg1',pp.SavitzkyGolay()),('_sg2',pp.SavitzkyGolay())])),\n",
    "                    # ('savgol*baseline', Pipeline([('_sg1',pp.SavitzkyGolay()),('_sg2',pp.StandardNormalVariate())])),\n",
    "                    ('savgol*gaussian1', Pipeline([('_sg1',pp.SavitzkyGolay()),('g2', pp.Gaussian(order = 1, sigma = 2) )])),\n",
    "                    ('savgol*gaussian2', Pipeline([('_sg1',pp.SavitzkyGolay()),('g2', pp.Gaussian(order = 2, sigma = 1) )])),\n",
    "                    # ('savgol*haar', Pipeline([('_sg1',pp.SavitzkyGolay()), ('haar', pp.Wavelet('haar'))])),\n",
    "                    # ('gaussian1*savgol', Pipeline([('_g1',pp.Gaussian(order = 1, sigma = 2)),('_sg3',pp.SavitzkyGolay())])),\n",
    "                    ('gaussian2*savgol', Pipeline([('_g2',pp.Gaussian(order = 1, sigma = 2)),('_sg4',pp.SavitzkyGolay())])),\n",
    "                    # ('haar*savgol', Pipeline([('_haar2',pp.Wavelet('haar')),('_sg5',pp.SavitzkyGolay())])),\n",
    "                    # ('haar*gaussian1', Pipeline([('_haar2',pp.Wavelet('haar')), ('g2', pp.Gaussian(order = 1, sigma = 2)) ])),\n",
    "                    ('haar*gaussian2', Pipeline([('_haar2',pp.Wavelet('haar')), ('g2', pp.Gaussian(order = 2, sigma = 1)) ])),\n",
    "                ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# custom Keras Callback that store \n",
    "class Auto_Save(Callback):\n",
    "    best_weights = []\n",
    "    def __init__(self):\n",
    "        super(Auto_Save, self).__init__()\n",
    "        self.best = np.Inf\n",
    "                \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_loss = logs.get('val_loss')\n",
    "        if np.less(current_loss, self.best):\n",
    "            self.best = current_loss            \n",
    "            Auto_Save.best_weights = self.model.get_weights()\n",
    "            print(\"Best so far >\", self.best)\n",
    "            \n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.params['verbose'] == 2:\n",
    "            print('\\nSaved best {0:6.4f}\\n'.format(self.best))\n",
    "\n",
    "\n",
    "class Print_LR(Callback):    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        iteration = self.model.optimizer.iterations.numpy()\n",
    "        # lr = clr(iteration).numpy()\n",
    "        lr = self.model.optimizer.learning_rate\n",
    "        if self.params['verbose'] == 2:\n",
    "            print(\"Iteration {} - Learning rate: {}\".format(iteration, lr) )\n",
    "\n",
    "# def cyclic_optimizer(X_train, BATCH_SIZE, p):\n",
    "#     MIN_LR, MAX_LR, CYCLE_LENGTH = p['MIN_LR'], p['MAX_LR'], p['CYCLE_LENGTH']\n",
    "#     steps_per_epoch = len(X_train) // BATCH_SIZE\n",
    "\n",
    "#     clr = CyclicalLearningRate(\n",
    "#         initial_learning_rate=MIN_LR,\n",
    "#         maximal_learning_rate=MAX_LR,\n",
    "#         scale_fn=lambda x: 1/(2.**(x-1)),\n",
    "#         step_size= CYCLE_LENGTH * steps_per_epoch\n",
    "#     )\n",
    "#     return Adam(clr)\n",
    "\n",
    "\n",
    "def keras_model(meta):\n",
    "    # print(input_shape)\n",
    "    input_shape = meta[\"X_shape_\"][1:]\n",
    "    # print(\"---\", meta[\"X_shape_\"], input_shape)\n",
    "    # input_shape = shape[1:]\n",
    "    # optimizer = meta[\"opt\"]\n",
    "    # model = models.xception(input_shape)\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(DepthwiseConv1D(kernel_size=7, padding=\"same\",depth_multiplier=2, activation='relu'))\n",
    "    model.add(DepthwiseConv1D(kernel_size=7, padding=\"same\",depth_multiplier=2, activation='relu'))\n",
    "    model.add(AveragePooling1D(pool_size=2,strides=2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(DepthwiseConv1D(kernel_size=5, padding=\"same\",depth_multiplier=2, activation='relu'))\n",
    "    model.add(DepthwiseConv1D(kernel_size=5, padding=\"same\",depth_multiplier=2, activation='relu'))\n",
    "    model.add(AveragePooling1D(pool_size=2,strides=2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(DepthwiseConv1D(kernel_size=9, padding=\"same\",depth_multiplier=2, activation='relu'))\n",
    "    model.add(DepthwiseConv1D(kernel_size=9, padding=\"same\",depth_multiplier=2, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2,strides=2))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(SeparableConv1D(128, kernel_size=5, depth_multiplier=1, padding=\"same\", activation='relu'))\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, padding=\"same\"))\n",
    "    model.add(MaxPooling1D(pool_size=5,strides=3))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=128, activation=\"sigmoid\"))\n",
    "    model.add(Dense(units=32, activation=\"sigmoid\"))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "    # # optimizer = Adadelta(learning_rate=1.0, rho=0.95, epsilon=1e-07)\n",
    "    # model.compile(loss = 'mean_squared_error', metrics=['mse'], optimizer = optimizer)\n",
    "    # model.compile(loss = 'mean_squared_error', metrics=['mse'], optimizer = \"adam\")\n",
    "    # model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(Xcal, Ycal, Xval, Yval, BATCH_SIZE, EPOCH, cycle_params):  \n",
    "    X_train, y_train = utils.load(Xcal, Ycal, y_cols=1)\n",
    "    X_test, y_test = utils.load(Xval, Yval, y_cols=1)\n",
    "    \n",
    "    y_scaler = MinMaxScaler()\n",
    "    y_scaler.fit(y_train.reshape((-1,1)))\n",
    "    y_valid = y_scaler.transform(y_test.reshape((-1,1)))\n",
    "  \n",
    "    transformer_pipeline = Pipeline([\n",
    "        ('scaler', MinMaxScaler()), \n",
    "        ('preprocessing', FeatureAugmentation(preprocessing)), \n",
    "    ])\n",
    "    \n",
    "    transformer_pipeline.fit(X_train)\n",
    "    X_valid = transformer_pipeline.transform(X_test)\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=400, verbose=0, mode='min') \n",
    "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    # reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.75, patience=100, verbose=1, min_delta=0.5e-5, mode='min')\n",
    "\n",
    "    def scale_fn(x):\n",
    "        return 1 / (2.0 ** (x - 1))\n",
    "        # return 1. ** x\n",
    "        \n",
    "    MIN_LR, MAX_LR, CYCLE_LENGTH = cycle_params['MIN_LR'], cycle_params['MAX_LR'], cycle_params['CYCLE_LENGTH']\n",
    "    def clr(epoch, lr):\n",
    "        initial_learning_rate = MIN_LR\n",
    "        maximal_learning_rate = MAX_LR\n",
    "        step_size = CYCLE_LENGTH\n",
    "        step_as_dtype = float(epoch)\n",
    "        cycle = math.floor(1 + step_as_dtype / (2 * step_size))\n",
    "        x = abs(step_as_dtype / step_size - 2 * cycle + 1)\n",
    "        mode_step = cycle # if scale_mode == \"cycle\" else step\n",
    "        return initial_learning_rate + ( maximal_learning_rate - initial_learning_rate) * max(0, (1 - x)) * scale_fn(mode_step)\n",
    "\n",
    "    # clr = CyclicalLearningRate(\n",
    "    #     initial_learning_rate=MIN_LR,\n",
    "    #     maximal_learning_rate=MAX_LR,\n",
    "    #     scale_fn=scale_fn,\n",
    "    #     step_size= CYCLE_LENGTH * steps_per_epoch\n",
    "    # )\n",
    "    \n",
    "    # kerasModel = keras_model((len(X_train),len(preprocessing)))\n",
    "    # # print(\"---\", X_train.shape)\n",
    "    # kerasModel.compile(loss = 'mean_squared_error', metrics=['mse'], optimizer = LazyAdam(0.001))\n",
    "    \n",
    "    lrScheduler = tf.keras.callbacks.LearningRateScheduler(clr)\n",
    "    \n",
    "    k_regressor = KerasRegressor(model = keras_model,\n",
    "                                loss = 'mean_squared_error', metrics=['mse'], optimizer = \"adam\",\n",
    "                                callbacks=[Auto_Save(), early_stop, lrScheduler, tensorboard_callback],\n",
    "                                epochs=EPOCH,\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                # scale_fn=scale_fn,\n",
    "                                fit__validation_data = (X_valid, y_valid),\n",
    "                                verbose = 2)\n",
    "\n",
    "    # estimation pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('trans', transformer_pipeline), \n",
    "        ('KerasNN', k_regressor)\n",
    "    ])\n",
    "\n",
    "    estimator = TransformedTargetRegressor(regressor = pipeline, transformer = y_scaler)\n",
    "    estimator.fit(X_train, y_train)\n",
    "\n",
    "    estimator.regressor_[1].model_.set_weights(Auto_Save.best_weights)\n",
    "\n",
    "    Y_preds = estimator.predict(X_test)\n",
    "    RMSE = math.sqrt(mean_squared_error(y_test, Y_preds))\n",
    "    print(\"MAE\", mean_absolute_error(y_test, Y_preds))\n",
    "    print(\"MSE\", mean_squared_error(y_test, Y_preds))\n",
    "    print(\"RMSE\", RMSE)\n",
    "    print(\"MAPE\", mean_absolute_percentage_error(y_test, Y_preds))\n",
    "    print(\"R²\", r2_score(y_test, Y_preds))\n",
    "    \n",
    "    return y_test, Y_preds, RMSE, estimator.regressor_[1].model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import glob\n",
    "rootdir = Path('data')\n",
    "file_list = [f for f in rootdir.glob('**/*') if f.is_dir()]\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 10000\n",
    "\n",
    "cycle_params = {\n",
    "    'MIN_LR': 1e-5,\n",
    "    'MAX_LR': 1e-2,\n",
    "    'CYCLE_LENGTH': 256    \n",
    "}\n",
    "\n",
    "for dir in file_list:\n",
    "    training_name = str(dir)\n",
    "    projdir = Path(dir)\n",
    "    Xcal = next(projdir.glob(\"*Xcal*\"))\n",
    "    Ycal = next(projdir.glob(\"*Ycal*\"))\n",
    "    Xval = next(projdir.glob(\"*Xval*\"))\n",
    "    Yval = next(projdir.glob(\"*Yval*\"))\n",
    "    \n",
    "    print(\"*\"*15)\n",
    "    print(str(dir))\n",
    "    print(\"-\"*15)    \n",
    "    \n",
    "    y_test, Y_preds, RMSE, model = train_model(Xcal, Ycal, Xval, Yval, BATCH_SIZE, EPOCHS, cycle_params)\n",
    "\n",
    "    np.savetxt(training_name + 'res'+str(RMSE)+'.csv', np.column_stack((y_test,Y_preds)))\n",
    "    model.save(training_name + '_model')\n",
    "    \n",
    "    np.savetxt(training_name +'/res'+str(RMSE)+'.csv', np.column_stack((y_test,Y_preds)))\n",
    "    model.save(training_name + '/model')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('pynirsENV': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b09f6e5407ec4329146609a0cb08cbbe4720f97bb26598a93c421b663bd10d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING sample_data\\ALPINE_C_424_Murguzur\n",
      "mean 0.2502768\n",
      "std 0.22829306\n",
      "median 0.18485376\n",
      "min -0.104465626\n",
      "max 1.3915194\n",
      "percentile [0.06791676 0.40602568]\n",
      "linalg 298.51172\n",
      "polyfit [1.3960305  0.46635726]\n",
      "polyfit [[1.3631628  1.3960305  1.36012011 ... 1.1916488  1.19263494 1.19377185]\n",
      " [0.46054249 0.46635726 0.48146435 ... 0.1880057  0.187743   0.18729798]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10924/194806486.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sample_data'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;31m# with open(\"scores.json\", \"w\") as write_file:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;31m#     json.dump(scores, write_file, indent=4)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10924/194806486.py\u001b[0m in \u001b[0;36mtraverse\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m    191\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LOADING\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m         \u001b[1;31m# score = benchmark_folder(x,y,f)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;31m# scores[f] = score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "import json\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "sys.path.append(os.path.abspath(\"\") + \"/../src/pynirs\")\n",
    "from nirs_set import NIRS_Set as NSet\n",
    "import preprocessor as pp\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Conv1D, Flatten, BatchNormalization, SpatialDropout1D, Dropout\n",
    "# from tensorflow.keras.optimizers import SGD\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "# from kennard_stone import train_test_split, KFold\n",
    "# from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from matplotlib.pyplot import plot\n",
    "\n",
    "SEED = 12485\n",
    "def set_seed(sd):\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['PYTHONHASHSEED']=str(sd)\n",
    "    random.seed(sd)\n",
    "    np.random.seed(sd)\n",
    "    tf.random.set_seed(sd)\n",
    "set_seed(SEED)\n",
    "\n",
    "hello = tf.constant(\"hello TensorFlow!\")\n",
    "\n",
    "def load_path(folder):\n",
    "    n = NSet(\"example\")\n",
    "    n.load(folder+\"/XCal.csv\", folder+\"/YCal.csv\", 0, 0, 0)\n",
    "    x_src = n.get_raw_x()\n",
    "\n",
    "    print(\"mean\", np.mean(x_src))\n",
    "    print(\"std\", np.std(x_src))\n",
    "    print(\"median\", np.median(x_src))\n",
    "    print(\"min\", np.min(x_src))\n",
    "    print(\"max\", np.max(x_src))\n",
    "    print(\"percentile\", np.percentile(x_src,[20, 80]))\n",
    "    print(\"linalg\", np.linalg.norm(x_src))\n",
    "    reference = np.mean(x_src, axis = 1)\n",
    "    print(\"polyfit\", np.polyfit(reference, x_src[:, 0], deg = 1))\n",
    "    p = np.polyfit(reference, x_src, deg = 1)\n",
    "    print(\"polyfit\", p)\n",
    "    print(\"polyfit\", p.shape)\n",
    "    return\n",
    "    x = x_src.copy()\n",
    "    # PROCESSING = ['x','x*detrend','x*snv','x*savgol1','x*msc','x*derivate','x*gaussian1','x*gaussian2','x*wv_haar','x*detrend*snv','x*detrend*rnv','x*detrend*savgol1','x*detrend*msc','x*detrend*derivate','x*detrend*wv_haar','x*wv_bior2.2','x*wv_db2','x*wv_dmey','x*wv_rbio6.8','x*wv_sym4','x*snv*savgol1','x*savgol1*savgol1','x*msc*savgol1','x*derivate*savgol1','x*gaussian1*savgol1','x*gaussian2*savgol1','x*wv_haar*savgol1','x*snv*msc','x*rnv*msc','x*savgol1*msc','x*msc*msc','x*derivate*msc','x*gaussian1*msc','x*gaussian2*msc','x*wv_haar*msc','x*snv*derivate','x*savgol1*derivate','x*msc*derivate','x*derivate*derivate','x*gaussian1*derivate','x*gaussian2*derivate','x*wv_haar*derivate']\n",
    "    PROCESSING = ['x'\n",
    "                # ,'x*detrend'\n",
    "                ,'x*savgol1'\n",
    "                # ,'x*derivate'\n",
    "                ,'x*gaussian1'\n",
    "                ,'x*gaussian2'\n",
    "                ,'x*wv_haar'\n",
    "                # ,'x*detrend*savgol1'\n",
    "                # ,'x*detrend*derivate'\n",
    "                # ,'x*detrend*wv_haar'\n",
    "                # # ,'x*detrend*gaussian1'\n",
    "                # # ,'x*detrend*gaussian2'\n",
    "                # ,'x*wv_bior2.2'\n",
    "                # ,'x*wv_db2'\n",
    "                # ,'x*wv_dmey'\n",
    "                # ,'x*wv_rbio6.8'\n",
    "                # ,'x*wv_sym4'\n",
    "                # ,'x*savgol1*savgol1'\n",
    "                # ,'x*derivate*savgol1'\n",
    "                # ,'x*gaussian1*savgol1'\n",
    "                # ,'x*gaussian2*savgol1'\n",
    "                # ,'x*wv_haar*savgol1'\n",
    "                # ,'x*savgol1*derivate'\n",
    "                # ,'x*derivate*derivate'\n",
    "                # ,'x*gaussian1*derivate'\n",
    "                # ,'x*gaussian2*derivate'\n",
    "                # ,'x*wv_haar*derivate'\n",
    "                ]\n",
    "    pp_spectra = pp.process(x, PROCESSING)\n",
    "    x = np.array(list(pp_spectra.values()))\n",
    "    x = np.swapaxes(x, 0, 1)\n",
    "    x = np.swapaxes(x, 1, 2)\n",
    "    print(x[0,:,0])\n",
    "    print(x[0,:,1])\n",
    "    print(x[0,:,2])\n",
    "    \n",
    "    x = x_src.copy()\n",
    "    pp_spectra = pp.process(x[0:2], PROCESSING)\n",
    "    x = np.array(list(pp_spectra.values()))\n",
    "    x = np.swapaxes(x, 0, 1)\n",
    "    x = np.swapaxes(x, 1, 2)\n",
    "    print(x[0,:,0])\n",
    "    print(x[0,:,1])\n",
    "    print(x[0,:,2])\n",
    "    \n",
    "    y_src = n.get_raw_y()\n",
    "    scaler_y = MinMaxScaler(feature_range=(0.1,0.9))\n",
    "    y = scaler_y.fit_transform(y_src)\n",
    "    \n",
    "    return x, y, scaler_y\n",
    "\n",
    "# def learn(X_train, X_test, y_train, y_test, input_shape, name, folder):\n",
    "#     model = Sequential()\n",
    "#     model.add(SpatialDropout1D(0.08, input_shape=input_shape))\n",
    "#     model.add(Conv1D (filters=8, kernel_size=15, strides=5, activation='selu'))\n",
    "#     # model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.3))\n",
    "#     model.add(Conv1D (filters=64, kernel_size=21, strides=3, activation='relu'))\n",
    "#     # model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Conv1D (filters=32, kernel_size=5, strides=3, activation='elu'))\n",
    "#     # model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(16, activation='sigmoid'))\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#     # model.summary()\n",
    "\n",
    "#     reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=50, verbose=1, min_delta=0.5e-5, mode='min')\n",
    "#     earlyStopping = EarlyStopping(monitor='val_loss', patience=90, verbose=0, mode='min') \n",
    "#     mcp_save = ModelCheckpoint(\"tmp.model\", save_best_only=True, monitor='val_loss', mode='min') \n",
    "\n",
    "#     model.compile(loss='mean_squared_error', metrics=['mae','mse'], optimizer='rmsprop')\n",
    "\n",
    "#     history = model.fit(X_train, y_train, \n",
    "#                 epochs=600, \n",
    "#                 batch_size=500, \n",
    "#                 shuffle=True, \n",
    "#                 validation_data = (X_test, y_test),\n",
    "#                 verbose=0, \n",
    "#                 callbacks=[earlyStopping])\n",
    "\n",
    "#     best_score = min(history.history['val_mse'])\n",
    "#     best_epoch = np.argmin(history.history['val_mse'])\n",
    "#     print(folder, input_shape, name, best_score, best_epoch)\n",
    "#     return best_score\n",
    "    \n",
    "    \n",
    "# def benchmark_folder(x,y,folder):\n",
    "#     print(\"***** BENCHMARKING\", folder, \"*****\")\n",
    "#     s = x.shape\n",
    "    \n",
    "#     # X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.3)\n",
    "    \n",
    "#     scores = {}\n",
    "    \n",
    "#     kf = KFold(n_splits=4)#, random_state=SEED, shuffle=True)\n",
    "#     fold_index = 0\n",
    "#     # for train_index, test_index in kf.split(x): #sklearn split\n",
    "#     for train_index, test_index in kf.split(x[:,:,0]): #kennard stone split\n",
    "        \n",
    "#         X_train, X_test = x[train_index], x[test_index]\n",
    "#         y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "#         score = learn(X_train, X_test, y_train, y_test, (s[1], s[2]), fold_index, folder)\n",
    "#         name = str(fold_index)\n",
    "#         scores[name] = score\n",
    "            \n",
    "#         # X_train_src = X_train[:,:,0:2]\n",
    "#         # X_test_src = X_test[:,:,0:2]\n",
    "        \n",
    "#         # for i in range(15, s[2], 15):\n",
    "#         #     X_train_sub = X_train[:,:,0:i]\n",
    "#         #     X_test_sub = X_test[:,:,0:i]\n",
    "#         #     score = learn(X_train_sub, X_test_sub, y_train, y_test, (s[1], i), str(i), folder)\n",
    "#         #     name = str(fold_index) + '>0-' + str(i)\n",
    "#         #     scores[name] = score\n",
    "            \n",
    "#         # for i in range(15, s[2], 15):\n",
    "#         #     X_train_sub = X_train[:,:,i-15:i]\n",
    "#         #     X_train_sub = np.concatenate((X_train_src, X_train_sub), axis = 2)\n",
    "#         #     X_test_sub = X_test[:,:,i-15:i]\n",
    "#         #     X_test_sub = np.concatenate((X_test_src, X_test_sub), axis = 2)\n",
    "#         #     score = learn(X_train_sub, X_test_sub, y_train, y_test, (s[1], 17), str(i), folder)\n",
    "#         #     name = str(fold_index) + '>' + str(i-15) + \"-\" + str(i)\n",
    "#         #     scores[name] = score\n",
    "            \n",
    "#         fold_index += 1\n",
    "        \n",
    "#     return scores\n",
    "    \n",
    "    \n",
    "def traverse(directory):\n",
    "    scores = {}\n",
    "    folders = [x[0] for x in os.walk(directory)]\n",
    "    for f in folders:\n",
    "        if f == directory:\n",
    "            continue\n",
    "        print(\"LOADING\", f)\n",
    "        x,y,_ = load_path(f)\n",
    "        # score = benchmark_folder(x,y,f)\n",
    "        # scores[f] = score\n",
    "        break\n",
    "    return scores\n",
    "\n",
    "scores = traverse('sample_data')\n",
    "# with open(\"scores.json\", \"w\") as write_file:\n",
    "#     json.dump(scores, write_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a027d9ccaa4161f7bb2a6671f63bb76a78cb94a27161911cf16ac7661ccf92de"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('pynirsENV': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implementation:\n",
    "\n",
    "# Loading\n",
    "# - Load raw data from csv files\n",
    "# - Filter rows and columns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TODO Data\n",
    "\n",
    "### load csv raw (train, test)\n",
    "# crop / resample data\n",
    "# scale data\n",
    "\n",
    "# load csv filter (train[-idx], train[idx])\n",
    "# load test / train / valid multiple sources\n",
    "# split data\n",
    "# apply transformation on data\n",
    "# apply aggregation on data\n",
    "# augment data\n",
    "# fit model\n",
    "# predict model\n",
    "# evaluate model\n",
    "# save model\n",
    "# load model\n",
    "# reporting\n",
    "# logging\n",
    "# aggregate models (ensemble, stacking, mean)\n",
    "# documentation\n",
    "# Include automatic NA management\n",
    "# TODO >>>>>> Enhance and refactor csvLoader for headers and multiple headers detection\n",
    "# Add more format for transformer instanciation\n",
    "# generate a dataset id dependant of data to avoid hash different with same data but different path - replace hashcode after loading ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def browse_folder_and_generate_txt(root_folder, extension, output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as out_file:\n",
    "        for foldername, subfolders, filenames in os.walk(root_folder):\n",
    "            for filename in filenames:\n",
    "                if filename.endswith(extension):\n",
    "                    file_path = os.path.join(foldername, filename)\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                            content = file.read()\n",
    "                        # Write file path and content to output file\n",
    "                        out_file.write(f\"File: {file_path}\\n\")\n",
    "                        out_file.write(\"\\n\" + \"-\"*10 + \"\\n\")\n",
    "                        out_file.write(content + \"\\n\")\n",
    "                        out_file.write(\"\\n\" + \"=\"*80 + \"\\n\\n\")  # Separator\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "\n",
    "root_folder = \"\"\"D:/Workspace/ML/NIRS/pinard/pinard/core\"\"\"\n",
    "extension = \"py\"\n",
    "output_file = \"core_code.txt\"\n",
    "\n",
    "browse_folder_and_generate_txt(root_folder, extension, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_files_from_txt(txt_file_path):\n",
    "    with open(txt_file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    current_file_path = None\n",
    "    current_code_chunk = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('# '):\n",
    "            if current_file_path and current_code_chunk:\n",
    "                # if current_file_path is a path with folder, create folders if they don't exist\n",
    "                if '/' in current_file_path:\n",
    "                    os.makedirs(os.path.dirname(current_file_path), exist_ok=True)\n",
    "                \n",
    "                # Write the current chunk to the current file\n",
    "                with open(current_file_path, 'a', encoding='utf-8') as code_file:\n",
    "                    code_file.write(''.join(current_code_chunk))\n",
    "                current_code_chunk = []\n",
    "            \n",
    "            # Update the current file path\n",
    "            current_file_path = line[2:].strip()\n",
    "        else:\n",
    "            current_code_chunk.append(line)\n",
    "    \n",
    "    # Write the last chunk to the last file\n",
    "    if current_file_path and current_code_chunk:\n",
    "        with open(current_file_path, 'a', encoding='utf-8') as code_file:\n",
    "            code_file.write(''.join(current_code_chunk))\n",
    "\n",
    "# Example usage\n",
    "txt_file_path = \"test_code.txt\"  # Use the existing variable\n",
    "generate_files_from_txt(txt_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Define custom transformers for sample and feature augmentation\n",
    "\n",
    "\n",
    "class AddOneTransformer(TransformerMixin, BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X + 1\n",
    "\n",
    "\n",
    "class DivTwoTransformer(TransformerMixin, BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X / 2\n",
    "\n",
    "\n",
    "class AddThreeTransformer(TransformerMixin, BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X + 3\n",
    "\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 2, 3], [10, 20, 30], [100, 200, 300]])\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Define the pipeline\n",
    "pipeline = [\n",
    "    MinMaxScaler(),\n",
    "    # {'sample_augmentation': [None, AddOneTransformer(), DivTwoTransformer()]},\n",
    "    # {'feature_augmentation': [None, AddThreeTransformer()]},\n",
    "]\n",
    "\n",
    "# Create the executor and execute the pipeline\n",
    "executor = PipelineExecutor(pipeline)\n",
    "executor.execute(X)\n",
    "\n",
    "# Retrieve data with default aggregation\n",
    "data = executor.get_data()\n",
    "print(\"Concatenated Data:\")\n",
    "print(data)\n",
    "\n",
    "# Retrieve data with 'union' aggregation\n",
    "data_union = executor.get_data(aggregation='union')\n",
    "print(\"\\nUnion Aggregated Data:\")\n",
    "print(data_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import gzip\n",
    "import zipfile\n",
    "from typing import List\n",
    "from collections import Counter\n",
    "\n",
    "def read_csv_first_lines(file_path: str) -> List[str]:\n",
    "    lines = []\n",
    "    \n",
    "    if file_path.endswith('.gz'):\n",
    "        with gzip.open(file_path, mode='rt', newline='', encoding='utf-8') as f:\n",
    "            reader = f.readlines()\n",
    "            for i, row in enumerate(reader):\n",
    "                lines.append(row.strip())\n",
    "                if i >= 4:  # We only want the first 5 lines\n",
    "                    break\n",
    "    elif file_path.endswith('.zip'):\n",
    "        with zipfile.ZipFile(file_path, 'r') as zip_file:\n",
    "            # Assuming there is only one CSV file in the zip\n",
    "            csv_filename = zip_file.namelist()[0]\n",
    "            with zip_file.open(csv_filename) as f:\n",
    "                reader = f.read().decode('utf-8').splitlines()\n",
    "                for i, row in enumerate(reader):\n",
    "                    lines.append(row.strip())\n",
    "                    if i >= 4:\n",
    "                        break\n",
    "    elif file_path.endswith('.csv'):\n",
    "        with open(file_path, mode='r', newline='', encoding='utf-8') as f:\n",
    "            reader = f.readlines()\n",
    "            for i, row in enumerate(reader):\n",
    "                lines.append(row.strip())\n",
    "                if i >= 4:\n",
    "                    break\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format\")\n",
    "    \n",
    "    return lines\n",
    "\n",
    "def detect_delimiter(lines: List[str]) -> str:\n",
    "    delimiters = [',', ';', '\\t', '|']\n",
    "    delimiter_counts = {d: [] for d in delimiters}\n",
    "\n",
    "    for line in lines:\n",
    "        for delimiter in delimiters:\n",
    "            delimiter_counts[delimiter].append(line.count(delimiter))\n",
    "\n",
    "    most_likely_delimiter = None\n",
    "    for delimiter, counts in delimiter_counts.items():\n",
    "        if len(set(counts)) == 1:  # Same count across all lines\n",
    "            if most_likely_delimiter is None or sum(counts) > sum(delimiter_counts[most_likely_delimiter]):\n",
    "                most_likely_delimiter = delimiter\n",
    "\n",
    "    if most_likely_delimiter is None:\n",
    "        raise ValueError(\"Could not detect a consistent delimiter\")\n",
    "    \n",
    "    return most_likely_delimiter\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    csv_path = \"example.csv\"  # Change to your file path\n",
    "    first_lines = read_csv_first_lines(csv_path)\n",
    "    delimiter = detect_delimiter(first_lines)\n",
    "    print(f\"Detected delimiter: {delimiter}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import gzip\n",
    "import zipfile\n",
    "from typing import List, Union\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def read_csv_first_lines(file_path: str, n_lines: int) -> List[str]:\n",
    "    lines = []\n",
    "\n",
    "    if file_path.endswith('.gz'):\n",
    "        with gzip.open(file_path, mode='rt', newline='', encoding='utf-8') as f:\n",
    "            reader = f.readlines()\n",
    "            for i, row in enumerate(reader):\n",
    "                lines.append(row.strip())\n",
    "                if i >= n_lines:  # We only want the first 5 lines\n",
    "                    break\n",
    "    elif file_path.endswith('.zip'):\n",
    "        with zipfile.ZipFile(file_path, 'r') as zip_file:\n",
    "            # Assuming there is only one CSV file in the zip\n",
    "            csv_filename = zip_file.namelist()[0]\n",
    "            with zip_file.open(csv_filename) as f:\n",
    "                reader = f.read().decode('utf-8').splitlines()\n",
    "                for i, row in enumerate(reader):\n",
    "                    lines.append(row.strip())\n",
    "                    if i >= n_lines:\n",
    "                        break\n",
    "    elif file_path.endswith('.csv'):\n",
    "        with open(file_path, mode='r', newline='', encoding='utf-8') as f:\n",
    "            reader = f.readlines()\n",
    "            for i, row in enumerate(reader):\n",
    "                lines.append(row.strip())\n",
    "                if i >= n_lines:\n",
    "                    break\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format\")\n",
    "\n",
    "    return lines\n",
    "\n",
    "\n",
    "def detect_delimiter(lines: List[str], numeric_delimiter: str) -> str:\n",
    "    delimiters = [';', '\\t', '|', ',']\n",
    "    if numeric_delimiter in delimiters:\n",
    "        delimiters.remove(numeric_delimiter)\n",
    "\n",
    "    delimiter_counts = {d: [] for d in delimiters}\n",
    "\n",
    "    for line in lines:\n",
    "        for delimiter in delimiters:\n",
    "            delimiter_counts[delimiter].append(line.count(delimiter))\n",
    "\n",
    "    most_likely_delimiter = None\n",
    "    for delimiter, counts in delimiter_counts.items():\n",
    "        if len(set(counts)) == 1:  # Same count across all lines\n",
    "            # print(delimiter, sum(counts), sum(delimiter_counts[most_likely_delimiter]) if most_likely_delimiter is not None else None)\n",
    "            if sum(counts) > 0:\n",
    "                if most_likely_delimiter is None or sum(counts) > sum(delimiter_counts[most_likely_delimiter]):\n",
    "                    most_likely_delimiter = delimiter\n",
    "\n",
    "    return most_likely_delimiter\n",
    "\n",
    "\n",
    "def detect_numeric_delimiter(lines):\n",
    "    numeric_delimiters = ['.', ',']\n",
    "    numeric_delimiter_counts = {numeric_delimiter: 0 for numeric_delimiter in numeric_delimiters}\n",
    "\n",
    "    for line in lines:\n",
    "        for numeric_delimiter in numeric_delimiters:\n",
    "            numeric_delimiter_counts[numeric_delimiter] += len(re.findall(re.escape(numeric_delimiter), line))\n",
    "\n",
    "    detected_numeric_delimiter = max(numeric_delimiter_counts, key=numeric_delimiter_counts.get)\n",
    "    \n",
    "    if numeric_delimiter_counts[detected_numeric_delimiter] == 0:\n",
    "        detected_numeric_delimiter = None\n",
    "\n",
    "    return detected_numeric_delimiter\n",
    "\n",
    "\n",
    "def is_float(value: str) -> bool:\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def is_int(value: str) -> bool:\n",
    "    try:\n",
    "        int(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def is_bool(value: str) -> bool:\n",
    "    return value.lower() in ['true', 'false']\n",
    "\n",
    "\n",
    "def count_fragments_by_type(line: Union[str, List[str]], separator: str) -> dict:\n",
    "    fragment_types = {'float': 0, 'int': 0, 'str': 0, 'bool': 0}\n",
    "    fragment_types_list = get_fragment_types(line, separator)\n",
    "    for fragment_type in fragment_types_list:\n",
    "        fragment_types[fragment_type] += 1\n",
    "    \n",
    "    return fragment_types\n",
    "\n",
    "\n",
    "def get_fragment_types(line: Union[str, List[str]], separator: str) -> List[str]:\n",
    "    fragments = []\n",
    "    if isinstance(line, list):\n",
    "        fragments = line\n",
    "    else:\n",
    "        fragments = line.split(separator)\n",
    "    \n",
    "    fragment_types = []\n",
    "    for fragment in fragments:\n",
    "        fragment = fragment.strip()\n",
    "        if is_int(fragment):\n",
    "            fragment_types.append('int')\n",
    "        elif is_float(fragment):\n",
    "            fragment_types.append('float')\n",
    "        elif is_bool(fragment):\n",
    "            fragment_types.append('bool')\n",
    "        else:\n",
    "            fragment_types.append('str')\n",
    "    return fragment_types\n",
    "\n",
    "\n",
    "def detect_column_header(lines: List[str], delimiter: str) -> bool:\n",
    "    columns = list(zip(*[line.split(delimiter) for line in lines]))\n",
    "    last_types = None\n",
    "    current_types = None\n",
    "    for i, column in reversed(list(enumerate(columns))):\n",
    "        column = list(column)\n",
    "        if len(column) < 4:\n",
    "            raise ValueError(\"Error: Too few lines to detect header\")\n",
    "        column = column[3:] # remove potential horizontal header\n",
    "        types = get_fragment_types(column, delimiter)\n",
    "        \n",
    "        \n",
    "    #     if types.count('float') + types.count('int') == len(types):\n",
    "    #         return False\n",
    "    # return True\n",
    "    \n",
    "    print(np.array(columns).shape)\n",
    "\n",
    "# def detect_horizontal_header(lines: List[str], delimiter: str) -> bool:\n",
    "#     types_of_lines = {tuple(sorted(count_fragments_by_type(line, delimiter).items())) for line in lines}\n",
    "    \n",
    "#     if len(types_of_lines) > 2:\n",
    "#         print(types_of_lines)\n",
    "#         return \"Error: Too many lines to detect header\"\n",
    "#     elif len(types_of_lines) == 1:\n",
    "#         return False\n",
    "#     else:\n",
    "#         return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "def process_folders_and_detect_delimiters(folder_paths: List[str]):\n",
    "    for folder_path in folder_paths:\n",
    "        for root, _, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                if file.endswith('.csv') or file.endswith('.gz') or file.endswith('.zip'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    print(file_path)\n",
    "                    try:\n",
    "                        first_lines = read_csv_first_lines(file_path, n_lines=15)\n",
    "                        numeric_delimiter = detect_numeric_delimiter(first_lines)\n",
    "                        delimiter = detect_delimiter(first_lines, numeric_delimiter)\n",
    "                        # first_line_types = count_fragments_by_type(first_lines[0], delimiter)\n",
    "                        # second_line_types = count_fragments_by_type(first_lines[1], delimiter)\n",
    "                        # horizontal_header = detect_horizontal_header(first_lines, delimiter)\n",
    "                        detect_column_header(first_lines, delimiter)\n",
    "                        # print(f\"File: {file_path}, Numeric: {numeric_delimiter}, Delimiter: {delimiter}, Horizontal Header: {horizontal_header}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {file_path}: {e}\")\n",
    "                return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "folder_paths = ['FOOD_CHEM_lost']\n",
    "process_folders_and_detect_delimiters(folder_paths)\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     csv_path = \"example1.csv\"  # Change to your file path\n",
    "#     first_lines = read_csv_first_lines(csv_path)\n",
    "#     delimiter = detect_delimiter(first_lines)\n",
    "#     print(f\"Detected delimiter: {delimiter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No .epub files in: Z:/light_novels/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def check_subfolders_for_epub(root_folder):\n",
    "    for foldername, subfolders, filenames in os.walk(root_folder):\n",
    "        has_epub = any(filename.endswith('.epub') for filename in filenames)\n",
    "        if not has_epub:\n",
    "            print(f\"No .epub files in: {foldername}\")\n",
    "\n",
    "# Example usage\n",
    "check_subfolders_for_epub(\"Z:/light_novels/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def copy_and_reorganize_files(source_path, new_source_path):\n",
    "    for root, dirs, files in os.walk(source_path):\n",
    "        for file in files:\n",
    "            # Get the relative path from the source_path\n",
    "            relative_path = os.path.relpath(root, source_path)\n",
    "            # Split the relative path into parts\n",
    "            parts = relative_path.split(os.sep)\n",
    "            if len(parts) >= 2:\n",
    "                author = parts[0]\n",
    "                serie = parts[1]\n",
    "                # Create the new directory structure\n",
    "                new_dir = os.path.join(new_source_path, f\"{author}_{serie}\")\n",
    "                os.makedirs(new_dir, exist_ok=True)\n",
    "                # Copy the file to the new directory\n",
    "                shutil.copy(os.path.join(root, file), os.path.join(new_dir, file))\n",
    "\n",
    "\n",
    "# Example usage\n",
    "source_path = \"D:/calibre_sorted\"\n",
    "new_source_path = \"D:/calibre_cpy\"\n",
    "copy_and_reorganize_files(source_path, new_source_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

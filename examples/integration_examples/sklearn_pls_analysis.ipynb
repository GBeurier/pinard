{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit-learn PLS Analysis with Pinard\n",
    "\n",
    "This notebook demonstrates how to perform a complete PLS regression analysis using Pinard with scikit-learn. The workflow includes:\n",
    "\n",
    "- Loading data\n",
    "- Preprocessing with multiple transformations\n",
    "- Training a PLS model\n",
    "- Making predictions and evaluating performance\n",
    "- Saving the model for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "from pinard import utils\n",
    "from pinard import preprocessing as pp\n",
    "from pinard.model_selection import train_test_split_idx\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (using example data files)\n",
    "try:\n",
    "    x_path = \"../Xcal.csv\"  # Adjust path if necessary\n",
    "    y_path = \"../Ycal.csv\"  # Adjust path if necessary\n",
    "    x, y = utils.load_csv(x_path, y_path, x_hdr=0, y_hdr=0, autoremove_na=True)\n",
    "    print(f\"Loaded data: X shape: {x.shape}, y shape: {y.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load CSV files: {e}\")\n",
    "    print(\"Generating random data instead...\")\n",
    "    # Generate synthetic data if files not available\n",
    "    x = np.random.rand(100, 200)  # 100 samples, 200 features\n",
    "    y = np.random.rand(100)      # Target values\n",
    "    print(f\"Generated data: X shape: {x.shape}, y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick data exploration\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot first sample\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x[0])\n",
    "plt.title(\"First Sample\")\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "\n",
    "# Plot target distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(y, bins=20)\n",
    "plt.title(\"Target Distribution\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data Into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "train_index, test_index = train_test_split_idx(x, y=y, method=\"random\", test_size=0.25, random_state=42)\n",
    "X_train, y_train = x[train_index], y[train_index]\n",
    "X_test, y_test = x[test_index], y[test_index]\n",
    "\n",
    "print(f\"Training set: X shape: {X_train.shape}, y shape: {y_train.shape}\")\n",
    "print(f\"Test set: X shape: {X_test.shape}, y shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Preprocessing Steps\n",
    "\n",
    "We will apply multiple preprocessing transformations to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing operators\n",
    "preprocessing = [\n",
    "    ('id', pp.IdentityTransformer()),    # Keep original data\n",
    "    ('savgol', pp.SavitzkyGolay()),      # Savitzky-Golay smoothing\n",
    "    ('gaussian1', pp.Gaussian(order=1, sigma=2)),  # Gaussian filtering\n",
    "]\n",
    "\n",
    "print(f\"Number of preprocessing operators: {len(preprocessing)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of preprocessing on a sample\n",
    "sample_idx = 0  # Use first sample for visualization\n",
    "sample = X_train[sample_idx:sample_idx+1]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Original data\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(sample[0])\n",
    "plt.title(\"Original Data\")\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "\n",
    "# Processed data for each operator\n",
    "for i, (name, transformer) in enumerate(preprocessing):\n",
    "    plt.subplot(2, 2, i+2)\n",
    "    processed = transformer.fit_transform(sample)\n",
    "    plt.plot(processed[0])\n",
    "    plt.title(f\"{name} Transformed\")\n",
    "    plt.xlabel(\"Feature Index\")\n",
    "    plt.ylabel(\"Value\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Train PLS Pipeline\n",
    "\n",
    "We will create a scikit-learn pipeline with the following steps:\n",
    "1. Feature scaling with MinMaxScaler\n",
    "2. Preprocessing with FeatureUnion (applies transformations in parallel)\n",
    "3. PLS regression model\n",
    "4. Target scaling (via TransformedTargetRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with FeatureUnion for preprocessing and PLS regression\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),          # Scale input features to [0,1]\n",
    "    ('preprocessing', FeatureUnion(preprocessing)),  # Apply preprocessing operators\n",
    "    ('pls', PLSRegression(n_components=10))  # PLS regression with 10 components\n",
    "])\n",
    "\n",
    "# Wrap the pipeline in TransformedTargetRegressor to scale the target variable\n",
    "estimator = TransformedTargetRegressor(\n",
    "    regressor=pipeline,\n",
    "    transformer=MinMaxScaler()  # Scale target variable to [0,1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "%time estimator.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions and Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = estimator.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Model performance metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot predictions vs actual\n",
    "plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('PLS Model: Actual vs Predicted')\n",
    "\n",
    "# Add R² value to plot\n",
    "plt.text(0.05, 0.95, f'R² = {r2:.4f}', transform=plt.gca().transAxes, \n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate PLS Components\n",
    "\n",
    "Let's look at how many components are optimal for our PLS model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance with different numbers of components\n",
    "max_components = min(20, min(X_train.shape))\n",
    "component_range = range(1, max_components + 1)\n",
    "r2_scores = []\n",
    "\n",
    "for n_components in component_range:\n",
    "    # Create a new pipeline with the specified number of components\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('preprocessing', FeatureUnion(preprocessing)),\n",
    "        ('pls', PLSRegression(n_components=n_components))\n",
    "    ])\n",
    "    \n",
    "    model = TransformedTargetRegressor(\n",
    "        regressor=pipeline,\n",
    "        transformer=MinMaxScaler()\n",
    "    )\n",
    "    \n",
    "    # Train and evaluate\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    r2_scores.append(r2)\n",
    "    \n",
    "    print(f\"Components: {n_components}, R²: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot R² vs number of components\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(component_range, r2_scores, 'o-')\n",
    "plt.xlabel('Number of PLS Components')\n",
    "plt.ylabel('R² Score')\n",
    "plt.title('Model Performance vs Number of PLS Components')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Find and highlight the optimal number of components\n",
    "optimal_components = component_range[np.argmax(r2_scores)]\n",
    "max_r2 = max(r2_scores)\n",
    "plt.axvline(x=optimal_components, color='r', linestyle='--', alpha=0.7)\n",
    "plt.scatter([optimal_components], [max_r2], color='r', s=100, zorder=5)\n",
    "plt.text(optimal_components + 0.5, max_r2, f'Optimal: {optimal_components} components\\nR²: {max_r2:.4f}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model\n",
    "\n",
    "Save the trained model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Train the final model with optimal number of components\n",
    "final_pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('preprocessing', FeatureUnion(preprocessing)),\n",
    "    ('pls', PLSRegression(n_components=optimal_components))\n",
    "])\n",
    "\n",
    "final_model = TransformedTargetRegressor(\n",
    "    regressor=final_pipeline,\n",
    "    transformer=MinMaxScaler()\n",
    ")\n",
    "\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(final_model, 'pls_model.joblib')\n",
    "print(\"Model saved to 'pls_model.joblib'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the Saved Model\n",
    "\n",
    "To ensure the saved model works correctly, let's reload it and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "loaded_model = joblib.load('pls_model.joblib')\n",
    "\n",
    "# Make predictions with the loaded model\n",
    "y_pred_loaded = loaded_model.predict(X_test)\n",
    "\n",
    "# Verify the predictions\n",
    "r2_loaded = r2_score(y_test, y_pred_loaded)\n",
    "print(f\"Loaded model R² score: {r2_loaded:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated a complete workflow for building a PLS regression model with Pinard and scikit-learn. We covered:\n",
    "\n",
    "1. Data loading and preprocessing with multiple transformations\n",
    "2. Building a pipeline with feature scaling and PLS regression\n",
    "3. Training and evaluating the model\n",
    "4. Optimizing the number of PLS components\n",
    "5. Saving and reloading the model\n",
    "\n",
    "The approach we used enables reproducible analysis and can be adapted to different types of spectral data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit-learn PLS Analysis with Pinard\n",
    "\n",
    "This notebook demonstrates how to perform a complete PLS regression analysis using Pinard with scikit-learn. The workflow includes:\n",
    "\n",
    "- Loading data\n",
    "- Preprocessing with multiple transformations\n",
    "- Training a PLS model\n",
    "- Making predictions and evaluating performance\n",
    "- Saving the model for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "from pinard import utils\n",
    "from pinard import preprocessing as pp\n",
    "from pinard.model_selection import train_test_split_idx\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (using example data files)\n",
    "try:\n",
    "    x_path = \"../Xcal.csv\"  # Adjust path if necessary\n",
    "    y_path = \"../Ycal.csv\"  # Adjust path if necessary\n",
    "    x, y = utils.load_csv(x_path, y_path, x_hdr=0, y_hdr=0, autoremove_na=True)\n",
    "    print(f\"Loaded data: X shape: {x.shape}, y shape: {y.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load CSV files: {e}\")\n",
    "    print(\"Generating random data instead...\")\n",
    "    # Generate synthetic data if files not available\n",
    "    x = np.random.rand(100, 200)  # 100 samples, 200 features\n",
    "    y = np.random.rand(100)      # Target values\n",
    "    print(f\"Generated data: X shape: {x.shape}, y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick data exploration\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot first sample\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x[0])\n",
    "plt.title(\"First Sample\")\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "\n",
    "# Plot target distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(y, bins=20)\n",
    "plt.title(\"Target Distribution\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data Into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "train_index, test_index = train_test_split_idx(x, y=y, method=\"random\", test_size=0.25, random_state=42)\n",
    "X_train, y_train = x[train_index], y[train_index]\n",
    "X_test, y_test = x[test_index], y[test_index]\n",
    "\n",
    "print(f\"Training set: X shape: {X_train.shape}, y shape: {y_train.shape}\")\n",
    "print(f\"Test set: X shape: {X_test.shape}, y shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Preprocessing Steps\n",
    "\n",
    "We will apply multiple preprocessing transformations to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing operators\n",
    "preprocessing = [\n",
    "    ('id', pp.IdentityTransformer()),    # Keep original data\n",
    "    ('savgol', pp.SavitzkyGolay()),      # Savitzky-Golay smoothing\n",
    "    ('gaussian1', pp.Gaussian(order=1, sigma=2)),  # Gaussian filtering\n",
    "]\n",
    "\n",
    "print(f\"Number of preprocessing operators: {len(preprocessing)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of preprocessing on a sample\n",
    "sample_idx = 0  # Use first sample for visualization\n",
    "sample = X_train[sample_idx:sample_idx+1]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Original data\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(sample[0])\n",
    "plt.title(\"Original Data\")\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "\n",
    "# Processed data for each operator\n",
    "for i, (name, transformer) in enumerate(preprocessing):\n",
    "    plt.subplot(2, 2, i+2)\n",
    "    processed = transformer.fit_transform(sample)\n",
    "    plt.plot(processed[0])\n",
    "    plt.title(f\"{name} Transformed\")\n",
    "    plt.xlabel(\"Feature Index\")\n",
    "    plt.ylabel(\"Value\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Train PLS Pipeline\n",
    "\n",
    "We will create a scikit-learn pipeline with the following steps:\n",
    "1. Feature scaling with MinMaxScaler\n",
    "2. Preprocessing with FeatureUnion (applies transformations in parallel)\n",
    "3. PLS regression model\n",
    "4. Target scaling (via TransformedTargetRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with FeatureUnion for preprocessing and PLS regression\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),          # Scale input features to [0,1]\n",
    "    ('preprocessing', FeatureUnion(preprocessing)),  # Apply preprocessing operators\n",
    "    ('pls', PLSRegression(n_components=10))  # PLS regression with 10 components\n",
    "])\n",
    "\n",
    "# Wrap the pipeline in TransformedTargetRegressor to scale the target variable\n",
    "estimator = TransformedTargetRegressor(\n",
    "    regressor=pipeline,\n",
    "    transformer=MinMaxScaler()  # Scale target variable to [0,1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "%time estimator.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions and Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = estimator.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Model performance metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot predictions vs actual\n",
    "plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('PLS Model: Actual vs Predicted')\n",
    "\n",
    "# Add R² value to plot\n",
    "plt.text(0.05, 0.95, f'R² = {r2:.4f}', transform=plt.gca().transAxes, \n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate PLS Components\n",
    "\n",
    "Let's look at how many components are optimal for our PLS model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance with different numbers of components\n",
    "max_components = min(20, min(X_train.shape))\n",
    "component_range = range(1, max_components + 1)\n",
    "r2_scores = []\n",
    "\n",
    "for n_components in component_range:\n",
    "    # Create a new pipeline with the specified number of components\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('preprocessing', FeatureUnion(preprocessing)),\n",
    "        ('pls', PLSRegression(n_components=n_components))\n",
    "    ])\n",
    "    \n",
    "    model = TransformedTargetRegressor(\n",
    "        regressor=pipeline,\n",
    "        transformer=MinMaxScaler()\n",
    "    )\n",
    "    \n",
    "    # Train and evaluate\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    r2_scores.append(r2)\n",
    "    \n",
    "    print(f\"Components: {n_components}, R²: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot R² vs number of components\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(component_range, r2_scores, 'o-')\n",
    "plt.xlabel('Number of PLS Components')\n",
    "plt.ylabel('R² Score')\n",
    "plt.title('Model Performance vs Number of PLS Components')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Find and highlight the optimal number of components\n",
    "optimal_components = component_range[np.argmax(r2_scores)]\n",
    "max_r2 = max(r2_scores)\n",
    "plt.axvline(x=optimal_components, color='r', linestyle='--', alpha=0.7)\n",
    "plt.scatter([optimal_components], [max_r2], color='r', s=100, zorder=5)\n",
    "plt.text(optimal_components + 0.5, max_r2, f'Optimal: {optimal_components} components\\nR²: {max_r2:.4f}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model\n",
    "\n",
    "Save the trained model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Train the final model with optimal number of components\n",
    "final_pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('preprocessing', FeatureUnion(preprocessing)),\n",
    "    ('pls', PLSRegression(n_components=optimal_components))\n",
    "])\n",
    "\n",
    "final_model = TransformedTargetRegressor(\n",
    "    regressor=final_pipeline,\n",
    "    transformer=MinMaxScaler()\n",
    ")\n",
    "\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(final_model, 'pls_model.joblib')\n",
    "print(\"Model saved to 'pls_model.joblib'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the Saved Model\n",
    "\n",
    "To ensure the saved model works correctly, let's reload it and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "loaded_model = joblib.load('pls_model.joblib')\n",
    "\n",
    "# Make predictions with the loaded model\n",
    "y_pred_loaded = loaded_model.predict(X_test)\n",
    "\n",
    "# Verify the predictions\n",
    "r2_loaded = r2_score(y_test, y_pred_loaded)\n",
    "print(f\"Loaded model R² score: {r2_loaded:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated a complete workflow for building a PLS regression model with Pinard and scikit-learn. We covered:\n",
    "\n",
    "1. Data loading and preprocessing with multiple transformations\n",
    "2. Building a pipeline with feature scaling and PLS regression\n",
    "3. Training and evaluating the model\n",
    "4. Optimizing the number of PLS components\n",
    "5. Saving and reloading the model\n",
    "\n",
    "The approach we used enables reproducible analysis and can be adapted to different types of spectral data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

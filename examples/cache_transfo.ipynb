{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache.py\n",
    "\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from typing import Optional, Union, List, Dict, Any\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import dataclasses\n",
    "\n",
    "\n",
    "class AddVal(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, val):\n",
    "        self.val = val\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X + self.val\n",
    "    \n",
    "data = np.array([ [1, 2, 3, 4], [10, 20, 30, 40], [100, 200, 300, 400] ], dtype=np.float32)\n",
    "pipeline = [AddVal(1), {\"s\": [None, AddVal(0.1), AddVal(0.01)]}, {\"f\": [None, AddVal(0.5)]}, [AddVal(1000), AddVal(10000)]]\n",
    "pipeline = [AddVal(0), {\"s\": [None, AddVal(1)]}, {\"f\": [None, AddVal(0.5)]}]\n",
    "\n",
    "def run_pipeline(pipeline, data):\n",
    "    # id = register_data(data)\n",
    "    data = data.reshape(1, data.shape[0], 1, data.shape[1])    \n",
    "    print(data.shape)\n",
    "    return exec_step(pipeline, data)        \n",
    "    \n",
    "\n",
    "def exec_pipeline(pipeline, data):\n",
    "    for step in pipeline:\n",
    "        data = exec_step(step, data)\n",
    "    return data\n",
    "\n",
    "def exec_step(step, data):\n",
    "    n_augmentations = data.shape[0]\n",
    "    n_samples = data.shape[1]\n",
    "    n_transformations = data.shape[2]\n",
    "    n_features = data.shape[3]\n",
    "    \n",
    "    if isinstance(step, dict):\n",
    "        if 's' in step: \n",
    "            # Sample augmentation\n",
    "            sample_transformers = step['s']\n",
    "            data = np.repeat(data, len(sample_transformers), axis=0)\n",
    "            for (i, st) in enumerate(sample_transformers): # should be parallel execution\n",
    "                aug_range = range(i * n_augmentations, (i + 1) * n_augmentations)\n",
    "                aug_data = data[aug_range, :, :, :]\n",
    "                aug_data = exec_step(st, aug_data)\n",
    "                data[aug_range, :, :, :] = aug_data\n",
    "            return data\n",
    "        \n",
    "        elif 'f' in step: \n",
    "            # Feature augmentation\n",
    "            feature_transformers = step['f']\n",
    "            data = np.repeat(data, len(feature_transformers), axis=2)\n",
    "            for (i, ft) in enumerate(feature_transformers): # should be parallel execution\n",
    "                tr_range = range(i * n_transformations, (i + 1) * n_transformations)\n",
    "                tr_data = data[:, :, tr_range, :]\n",
    "                tr_data = exec_step(ft, tr_data)\n",
    "                data[:, :, tr_range, :] = tr_data\n",
    "            return data\n",
    "        return None\n",
    "    \n",
    "    elif isinstance(step, list):\n",
    "        # Sequential execution\n",
    "        for s in step: \n",
    "            data = exec_step(s, data)\n",
    "        return data\n",
    "    \n",
    "    elif isinstance(step, TransformerMixin):\n",
    "        # Transformer execution (to apply to every transformation dimension)\n",
    "        for tr in range(n_transformations): # should be parallel execution\n",
    "            dataview = data[:, :, tr, :].reshape(n_augmentations * n_samples, n_features)\n",
    "            new_data = dataview if step is None else step.fit_transform(dataview)\n",
    "            data[:, :, tr, :] = new_data.reshape(n_augmentations, n_samples, n_features)\n",
    "        return data\n",
    "    \n",
    "    elif step is None:\n",
    "        # Identity transformation\n",
    "        return data\n",
    "    \n",
    "    return None\n",
    "\n",
    "def filter_data(data, bool_mask=None, union_type=None):\n",
    "    n_augmentations = data.shape[0]\n",
    "    n_samples = data.shape[1]\n",
    "    n_transformations = data.shape[2]\n",
    "    n_features = data.shape[3]\n",
    "    \n",
    "    if bool_mask is not None:\n",
    "        data = data[:, bool_mask, :, :]\n",
    "        n_samples = data.shape[1]\n",
    "    \n",
    "    if union_type is None:\n",
    "        return data\n",
    "    \n",
    "    total_samples = n_augmentations * n_samples\n",
    "    if union_type  == 'concat': \n",
    "        # concat features - (n_transformations, n_features) > [ [1, 2, 3, 4], [10, 20, 30, 40] ] -> [1, 2, 3, 4, 10, 20, 30, 40]\n",
    "        return data.reshape(total_samples, n_transformations * n_features)\n",
    "    elif union_type == 'interlaced': \n",
    "        # interlace features - (n_transformations, n_features) > [ [1, 2, 3, 4], [10, 20, 30, 40] ] -> [1, 10, 2, 20, 3, 30, 4, 40]\n",
    "        data = data.reshape(total_samples, n_transformations, n_features)\n",
    "        return data.reshape(total_samples, n_transformations * n_features, order='F')\n",
    "    elif union_type == 'union': \n",
    "        # fold augmentation axis\n",
    "        return data.reshape(total_samples, n_transformations, n_features)\n",
    "    elif union_type == 'transpose_union': \n",
    "        # fold augmentation axis and transpose transformation and feature axis\n",
    "        return np.transpose(data.reshape(total_samples, n_transformations, n_features), (0, 2, 1))\n",
    "\n",
    "    return None\n",
    "\n",
    "print(data.shape)\n",
    "print(data, \"\\n\", \"x\" * 50)\n",
    "data = run_pipeline(pipeline, data)\n",
    "print(data.shape)\n",
    "print(data)\n",
    "print(\"*\" * 50)\n",
    "\n",
    "## test filter_data function\n",
    "print(filter_data(data, bool_mask=[0, 2]))\n",
    "print(\"=\" * 50)\n",
    "print(filter_data(data, bool_mask=[0, 2], union_type='concat'))\n",
    "print(\"=\" * 50)\n",
    "print(filter_data(data, union_type='interlaced'))\n",
    "print(\"=\" * 50)\n",
    "print(filter_data(data, union_type='union'))\n",
    "print(\"=\" * 50)\n",
    "print(filter_data(data, union_type='transpose_union'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "class PipelineRunner:\n",
    "    def __init__(self, pipeline, cache_enabled=True, fit_on_train_only=True):\n",
    "        self.pipeline = pipeline\n",
    "        self.cache_enabled = cache_enabled\n",
    "        self.fit_on_train_only = fit_on_train_only\n",
    "        self.cache = {}\n",
    "\n",
    "    def run_pipeline(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            train_data, test_data = data\n",
    "        else:\n",
    "            train_data = data\n",
    "            test_data = None\n",
    "\n",
    "        combined_data = (train_data, test_data) if test_data is not None else (train_data,)\n",
    "        data_shape = train_data.shape\n",
    "        # Reshape data to match expected dimensions\n",
    "        reshaped_data = [d.reshape(1, d.shape[0], 1, d.shape[1]) for d in combined_data]\n",
    "\n",
    "        # Execute the pipeline\n",
    "        transformed_data = self.exec_pipeline(self.pipeline, reshaped_data)\n",
    "\n",
    "        # Reshape back to original dimensions\n",
    "        transformed_data = [d.reshape(-1, d.shape[-1]) for d in transformed_data]\n",
    "        return tuple(transformed_data) if test_data is not None else transformed_data[0]\n",
    "\n",
    "    def exec_pipeline(self, pipeline, data_list):\n",
    "        return self.exec_step(pipeline, data_list)\n",
    "\n",
    "    def exec_step(self, step, data_list):\n",
    "        if isinstance(step, dict):\n",
    "            if 's' in step:\n",
    "                # Sample augmentation\n",
    "                sample_transformers = step['s']\n",
    "                augmented_data_list = []\n",
    "                for st in sample_transformers:\n",
    "                    transformed = [self.exec_step(st, [data.copy() for data in data_list]) for data in data_list]\n",
    "                    augmented_data_list.extend(transformed)\n",
    "                return augmented_data_list\n",
    "\n",
    "            elif 'f' in step:\n",
    "                # Feature augmentation\n",
    "                feature_transformers = step['f']\n",
    "\n",
    "                def transform_features(ft):\n",
    "                    return [self.exec_step(ft, [data.copy() for data in data_list]) for data in data_list]\n",
    "\n",
    "                transformed_data = Parallel(n_jobs=-1)(\n",
    "                    delayed(transform_features)(ft) for ft in feature_transformers\n",
    "                )\n",
    "                # Flatten the list of lists\n",
    "                transformed_data = [item for sublist in transformed_data for item in sublist]\n",
    "                return transformed_data\n",
    "\n",
    "            else:\n",
    "                raise Exception(f\"Unknown augmentation key in dict: {step.keys()}\")\n",
    "\n",
    "        elif isinstance(step, list):\n",
    "            # Sequential execution\n",
    "            for s in step:\n",
    "                data_list = self.exec_step(s, data_list)\n",
    "            return data_list\n",
    "\n",
    "        elif isinstance(step, TransformerMixin):\n",
    "            # Transformer execution\n",
    "            transformed_data_list = []\n",
    "            for idx, data in enumerate(data_list):\n",
    "                n_augmentations, n_samples, n_transformations, n_features = data.shape\n",
    "\n",
    "                # We combine augmentations and samples for processing\n",
    "                data_view = data.reshape(-1, n_features)\n",
    "\n",
    "                cache_key = self.get_cache_key(step, idx)\n",
    "                if self.cache_enabled and cache_key in self.cache:\n",
    "                    transformed_data = self.cache[cache_key]\n",
    "                else:\n",
    "                    # Fit on train data if specified\n",
    "                    if self.fit_on_train_only and idx == 0:\n",
    "                        step.fit(data_view)\n",
    "                    elif not self.fit_on_train_only:\n",
    "                        step.fit(data_view)\n",
    "\n",
    "                    transformed_data = step.transform(data_view)\n",
    "                    if self.cache_enabled:\n",
    "                        self.cache[cache_key] = transformed_data\n",
    "\n",
    "                # Reshape back to original dimensions\n",
    "                transformed_data = transformed_data.reshape(n_augmentations, n_samples, n_transformations, n_features)\n",
    "                transformed_data_list.append(transformed_data)\n",
    "            return transformed_data_list\n",
    "\n",
    "        elif step is None:\n",
    "            return data_list\n",
    "\n",
    "        else:\n",
    "            raise Exception(f\"Unknown step type: {type(step)}\")\n",
    "\n",
    "    def get_cache_key(self, transformer, data_idx):\n",
    "        transformer_id = id(transformer)\n",
    "        return (transformer_id, data_idx)\n",
    "\n",
    "    def filter_data(self, data, bool_mask=None, union_type=None):\n",
    "        n_augmentations, n_samples, n_transformations, n_features = data.shape\n",
    "\n",
    "        if bool_mask is not None:\n",
    "            data = data[:, bool_mask, :, :]\n",
    "            n_samples = data.shape[1]\n",
    "\n",
    "        total_samples = n_augmentations * n_samples\n",
    "\n",
    "        if union_type is None:\n",
    "            return data\n",
    "\n",
    "        if union_type == 'concat':\n",
    "            return data.reshape(total_samples, n_transformations * n_features)\n",
    "        elif union_type == 'interlaced':\n",
    "            data = data.reshape(total_samples, n_transformations, n_features)\n",
    "            return data.reshape(total_samples, n_transformations * n_features, order='F')\n",
    "        elif union_type == 'union':\n",
    "            return data.reshape(total_samples, n_transformations, n_features)\n",
    "        elif union_type == 'transpose_union':\n",
    "            return np.transpose(data.reshape(total_samples, n_transformations, n_features), (0, 2, 1))\n",
    "        else:\n",
    "            raise Exception(f\"Unknown union_type: {union_type}\")\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "class AddVal(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, val):\n",
    "        self.val = val\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X + self.val\n",
    "\n",
    "\n",
    "data = np.array([[1, 2, 3, 4],\n",
    "                 [10, 20, 30, 40],\n",
    "                 [100, 200, 300, 400]], dtype=np.float32)\n",
    "\n",
    "pipeline = [AddVal(0), {\"s\": [None, AddVal(1)]}, {\"f\": [None, AddVal(0.5)]}]\n",
    "\n",
    "runner = PipelineRunner(pipeline)\n",
    "train_data, test_data = data, data * 2\n",
    "train_transformed, test_transformed = runner.run_pipeline((train_data, test_data))\n",
    "\n",
    "print(\"Transformed Train Data:\")\n",
    "print(train_transformed)\n",
    "print(\"Transformed Test Data:\")\n",
    "print(test_transformed)\n",
    "\n",
    "# test filter_data function\n",
    "print(filter_data(data, bool_mask=[0, 2]))\n",
    "print(\"=\" * 50)\n",
    "print(filter_data(data, bool_mask=[0, 2], union_type='concat'))\n",
    "print(\"=\" * 50)\n",
    "print(filter_data(data, union_type='interlaced'))\n",
    "print(\"=\" * 50)\n",
    "print(filter_data(data, union_type='union'))\n",
    "print(\"=\" * 50)\n",
    "print(filter_data(data, union_type='transpose_union'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5487874  0.83054995 0.85328039 0.34156538 0.68640189]\n",
      " [0.75376423 0.9629305  0.23135579 0.05982465 0.93923489]\n",
      " [0.67724546 0.88052452 0.19532868 0.06138371 0.61079279]\n",
      " [0.56105231 0.51670443 0.61890682 0.57356218 0.55995812]\n",
      " [0.50246966 0.18248516 0.05080848 0.70851576 0.57797089]\n",
      " [0.5573045  0.24025563 0.2526094  0.40955923 0.41291195]\n",
      " [0.98278545 0.4290053  0.29444606 0.26173855 0.39284563]\n",
      " [0.09153474 0.4609441  0.09106405 0.63149356 0.33590656]\n",
      " [0.28339143 0.64299079 0.31539358 0.08798257 0.92695799]\n",
      " [0.64796579 0.0352152  0.74557884 0.28477654 0.34293417]] (10, 5)\n",
      "[[[[0.5487874  0.83054995 0.85328039 0.34156538 0.68640189]]\n",
      "\n",
      "  [[0.75376423 0.9629305  0.23135579 0.05982465 0.93923489]]\n",
      "\n",
      "  [[0.67724546 0.88052452 0.19532868 0.06138371 0.61079279]]\n",
      "\n",
      "  [[0.56105231 0.51670443 0.61890682 0.57356218 0.55995812]]\n",
      "\n",
      "  [[0.50246966 0.18248516 0.05080848 0.70851576 0.57797089]]\n",
      "\n",
      "  [[0.5573045  0.24025563 0.2526094  0.40955923 0.41291195]]\n",
      "\n",
      "  [[0.98278545 0.4290053  0.29444606 0.26173855 0.39284563]]\n",
      "\n",
      "  [[0.09153474 0.4609441  0.09106405 0.63149356 0.33590656]]\n",
      "\n",
      "  [[0.28339143 0.64299079 0.31539358 0.08798257 0.92695799]]\n",
      "\n",
      "  [[0.64796579 0.0352152  0.74557884 0.28477654 0.34293417]]]] (1, 10, 1, 5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "random_2D_array = np.random.rand(10, 5)\n",
    "expanded_4d_array = random_2D_array[np.newaxis, :, np.newaxis, :]\n",
    "print(random_2D_array, random_2D_array.shape)\n",
    "print(expanded_4d_array, expanded_4d_array.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

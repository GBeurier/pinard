{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Importation des modules\n",
    "\n",
    "import math_mod as mm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import plotting as pltg\n",
    "import sampling_techniques as st\n",
    "import seaborn as sns\n",
    "import typing as tp\n",
    "import utilitaire as ut\n",
    "\n",
    "from pinard import nirs_set as n_set\n",
    "from pinard import preprocessor as pp\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les préprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Préprocessing à effectuer pour les données spectrals\n",
    "\n",
    "preprocessing = [   \n",
    "                    ('id', pp.IdentityTransformer()),\n",
    "                    ('savgol', pp.SavitzkyGolay()),\n",
    "                    ('gaussian1', pp.Gaussian(order = 1, sigma = 2)),\n",
    "                    ('gaussian2', pp.Gaussian(order = 2, sigma = 1)),\n",
    "                    ('haar', pp.Wavelet('haar')),\n",
    "                    ('savgol*savgol', Pipeline([('_sg1',pp.SavitzkyGolay()),('_sg2',pp.SavitzkyGolay())])),\n",
    "                    ('gaussian1*savgol', Pipeline([('_g1',pp.Gaussian(order = 1, sigma = 2)),('_sg3',pp.SavitzkyGolay())])),\n",
    "                    ('gaussian2*savgol', Pipeline([('_g2',pp.Gaussian(order = 1, sigma = 2)),('_sg4',pp.SavitzkyGolay())])),\n",
    "                    ('haar*savgol', Pipeline([('_haar2',pp.Wavelet('haar')),('_sg5',pp.SavitzkyGolay())]))\n",
    "                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Fonction pour supprimer l'affichage des warnings sklearn\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Fonction transformant des ndarray en dataframes, et applique (au choix) un préprocessing, un retrait des outliers, et un MinMaxScaler\n",
    "\n",
    "def to_dataframe(features: tp.Union[pd.DataFrame, np.ndarray, list],\n",
    "                 labels: tp.Union[pd.DataFrame, np.ndarray, list],\n",
    "                 pre_pro: bool=True,\n",
    "                 scale: bool=True,\n",
    "                 rm_outliers: bool=True,\n",
    "                 random_state: tp.Union[int, None]=0) -> tp.Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \n",
    "    # Transforme les features en dataframe, et le reshape, si besoin\n",
    "     \n",
    "    if not isinstance(features, pd.DataFrame):\n",
    "\n",
    "        if len(features.shape) == 1:\n",
    "            \n",
    "            features = features.reshape(-1, 1)\n",
    "\n",
    "        features = pd.DataFrame(data=features, columns=[i for i in range(0, features.shape[1], 1)])\n",
    "\n",
    "    # Transforme les labels en dataframe, et le reshape, si besoin\n",
    "\n",
    "    if not isinstance(labels, pd.DataFrame):\n",
    "\n",
    "        if len(labels.shape) == 1:\n",
    "            \n",
    "            labels = labels.reshape(-1, 1)\n",
    "                        \n",
    "        labels = pd.DataFrame(data=labels, columns=[\"labels {}\".format(i) for i in range(0, labels.shape[1], 1)])\n",
    "        \n",
    "    # Retire les outliers\n",
    "\n",
    "    if rm_outliers:\n",
    "\n",
    "        iso_f = IsolationForest(random_state=random_state)\n",
    "        outliers = iso_f.fit_predict(features)\n",
    "\n",
    "        features = features[outliers != -1]\n",
    "        labels = labels[outliers != -1]\n",
    "\n",
    "    # Effectue un préprocessing sur les features\n",
    "\n",
    "    if pre_pro:\n",
    "\n",
    "        p = FeatureUnion(preprocessing).fit(features)\n",
    "        features = pd.DataFrame(data=p.transform(features), columns=[i for i in range(0, p.transform(features).shape[1], 1)])\n",
    "    \n",
    "    # Effectue un scale sur les labels\n",
    "\n",
    "    if scale:\n",
    "\n",
    "        sclr = MinMaxScaler().fit(labels)\n",
    "        labels = pd.DataFrame(data=sclr.transform(labels), columns=sclr.get_feature_names_out())\n",
    "    \n",
    "    return (features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Fonction permettant d'afficher les dimensions des jeux de données d'entraînement et de test \n",
    "\n",
    "def print_shape(a: pd.DataFrame,\n",
    "                b: pd.DataFrame,\n",
    "                c: pd.DataFrame,\n",
    "                d: pd.DataFrame) -> None:\n",
    "\n",
    "    print(\"x_train : {}\\ny_train : {}\\nx_test : {}\\ny_test : {}\\n\".format(a.shape, b.shape, c.shape, d.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Fonction affichant un violinplot de la distribution des valeurs de la variable cible, entre le train_set et le test_set\n",
    "\n",
    "def plot_distrib(y_train: pd.DataFrame, y_test: pd.DataFrame, largeur: int=10, hauteur: int=6) -> None:\n",
    "\n",
    "    new_df = pd.concat([y_train.assign(Set=\"Train\"), y_test.assign(Set=\"Test\")], axis=0)\n",
    "\n",
    "    new_df.columns = new_df.columns.str.replace('labels 0', 'Target')\n",
    "\n",
    "    plt.figure(figsize=(largeur, hauteur))\n",
    "\n",
    "    sns.violinplot(data=new_df, x=\"Target\", y=\"Set\")\n",
    "\n",
    "    plt.title(\"Violinplot de la distribution des valeurs de la variable cible, dans le Train set et le Test set\", pad=10)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Fonction affichant les scores de prédictions\n",
    "\n",
    "def print_scores(y_test: tp.Union[tp.List[float], np.ndarray], pred: tp.Union[tp.List[float], np.ndarray]) -> None:\n",
    "\n",
    "    for metric in [\"mape\", \"mse\", \"mae\", \"rmse\", \"r2\"]:\n",
    "\n",
    "        print(\"{} = {}\".format(metric.upper(), ut.metric_choice(y_test, pred, scoring=metric)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Fonction permettant de récupérer les scores des métriques\n",
    "\n",
    "def get_df_metrics_values(dic_score: tp.Dict[tp.Tuple[str, int, str], float]) -> pd.DataFrame:\n",
    "\n",
    "    # Récupération des clés et des valeurs du dictionnaire\n",
    "\n",
    "    list_tup_keys = list(dic_score.keys())\n",
    "    list_values = list(dic_score.values())\n",
    "\n",
    "    # Transformation en dataframe\n",
    "\n",
    "    df_score_metrics = pd.DataFrame({\"Technique\": [tup[0] for tup in list_tup_keys], \n",
    "                                     \"Random_state\": [tup[1] for tup in list_tup_keys], \n",
    "                                     \"Metric\": [tup[2] for tup in list_tup_keys],\n",
    "                                     \"Score\": list_values})\n",
    "\n",
    "    return df_score_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Fonction pour plotter les scores de métriques\n",
    "\n",
    "def plot_metrics_values(df_score_metrics: pd.DataFrame, largeur: int=20, hauteur: int=20) -> None:\n",
    "\n",
    "    # Itérateur de subplot\n",
    "\n",
    "    i = 1\n",
    "\n",
    "    # Affichage des scores(des métriques), en fonction des techniques\n",
    " \n",
    "    plt.figure(figsize=(largeur, hauteur))\n",
    "\n",
    "    for metric, color in zip([\"mape\", \"mse\", \"mae\", \"rmse\", \"r2\"], [\"blue\", \"red\", \"green\", \"purple\", \"brown\"]):\n",
    "\n",
    "        df_tmp = df_score_metrics[df_score_metrics[\"Metric\"] == metric]\n",
    "\n",
    "        # Affichage des subplots\n",
    "        \n",
    "        plt.subplot(3, 2, i)\n",
    "        \n",
    "        plt.scatter(data=df_tmp.sort_values(by=\"Score\", ascending=True),\n",
    "                    x=\"Technique\",\n",
    "                    y=\"Score\", edgecolors='black', c=color)\n",
    "\n",
    "        plt.ylabel(metric.upper())\n",
    "        plt.xlabel(\"Technique\".upper())\n",
    "\n",
    "        # Incrémentation de l'itérateur\n",
    "\n",
    "        i+=1\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Fonction retournant le score de performance\n",
    "\n",
    "def score_perf(vec: tp.Tuple[float]) -> float:\n",
    "\n",
    "    max_ = max(vec)\n",
    "\n",
    "    return  1/(((max_/vec[0])*vec[0])**2 +\n",
    "               ((max_/vec[1])*vec[1])**2 +\n",
    "               ((max_/vec[2])*vec[2])**2 +\n",
    "               ((max_/vec[3])*vec[3])**2 +\n",
    "               ((max_/vec[4])*vec[4])**2)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Fonction calculant un score de performance\n",
    "\n",
    "def compute_score_perf(y_test: tp.Union[tp.List[float], np.ndarray], pred: tp.Union[tp.List[float], np.ndarray], **kwargs) -> float:\n",
    "\n",
    "    # Vecteur de coordonnées \n",
    "\n",
    "    vec_coord = []\n",
    "\n",
    "    # Chaque métriques devient une coordonnée du vecteur\n",
    "\n",
    "    for metric in [\"mape\", \"mse\", \"mae\", \"rmse\", \"r2\"]:\n",
    "\n",
    "        if metric != \"r2\":\n",
    "\n",
    "            vec_coord.append(ut.metric_choice(y_test, pred, scoring=metric))\n",
    "\n",
    "        else:\n",
    "\n",
    "            vec_coord.append(1 - ut.metric_choice(y_test, pred, scoring=metric))\n",
    "\n",
    "    vec_coord = tuple(vec_coord)\n",
    "\n",
    "    # Retouner le score de performance\n",
    "\n",
    "    return score_perf(vec_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Fonction calculant le score de performance d'un modèle\n",
    "\n",
    "def get_df_score_perf(df_score_metrics: pd.DataFrame, tab_tech: tp.List[str]) -> pd.DataFrame:\n",
    "\n",
    "    # Tableau qui contiendra les points de coordonnées : (mape, mse, mae, rmse, r2)\n",
    "\n",
    "    tab_points = []\n",
    "\n",
    "    # Compteur de lignes\n",
    "\n",
    "    line = 0\n",
    "\n",
    "    # Pour chaque techniques de splitting\n",
    "\n",
    "    for tech in tab_tech:\n",
    "\n",
    "        # Récupérer les coordonnées de la technique\n",
    "\n",
    "        vec_coord = []\n",
    "\n",
    "        df_tmp = df_score_metrics[df_score_metrics[\"Technique\"] == tech]\n",
    "\n",
    "        for i in range(len(df_tmp)):\n",
    "\n",
    "            if df_tmp.loc[line, ('Metric')] != \"r2\":\n",
    "\n",
    "                vec_coord.append(df_tmp.loc[line, ('Score')])\n",
    "\n",
    "            else: \n",
    "                \n",
    "                vec_coord.append(1 - df_tmp.loc[line, ('Score')])\n",
    "\n",
    "            line+=1\n",
    "\n",
    "        # Transformer le tableau en tuple\n",
    "\n",
    "        tab_points.append(tuple(vec_coord))\n",
    "\n",
    "    # Calculer les scores de performance\n",
    "\n",
    "    tab_norm = [score_perf(tup) for tup in tab_points]\n",
    "\n",
    "    # Mise en forme des résultats\n",
    "\n",
    "    df_tmp = pd.DataFrame({\"Technique\" : tab_tech,\n",
    "                           \"Score\" : tab_norm})\n",
    "    \n",
    "    # Trier dans l'ordre croissant\n",
    "    \n",
    "    return df_tmp.sort_values(by=[\"Score\"], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Fonction calculant le score de performance en fonction de la technique\n",
    "\n",
    "def plot_score_perf(df_score_perf: pd.DataFrame, largeur: int=12, hauteur: int=6) -> None:\n",
    "\n",
    "    plt.figure(figsize=(largeur, hauteur))\n",
    "\n",
    "    plt.scatter(data=df_score_perf,\n",
    "                x=\"Technique\",\n",
    "                y=\"Score\", edgecolors='black', c='blue')\n",
    "\n",
    "    plt.ylabel(\"Score de performance\")\n",
    "    plt.xlabel(\"Technique\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test de l'impact du mode de partage des données sur les prédictions du modèle PLSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Mise en place du tableau contenant les techniques de splitting à tester\n",
    "\n",
    "tab_tech = [\"kennard_stone\", \"k_mean\", \"cluster\", \"custom\", \"random\", \"SPlit\", \"spxy\", \"systematic\"]\n",
    "\n",
    "# Paramètrage des préprocessing à effectuer sur les données\n",
    "\n",
    "pre_pro = True\n",
    "scale = True\n",
    "rm_outliers = False\n",
    "random_state = 0\n",
    "\n",
    "# Chargement des données\n",
    "\n",
    "n = n_set.NIRS_Set('data')\n",
    "features, labels = n.load('Xcal.txt', 'Ycal.txt', x_hdr=0, y_hdr=0, y_cols=0)\n",
    "features, labels = to_dataframe(features=features,\n",
    "                                labels=labels,\n",
    "                                pre_pro=pre_pro,\n",
    "                                scale=scale,\n",
    "                                rm_outliers=rm_outliers,\n",
    "                                random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Paramètres utilisés\n",
    "\n",
    "n_iter = 500\n",
    "test_size = 0.2\n",
    "n_cluster = 8\n",
    "pca = 0.999\n",
    "\n",
    "# Lecture du fichier résultat\n",
    "\n",
    "df_res = pd.read_csv(\"splitting_state_prtrt_scl_rdstt_0.csv\")\n",
    "df_res.rename(columns = {'tech':'Technique'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Mise en place du dictionnaire de scores\n",
    "\n",
    "dic_score = {}\n",
    "\n",
    "# Pour chaque techniques de splitting\n",
    "\n",
    "for tech in tab_tech:\n",
    "\n",
    "    # Récupérer le minimum de dissimilarité\n",
    "\n",
    "    df_tmp = df_res[df_res[\"Technique\"] == tech]\n",
    "    df_tmp = df_tmp[df_tmp[\"Score\"] == df_tmp[\"Score\"].min()]\n",
    "\n",
    "    key_tech, key_r_state = df_tmp['Technique'].iloc[0], df_tmp['random_state'].iloc[0]\n",
    "    value_score = df_tmp['Score'].iloc[0]\n",
    "\n",
    "    # Split des données en fonction du combo\n",
    "\n",
    "    x_train, x_test, y_train, y_test = st.sampling_train_test_split(features=features,\n",
    "                                                                    labels=labels,\n",
    "                                                                    test_size=test_size,\n",
    "                                                                    tech=key_tech,\n",
    "                                                                    random_state=key_r_state,\n",
    "                                                                    n_cluster=n_cluster,\n",
    "                                                                    pca=pca)\n",
    "\n",
    "    # Fit de l'estimateur + Prédiction de l'estimateur\n",
    "\n",
    "    estimator = PLSRegression(n_components=30, scale=False, max_iter=1)\n",
    "    estimator.fit(x_train, y_train)\n",
    "    pred = estimator.predict(x_test)\n",
    "\n",
    "    # Calcule du score des métrics\n",
    "\n",
    "    for metric in [\"mape\", \"mse\", \"mae\", \"rmse\", \"r2\"]:\n",
    "\n",
    "        dic_score[(key_tech, key_r_state, metric)] = ut.metric_choice(y_test, pred, scoring=metric)\n",
    "\n",
    "    # Visualisation de la distribution des valeurs de la variable cible + Plotting les résultats\n",
    "\n",
    "    print(\"Technique + valeur du {}(dissimilarité) : ({}, {}) = {}\\n\".format(\"min\".upper(), key_tech, key_r_state, value_score))\n",
    "    print_shape(x_train, y_train, x_test, y_test)\n",
    "    print_scores(y_test, pred)\n",
    "    plot_distrib(y_train, y_test)\n",
    "    pltg.plot_result(labels, x_train, x_test, y_train, y_test, estimator, largeur=10, hauteur=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Récupération des scores de métriques\n",
    "\n",
    "df_score_metrics = get_df_metrics_values(dic_score)\n",
    "\n",
    "# Sauvegarde des résultats\n",
    "\n",
    "df_score_metrics.to_csv(\"Xcal_plsr_metric_score_res.csv\", index=False)\n",
    "\n",
    "# Plotting des résultats\n",
    "\n",
    "plot_metrics_values(df_score_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Récupérer les score de performances\n",
    "\n",
    "df_score_perf = get_df_score_perf(df_score_metrics, tab_tech)\n",
    "\n",
    "# Sauvegarde des résultats\n",
    "\n",
    "df_score_perf.to_csv(\"Xcal_plsr_perf_score_res.csv\", index=False)\n",
    "\n",
    "plot_score_perf(df_score_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparamétrisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting selon le meilleur score de performance\n",
    "\n",
    "x_train, x_test, y_train, y_test = st.sampling_train_test_split(features=features,\n",
    "                                                                labels=labels,\n",
    "                                                                test_size=test_size,\n",
    "                                                                tech=\"cluster\",\n",
    "                                                                random_state=157,\n",
    "                                                                n_cluster=n_cluster,\n",
    "                                                                pca=pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Mise en place des Kfold\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=False).split(x_train)\n",
    "\n",
    "# Paramètres à tester\n",
    "\n",
    "params = {\"n_components\" : np.arange(1, 51, 1)}\n",
    "\n",
    "# Création de notre métrique de score \n",
    "\n",
    "scorer_metric = make_scorer(compute_score_perf, greater_is_better=True)\n",
    "\n",
    "# Recharche des meilleurs hyperparamètres\n",
    "\n",
    "grid_cv = GridSearchCV(estimator=PLSRegression(scale=False, max_iter=1), param_grid=params, scoring=scorer_metric, cv=kfold)\n",
    "\n",
    "grid_cv.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Affichage des résultats\n",
    "\n",
    "pred = grid_cv.predict(x_test)\n",
    "\n",
    "print(\"Best params : {}\\n\".format(grid_cv.best_params_))\n",
    "\n",
    "print(\"Best score Grid_search : {}\".format(grid_cv.best_score_))\n",
    "print(\"Score performance Test set : {}\\n\".format(compute_score_perf(y_test, pred)))\n",
    "\n",
    "print_scores(y_test, pred)\n",
    "\n",
    "pltg.plot_result(labels, x_train, x_test, y_train, y_test, grid_cv, largeur=10, hauteur=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauvegarde du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(grid_cv.best_estimator_, open(\"Xcal_plsr_grid.sav\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66168a667a11ac16aca0d0d9742c8419f93457ff66115bf85239ea370d417636"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

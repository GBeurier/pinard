{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 03:05:48,197 - INFO - ================================================================================\n",
      "2024-11-01 03:05:48,198 - INFO - ### PREPARING DATA ###\n",
      "2024-11-01 03:05:48,198 - INFO - ### LOADING DATASET ###\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Browsing sample_data/YamMould\n",
      "No train_group file found for sample_data/YamMould.\n",
      "No test_group file found for sample_data/YamMould.\n",
      "{'initial_shape': (359, 1050), 'delimiter': ';', 'numeric_delimiter': '.', 'header_line': 0, 'final_shape': (359, 1050), 'na_handling': {'strategy': 'abort', 'nb_removed_rows': None, 'removed_rows': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 03:05:48,709 - INFO - Dataset(x_train:(359, 1050) - y_train:(359, 1), x_test:(222, 1050) - y_test:(222, 1))\n",
      "2024-11-01 03:05:48,709 - INFO - ### PROCESSING DATASET ###\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'initial_shape': (222, 1050), 'delimiter': ';', 'numeric_delimiter': '.', 'header_line': 0, 'final_shape': (222, 1050), 'na_handling': {'strategy': 'abort', 'nb_removed_rows': None, 'removed_rows': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 03:05:49,069 - INFO - Dataset(x_train:(359, 12600) - y_train:(359, 1), x_test:(222, 12600) - y_test:(222, 1))\n",
      "Folds size: 239-120, 239-120, 240-119\n",
      "2024-11-01 03:05:49,074 - INFO - ### PREPARING MODEL ###\n",
      "2024-11-01 03:05:49,190 - INFO - Running config > {'dataset': 'sample_data/YamMould', 'x_pipeline': [{'class': 'sklearn.preprocessing.RobustScaler', 'params': {'copy': True, 'quantile_range': [25.0, 75.0], 'unit_variance': False, 'with_centering': True, 'with_scaling': True}}, {'split': {'class': 'sklearn.model_selection.RepeatedKFold', 'params': {'cv': {'class': 'sklearn.model_selection.KFold', 'params': None}, 'n_repeats': 1, 'random_state': None, 'cvargs': {'n_splits': 3}}}}, {'parallel': [{'class': 'sklearn.preprocessing.FunctionTransformer', 'params': {'accept_sparse': False, 'check_inverse': True, 'feature_names_out': None, 'func': None, 'inv_kw_args': None, 'inverse_func': None, 'kw_args': None, 'validate': False}}, {'class': 'pinard.transformations.SavitzkyGolay', 'params': {'copy': True, 'delta': 1.0, 'deriv': 2, 'polyorder': 2, 'window_length': 17}}, {'class': 'pinard.transformations.SavitzkyGolay', 'params': {'copy': True, 'delta': 1.0, 'deriv': 0, 'polyorder': 2, 'window_length': 5}}, {'class': 'pinard.transformations.Gaussian', 'params': {'copy': True, 'order': 1, 'sigma': 2}}, {'class': 'pinard.transformations.Gaussian', 'params': {'copy': True, 'order': 2, 'sigma': 1}}, {'class': 'pinard.transformations.Gaussian', 'params': {'copy': True, 'order': 0, 'sigma': 2}}, {'class': 'sklearn.preprocessing.StandardScaler', 'params': {'copy': True, 'with_mean': True, 'with_std': True}}, {'class': 'pinard.transformations.MultiplicativeScatterCorrection', 'params': {'copy': True, 'scale': False}}, {'class': 'pinard.transformations.Detrend', 'params': {'bp': 0, 'copy': True}}, {'class': 'pinard.transformations.Derivate', 'params': {'copy': True, 'delta': 1, 'order': 2}}, {'class': 'pinard.transformations.Derivate', 'params': {'copy': True, 'delta': 1, 'order': 2}}, {'class': 'pinard.transformations.Wavelet', 'params': {'copy': True, 'mode': 'periodization', 'wavelet': 'haar'}}]}, {'class': 'sklearn.preprocessing.MinMaxScaler', 'params': {'clip': False, 'copy': True, 'feature_range': [0, 1]}}], 'y_pipeline': {'class': 'sklearn.preprocessing.MinMaxScaler', 'params': {'clip': False, 'copy': True, 'feature_range': [0, 1]}}, 'model': {'function': 'pinard.presets.ref_models.bacon_classification'}, 'experiment': {'action': 'finetune', 'task': 'classification', 'finetune_params': {'n_trials': 5, 'model_params': {'filters_1': [8, 16, 32, 64], 'filters_2': [8, 16, 32, 64], 'filters_3': [8, 16, 32, 64]}}, 'training_params': {'epochs': 5, 'verbose': 0, 'loss': 'binary_crossentropy'}, 'metrics': ['accuracy'], 'num_classes': 2}, 'seed': 246918912}\n",
      "2024-11-01 03:05:49,195 - INFO - Starting new experiment experiment_8dde59fa.\n",
      "2024-11-01 03:05:49,196 - INFO - Experiment prepared at results\\sample_dataYamMould\\bacon_classification\\experiment_8dde59fa\n",
      "2024-11-01 03:05:49,197 - INFO - Finetuning the model\n",
      "[I 2024-11-01 03:05:49,197] A new study created in memory with name: no-name-f4cb013e-3e47-4d05-a1a0-8bd9862f897a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model cloned\n",
      "Model cloned\n",
      "Using framework: tensorflow\n",
      "Model cloned\n",
      "Model cloned\n",
      "Model cloned\n",
      "Model cloned\n",
      "Model cloned\n",
      "Model cloned\n",
      "Using framework: tensorflow\n",
      "Model cloned\n",
      "Using framework: tensorflow\n",
      "Training fold with shapes: (239, 1050, 12) (239, 1) (120, 1050, 12) (120, 1)\n",
      "binary_crossentropy ['accuracy']\n",
      "Model cloned\n",
      "Using framework: tensorflow\n",
      "Training fold with shapes: (239, 1050, 12) (239, 1) (120, 1050, 12) (120, 1)\n",
      "binary_crossentropy ['accuracy']\n",
      "Training with shapes: (239, 1050, 12) (239, 1) (120, 1050, 12) (120, 1)\n",
      "Training fold with shapes: (239, 1050, 12) (239, 1) (120, 1050, 12) (120, 1)\n",
      "binary_crossentropy ['accuracy']\n",
      "Training with shapes: (239, 1050, 12) (239, 1) (120, 1050, 12) (120, 1)\n",
      "Training with shapes: (239, 1050, 12) (239, 1) (120, 1050, 12) (120, 1)\n",
      "Model cloned\n",
      "Using framework: tensorflow\n",
      "Model cloned\n",
      "Using framework: tensorflow\n",
      "Training fold with shapes: (239, 1050, 12) (239, 1) (120, 1050, 12) (120, 1)\n",
      "binary_crossentropy ['accuracy']\n",
      "Training fold with shapes: (239, 1050, 12) (239, 1) (120, 1050, 12) (120, 1)\n",
      "binary_crossentropy ['accuracy']\n",
      "Training with shapes: (239, 1050, 12) (239, 1) (120, 1050, 12) (120, 1)\n",
      "Training with shapes: (239, 1050, 12) (239, 1) (120, 1050, 12) (120, 1)\n",
      "Training fold with shapes: (239, 1050, 12) (239, 1) (120, 1050, 12) (120, 1)\n",
      "binary_crossentropy ['accuracy']\n",
      "Training with shapes: (239, 1050, 12) (239, 1) (120, 1050, 12) (120, 1)\n",
      "Training fold with shapes: (239, 1050, 12) (239, 1) (120, 1050, 12) (120, 1)\n",
      "binary_crossentropy ['accuracy']\n",
      "Training with shapes: (239, 1050, 12) (239, 1) (120, 1050, 12) (120, 1)\n",
      "Training fold with shapes: (239, 1050, 12) (239, 1) (120, 1050, 12) (120, 1)\n",
      "binary_crossentropy ['accuracy']\n",
      "Training with shapes: (239, 1050, 12) (239, 1) (120, 1050, 12) (120, 1)\n",
      "Training fold with shapes: (239, 1050, 12) (239, 1) (120, 1050, 12) (120, 1)\n",
      "binary_crossentropy ['accuracy']\n",
      "Training with shapes: (239, 1050, 12) (239, 1) (120, 1050, 12) (120, 1)\n",
      "Training fold with shapes: (239, 1050, 12) (239, 1) (120, 1050, 12) (120, 1)\n",
      "binary_crossentropy ['accuracy']\n",
      "Training with shapes: (239, 1050, 12) (239, 1) (120, 1050, 12) (120, 1)\n",
      "Training fold with shapes: (240, 1050, 12) (240, 1) (119, 1050, 12) (119, 1)\n",
      "binary_crossentropy ['accuracy']\n",
      "Training with shapes: (240, 1050, 12) (240, 1) (119, 1050, 12) (119, 1)\n",
      "Training fold with shapes: (240, 1050, 12) (240, 1) (119, 1050, 12) (119, 1)\n",
      "binary_crossentropy ['accuracy']\n",
      "Training with shapes: (240, 1050, 12) (240, 1) (119, 1050, 12) (119, 1)\n",
      "Training fold with shapes: (240, 1050, 12) (240, 1) (119, 1050, 12) (119, 1)\n",
      "binary_crossentropy ['accuracy']\n",
      "Training with shapes: (240, 1050, 12) (240, 1) (119, 1050, 12) (119, 1)\n",
      "Training fold with shapes: (240, 1050, 12) (240, 1) (119, 1050, 12) (119, 1)\n",
      "binary_crossentropy ['accuracy']\n",
      "Training with shapes: (240, 1050, 12) (240, 1) (119, 1050, 12) (119, 1)\n",
      "Training fold with shapes: (240, 1050, 12) (240, 1) (119, 1050, 12) (119, 1)\n",
      "binary_crossentropy ['accuracy']\n",
      "Training with shapes: (240, 1050, 12) (240, 1) (119, 1050, 12) (119, 1)\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-01 03:06:15,890] Trial 0 finished with value: 0.5945945945945946 and parameters: {'filters_1': 64, 'filters_2': 64, 'filters_3': 8}. Best is trial 0 with value: 0.5945945945945946.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-01 03:06:17,944] Trial 4 finished with value: 0.5945945945945946 and parameters: {'filters_1': 8, 'filters_2': 32, 'filters_3': 8}. Best is trial 0 with value: 0.5945945945945946.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-01 03:06:18,201] Trial 3 finished with value: 0.5945945945945946 and parameters: {'filters_1': 8, 'filters_2': 64, 'filters_3': 16}. Best is trial 0 with value: 0.5945945945945946.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-01 03:06:18,372] Trial 1 finished with value: 0.5945945945945946 and parameters: {'filters_1': 32, 'filters_2': 8, 'filters_3': 8}. Best is trial 0 with value: 0.5945945945945946.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-01 03:06:18,418] Trial 2 finished with value: 0.5945945945945946 and parameters: {'filters_1': 16, 'filters_2': 8, 'filters_3': 8}. Best is trial 0 with value: 0.5945945945945946.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model params: {'filters_1': 64, 'filters_2': 64, 'filters_3': 8}\n",
      "Training fold with shapes: (239, 1050, 12) (239, 1) (120, 1050, 12) (120, 1)\n",
      "binary_crossentropy None\n",
      "Training with shapes: (239, 1050, 12) (239, 1) (120, 1050, 12) (120, 1)\n",
      "Training fold with shapes: (239, 1050, 12) (239, 1) (120, 1050, 12) (120, 1)\n",
      "binary_crossentropy None\n",
      "Training with shapes: (239, 1050, 12) (239, 1) (120, 1050, 12) (120, 1)\n",
      "Training fold with shapes: (240, 1050, 12) (240, 1) (119, 1050, 12) (119, 1)\n",
      "binary_crossentropy None\n",
      "Training with shapes: (240, 1050, 12) (240, 1) (119, 1050, 12) (119, 1)\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 03:06:28,256 - INFO - Evaluation Metrics fold_0: {'accuracy': 0.7117117117117117}\n",
      "2024-11-01 03:06:28,257 - INFO - Evaluation Metrics fold_1: {'accuracy': 0.5630630630630631}\n",
      "2024-11-01 03:06:28,258 - INFO - Evaluation Metrics fold_2: {'accuracy': 0.6801801801801802}\n",
      "2024-11-01 03:06:28,258 - INFO - Evaluation Metrics mean: {'accuracy': 0.7027027027027027}\n",
      "2024-11-01 03:06:28,260 - INFO - Evaluation Metrics best: {'accuracy': 0.7117117117117117}\n",
      "2024-11-01 03:06:28,261 - INFO - Evaluation Metrics weighted: {'accuracy': 0.7027027027027027}\n",
      "2024-11-01 03:06:28,266 - INFO - Metrics saved to results\\sample_dataYamMould\\bacon_classification\\experiment_8dde59fa\\metrics.json\n",
      "2024-11-01 03:06:28,268 - INFO - Best parameters {'filters_1': 64, 'filters_2': 64, 'filters_3': 8} saved to results\\sample_dataYamMould\\bacon_classification\\experiment_8dde59fa\\best_params.json\n",
      "2024-11-01 03:06:28,270 - INFO - Predictions saved to results\\sample_dataYamMould\\bacon_classification\\experiment_8dde59fa\\predictions.csv\n",
      "2024-11-01 03:06:28,277 - INFO - Updated experiments at results\\sample_dataYamMould\\bacon_classification\\experiments.json\n",
      "2024-11-01 03:06:28,284 - INFO - Updated experiments at results\\sample_dataYamMould\\experiments.json\n",
      "2024-11-01 03:06:28,285 - INFO - Updated experiments at results\\sample_dataYamMould\\bacon_classification\\experiments.json and results\\sample_dataYamMould\\experiments.json\n",
      "2024-11-01 03:06:28,286 - INFO - ================================================================================\n",
      "2024-11-01 03:06:28,286 - INFO - ### PREPARING DATA ###\n",
      "2024-11-01 03:06:28,287 - INFO - ### LOADING DATASET ###\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [0.3640553  0.28801843 0.34792627]\n",
      ">> Browsing sample_data/mock_data3_classif\n",
      "No train_group file found for sample_data/mock_data3_classif.\n",
      "No test_group file found for sample_data/mock_data3_classif.\n",
      "{'initial_shape': (3093, 1050), 'delimiter': ';', 'numeric_delimiter': '.', 'header_line': 0, 'final_shape': (3093, 1050), 'na_handling': {'strategy': 'abort', 'nb_removed_rows': None, 'removed_rows': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 03:06:30,736 - INFO - Dataset(x_train:(3093, 1050) - y_train:(3093, 1), x_test:(732, 1050) - y_test:(732, 1))\n",
      "2024-11-01 03:06:30,736 - INFO - ### PROCESSING DATASET ###\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'initial_shape': (732, 1050), 'delimiter': ';', 'numeric_delimiter': '.', 'header_line': 0, 'final_shape': (732, 1050), 'na_handling': {'strategy': 'abort', 'nb_removed_rows': None, 'removed_rows': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 03:06:32,310 - INFO - Dataset(x_train:(3093, 12600) - y_train:(3093, 1), x_test:(732, 12600) - y_test:(732, 1))\n",
      "Folds size: 2062-1031, 2062-1031, 2062-1031\n",
      "2024-11-01 03:06:32,313 - INFO - ### PREPARING MODEL ###\n",
      "2024-11-01 03:06:32,432 - INFO - Running config > {'dataset': 'sample_data/mock_data3_classif', 'x_pipeline': [{'class': 'sklearn.preprocessing.RobustScaler', 'params': {'copy': True, 'quantile_range': [25.0, 75.0], 'unit_variance': False, 'with_centering': True, 'with_scaling': True}}, {'split': {'class': 'sklearn.model_selection.RepeatedKFold', 'params': {'cv': {'class': 'sklearn.model_selection.KFold', 'params': None}, 'n_repeats': 1, 'random_state': None, 'cvargs': {'n_splits': 3}}}}, {'parallel': [{'class': 'sklearn.preprocessing.FunctionTransformer', 'params': {'accept_sparse': False, 'check_inverse': True, 'feature_names_out': None, 'func': None, 'inv_kw_args': None, 'inverse_func': None, 'kw_args': None, 'validate': False}}, {'class': 'pinard.transformations.SavitzkyGolay', 'params': {'copy': True, 'delta': 1.0, 'deriv': 2, 'polyorder': 2, 'window_length': 17}}, {'class': 'pinard.transformations.SavitzkyGolay', 'params': {'copy': True, 'delta': 1.0, 'deriv': 0, 'polyorder': 2, 'window_length': 5}}, {'class': 'pinard.transformations.Gaussian', 'params': {'copy': True, 'order': 1, 'sigma': 2}}, {'class': 'pinard.transformations.Gaussian', 'params': {'copy': True, 'order': 2, 'sigma': 1}}, {'class': 'pinard.transformations.Gaussian', 'params': {'copy': True, 'order': 0, 'sigma': 2}}, {'class': 'sklearn.preprocessing.StandardScaler', 'params': {'copy': True, 'with_mean': True, 'with_std': True}}, {'class': 'pinard.transformations.MultiplicativeScatterCorrection', 'params': {'copy': True, 'scale': False}}, {'class': 'pinard.transformations.Detrend', 'params': {'bp': 0, 'copy': True}}, {'class': 'pinard.transformations.Derivate', 'params': {'copy': True, 'delta': 1, 'order': 2}}, {'class': 'pinard.transformations.Derivate', 'params': {'copy': True, 'delta': 1, 'order': 2}}, {'class': 'pinard.transformations.Wavelet', 'params': {'copy': True, 'mode': 'periodization', 'wavelet': 'haar'}}]}, {'class': 'sklearn.preprocessing.MinMaxScaler', 'params': {'clip': False, 'copy': True, 'feature_range': [0, 1]}}], 'y_pipeline': {'class': 'sklearn.preprocessing.MinMaxScaler', 'params': {'clip': False, 'copy': True, 'feature_range': [0, 1]}}, 'model': {'function': 'pinard.presets.ref_models.bacon_classification'}, 'experiment': {'action': 'finetune', 'task': 'classification', 'finetune_params': {'n_trials': 5, 'model_params': {'filters_1': [8, 16, 32, 64], 'filters_2': [8, 16, 32, 64], 'filters_3': [8, 16, 32, 64]}}, 'training_params': {'epochs': 5, 'verbose': 0, 'loss': 'binary_crossentropy'}, 'metrics': ['accuracy'], 'num_classes': 21}, 'seed': 246918912}\n",
      "2024-11-01 03:06:32,434 - INFO - Starting new experiment experiment_f8ed3c9e.\n",
      "2024-11-01 03:06:32,436 - INFO - Experiment prepared at results\\sample_datamock_data3_classif\\bacon_classification\\experiment_f8ed3c9e\n",
      "2024-11-01 03:06:32,437 - INFO - Finetuning the model\n",
      "[I 2024-11-01 03:06:32,438] A new study created in memory with name: no-name-b74c2625-32b4-46d1-8c05-2dc61002a4cb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model cloned\n",
      "Model cloned\n",
      "Using framework: tensorflow\n",
      "Model cloned\n",
      "Model cloned\n",
      "Model cloned\n",
      "Model cloned\n",
      "Model cloned\n",
      "Model cloned\n",
      "Using framework: tensorflow\n",
      "Model cloned\n",
      "Using framework: tensorflow\n",
      "Model cloned\n",
      "Using framework: tensorflow\n",
      "Model cloned\n",
      "Using framework: tensorflow\n",
      "Model cloned\n",
      "Using framework: tensorflow\n",
      "Training fold with shapes: (2062, 1050, 12) (2062, 1) (1031, 1050, 12) (1031, 1)\n",
      "binary_crossentropy ['accuracy']\n",
      "Training fold with shapes: (2062, 1050, 12) (2062, 1) (1031, 1050, 12) (1031, 1)\n",
      "binary_crossentropy ['accuracy']\n",
      "Training fold with shapes: (2062, 1050, 12) (2062, 1) (1031, 1050, 12) (1031, 1)\n",
      "binary_crossentropy ['accuracy']\n",
      "Training fold with shapes: (2062, 1050, 12) (2062, 1) (1031, 1050, 12) (1031, 1)\n",
      "binary_crossentropy ['accuracy']\n",
      "Training fold with shapes: (2062, 1050, 12) (2062, 1) (1031, 1050, 12) (1031, 1)\n",
      "binary_crossentropy ['accuracy']\n",
      "Training with shapes: (2062, 1050, 12) (2062, 1) (1031, 1050, 12) (1031, 1)\n",
      "Training with shapes: (2062, 1050, 12) (2062, 1) (1031, 1050, 12) (1031, 1)\n",
      "Training with shapes: (2062, 1050, 12) (2062, 1) (1031, 1050, 12) (1031, 1)\n",
      "Training with shapes: (2062, 1050, 12) (2062, 1) (1031, 1050, 12) (1031, 1)\n",
      "Training with shapes: (2062, 1050, 12) (2062, 1) (1031, 1050, 12) (1031, 1)\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "\n",
    "import time\n",
    "from pinard.presets.ref_models import decon, bacon, customizable_bacon, bacon_classification\n",
    "from pinard.presets.preprocessings import decon_set, bacon_set\n",
    "from pinard.data_splitters import KennardStoneSplitter\n",
    "from pinard.transformations import StandardNormalVariate as SNV, SavitzkyGolay as SG, Gaussian as GS, Derivate as  Dv\n",
    "from pinard.transformations import Rotate_Translate as RT, Spline_X_Simplification as SXS, Random_X_Operation as RXO\n",
    "from pinard.transformations import CropTransformer\n",
    "from pinard.core.runner import ExperimentRunner\n",
    "from pinard.core.config import Config\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, RepeatedKFold, StratifiedKFold, RepeatedStratifiedKFold, ShuffleSplit, GroupKFold, StratifiedShuffleSplit, BaseCrossValidator, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "model_sklearn = {\n",
    "    \"class\": \"sklearn.cross_decomposition.PLSRegression\",\n",
    "    \"model_params\": {\n",
    "        \"n_components\": 21,\n",
    "    }\n",
    "}\n",
    "    \n",
    "finetune_pls_experiment = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"finetune_params\": {\n",
    "        'model_params': {\n",
    "            'n_components': ('int', 5, 20),\n",
    "        },\n",
    "        'training_params': {},\n",
    "        'tuner': 'sklearn'\n",
    "    }\n",
    "}\n",
    "\n",
    "bacon_train = {\"action\": \"train\", \"training_params\": {\"epochs\": 2000, \"batch_size\": 500, \"patience\": 200, \"cyclic_lr\": True, \"base_lr\": 1e-6, \"max_lr\": 1e-3, \"step_size\": 400}}\n",
    "bacon_train_short = {\"action\": \"train\", \"training_params\": {\"epochs\": 10, \"batch_size\": 500, \"patience\": 20, \"cyclic_lr\": True, \"base_lr\": 1e-6, \"max_lr\": 1e-3, \"step_size\": 40}}\n",
    "bacon_finetune = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"finetune_params\": {\n",
    "        \"n_trials\": 5,\n",
    "        \"model_params\": {\n",
    "            \"filters_1\": [8, 16, 32, 64], \n",
    "            \"filters_2\": [8, 16, 32, 64], \n",
    "            \"filters_3\": [8, 16, 32, 64]\n",
    "        }\n",
    "    },\n",
    "    \"training_params\": {\n",
    "        \"epochs\": 10,\n",
    "        \"verbose\":0\n",
    "    }\n",
    "}\n",
    "\n",
    "full_bacon_finetune = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"training_params\": {\n",
    "        \"epochs\": 500,\n",
    "        \"patience\": 100,\n",
    "    },\n",
    "    \"finetune_params\": {\n",
    "        \"nb_trials\": 150,\n",
    "        \"model_params\": {\n",
    "            'spatial_dropout': (float, 0.01, 0.5),\n",
    "            'filters1': [4, 8, 16, 32, 64, 128, 256],\n",
    "            'kernel_size1': [3, 5, 7, 9, 11, 13, 15],\n",
    "            # 'strides1': [1, 2, 3, 4, 5],\n",
    "            # 'activation1': ['relu', 'selu', 'elu', 'swish'],\n",
    "            'dropout_rate': (float, 0.01, 0.5),\n",
    "            'filters2': [4, 8, 16, 32, 64, 128, 256],\n",
    "            # 'kernel_size2': [3, 5, 7, 9, 11, 13, 15],\n",
    "            # 'strides2': [1, 2, 3, 4, 5],\n",
    "            'activation2': ['relu', 'selu', 'elu', 'swish'],\n",
    "            'normalization_method1': ['BatchNormalization', 'LayerNormalization'],\n",
    "            'filters3': [4, 8, 16, 32, 64, 128, 256],\n",
    "            # 'kernel_size3': [3, 5, 7, 9, 11, 13, 15],\n",
    "            # 'strides3': [1, 2, 3, 4, 5],\n",
    "            'activation3': ['relu', 'selu', 'elu', 'swish'],\n",
    "            # 'normalization_method2': ['BatchNormalization', 'LayerNormalization'],\n",
    "            # 'dense_units': [4, 8, 16, 32, 64, 128, 256],\n",
    "            'dense_activation': ['relu', 'selu', 'elu', 'swish'],\n",
    "        },\n",
    "        # \"training_params\": {\n",
    "        #     \"batch_size\": [32, 64, 128, 256, 512],\n",
    "        #     \"cyclic_lr\": [True, False],\n",
    "        #     \"base_lr\": (float, 1e-6, 1e-2),\n",
    "        #     \"max_lr\": (float, 1e-3, 1e-1),\n",
    "        #     \"step_size\": (int, 500, 5000),\n",
    "        # },\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "x_pipeline_full = [\n",
    "    RobustScaler(),\n",
    "    {\"samples\": [None, None,None,None,SXS, RXO]},\n",
    "    {\"split\": RepeatedKFold(n_splits=3, n_repeats=1)},\n",
    "    {\"features\": [None, GS(2,1), SG, SNV, Dv, [GS, SNV], [GS, GS],[GS, SG],[SG, SNV], [GS, Dv], [SG, Dv]]},\n",
    "    MinMaxScaler()\n",
    "]\n",
    "\n",
    "\n",
    "bacon_finetune_classif = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"task\": \"classification\",\n",
    "    \"finetune_params\": {\n",
    "        \"n_trials\": 5,\n",
    "        \"model_params\": {\n",
    "            \"filters_1\": [8, 16, 32, 64], \n",
    "            \"filters_2\": [8, 16, 32, 64], \n",
    "            \"filters_3\": [8, 16, 32, 64]\n",
    "        }\n",
    "    },\n",
    "    \"training_params\": {\n",
    "        \"epochs\": 5,\n",
    "        \"verbose\":0\n",
    "    }\n",
    "}\n",
    "\n",
    "finetune_randomForestclassifier = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"task\": \"classification\",\n",
    "    \"finetune_params\": {\n",
    "        'model_params': {\n",
    "            'n_estimators': ('int', 5, 20),\n",
    "        },\n",
    "        'training_params': {},\n",
    "        'tuner': 'sklearn'\n",
    "    }\n",
    "}\n",
    "\n",
    "x_pipeline_PLS = [\n",
    "    RobustScaler(),\n",
    "    {\"split\": RepeatedKFold(n_splits=3, n_repeats=1)},\n",
    "    {\"features\": [None, GS(2,1), SG, SNV, Dv, [GS, SNV], [GS, GS],[GS, SG],[SG, SNV], [GS, Dv], [SG, Dv]]},\n",
    "    MinMaxScaler()\n",
    "]\n",
    "            \n",
    "            \n",
    "x_pipeline = [\n",
    "    RobustScaler(), \n",
    "    {\"split\": RepeatedKFold(n_splits=3, n_repeats=1)}, \n",
    "    bacon_set(),\n",
    "    MinMaxScaler()\n",
    "]\n",
    "\n",
    "x_pipelineb = [\n",
    "    RobustScaler(), \n",
    "    {\"samples\": [RT(6)], \"balance\": True},\n",
    "    # {\"samples\": [None, RT]},\n",
    "    {\"split\": RepeatedKFold(n_splits=3, n_repeats=1)}, \n",
    "    bacon_set(),\n",
    "    MinMaxScaler()\n",
    "]\n",
    "\n",
    "\n",
    "seed = 123459456\n",
    "\n",
    "datasets = \"sample_data/mock_data3_classif\"\n",
    "y_pipeline = MinMaxScaler()\n",
    "# processing only\n",
    "config1 = Config(\"sample_data/Malaria2024\", x_pipeline_full, y_pipeline, None, None, seed)\n",
    "## TRAINING\n",
    "# regression\n",
    "config2 = Config(\"sample_data/mock_data2\", x_pipeline, y_pipeline, bacon, bacon_train_short, seed)\n",
    "config3 = Config(\"sample_data/mock_data3\", x_pipeline_PLS, y_pipeline, model_sklearn, None, seed)\n",
    "# classification\n",
    "config4 = Config(\"sample_data/YamMould\", x_pipeline, None, bacon_classification, {\"task\":\"classification\", \"training_params\":{\"epochs\":10, \"patience\": 100, \"verbose\":0}}, seed*2)\n",
    "config4b = Config(\"sample_data/YamMould\", x_pipelineb, None, bacon_classification, {\"task\":\"classification\", \"training_params\":{\"epochs\":10, \"patience\": 100, \"verbose\":0}}, seed*2)\n",
    "config5 = Config(\"sample_data/mock_data3_binary\", x_pipeline, None, bacon_classification, {\"task\":\"classification\", \"training_params\":{\"epochs\":5}, \"verbose\":0}, seed*2)\n",
    "config6 = Config(\"sample_data/WhiskyConcentration\", x_pipeline, None, RandomForestClassifier, {\"task\":\"classification\"}, seed*2)\n",
    "config7 = Config(\"sample_data/Malaria2024\", x_pipeline, None, RandomForestClassifier, {\"task\":\"classification\"}, seed*2)\n",
    "## FINETUNING\n",
    "# regression\n",
    "config8 = Config(\"sample_data/mock_data3\", x_pipeline, y_pipeline, bacon, bacon_finetune, seed)\n",
    "config9 = Config(\"sample_data/mock_data3\", x_pipeline, y_pipeline, model_sklearn, finetune_pls_experiment, seed)\n",
    "# classification\n",
    "config10 = Config(\"sample_data/mock_data3_classif\", x_pipeline, y_pipeline, bacon_classification, bacon_finetune_classif, seed*2)\n",
    "config10b = Config(\"sample_data/YamMould\", x_pipeline, y_pipeline, bacon_classification, bacon_finetune_classif, seed*2)\n",
    "config11 = Config(\"sample_data/mock_data3_classif\", x_pipeline, None, RandomForestClassifier, finetune_randomForestclassifier, seed*2)\n",
    "config11b = Config(\"sample_data/YamMould\", x_pipeline, None, RandomForestClassifier, finetune_randomForestclassifier, seed*2)\n",
    "\n",
    "\n",
    "configs = [config10, config10b]\n",
    "\n",
    "start = time.time()\n",
    "runner = ExperimentRunner(configs, resume_mode=\"restart\")\n",
    "dataset, model_manager = runner.run()\n",
    "end = time.time()\n",
    "print(f\"Time elapsed: {end-start} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinard\n",
    "print(pinard.__version__)\n",
    "\n",
    "# load malaria manually and apply RT transformation manually and display chart of transformed samples\n",
    "\n",
    "config1 = Config(\"sample_data/Malaria2024\", x_pipeline, y_pipeline, None, None, seed)\n",
    "runner = ExperimentRunner([config1], resume_mode=\"restart\")\n",
    "dataset, model_manager = runner.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ace_tools as tools\n",
    "import pandas as pd\n",
    "\n",
    "# Sklearn metrics list\n",
    "sklearn_metrics = [\n",
    "    \"explained_variance\", \"r2\", \"max_error\", \"matthews_corrcoef\",\n",
    "    \"neg_median_absolute_error\", \"neg_mean_absolute_error\",\n",
    "    \"neg_mean_absolute_percentage_error\", \"neg_mean_squared_error\",\n",
    "    \"neg_mean_squared_log_error\", \"neg_root_mean_squared_error\",\n",
    "    \"neg_root_mean_squared_log_error\", \"neg_mean_poisson_deviance\",\n",
    "    \"neg_mean_gamma_deviance\", \"d2_absolute_error_score\", \"accuracy\",\n",
    "    \"top_k_accuracy\", \"roc_auc\", \"roc_auc_ovr\", \"roc_auc_ovo\",\n",
    "    \"roc_auc_ovr_weighted\", \"roc_auc_ovo_weighted\", \"balanced_accuracy\",\n",
    "    \"average_precision\", \"neg_log_loss\", \"neg_brier_score\",\n",
    "    \"positive_likelihood_ratio\", \"neg_negative_likelihood_ratio\",\n",
    "    \"adjusted_rand_score\", \"rand_score\", \"homogeneity_score\",\n",
    "    \"completeness_score\", \"v_measure_score\", \"mutual_info_score\",\n",
    "    \"adjusted_mutual_info_score\", \"normalized_mutual_info_score\",\n",
    "    \"fowlkes_mallows_score\"\n",
    "]\n",
    "\n",
    "# Tensorflow/keras metrics list\n",
    "tensorflow_metrics = [\n",
    "    \"MeanSquaredError\", \"RootMeanSquaredError\", \"MeanAbsoluteError\",\n",
    "    \"MeanAbsolutePercentageError\", \"MeanSquaredLogarithmicError\",\n",
    "    \"CosineSimilarity\", \"LogCoshError\", \"R2Score\", \"AUC\",\n",
    "    \"FalseNegatives\", \"FalsePositives\", \"Precision\", \"PrecisionAtRecall\",\n",
    "    \"Recall\", \"RecallAtPrecision\", \"SensitivityAtSpecificity\",\n",
    "    \"SpecificityAtSensitivity\", \"TrueNegatives\", \"TruePositives\",\n",
    "    \"Hinge\", \"SquaredHinge\", \"CategoricalHinge\", \"KLDivergence\",\n",
    "    \"Poisson\", \"BinaryCrossentropy\", \"CategoricalCrossentropy\",\n",
    "    \"SparseCategoricalCrossentropy\", \"Accuracy\", \"BinaryAccuracy\",\n",
    "    \"CategoricalAccuracy\", \"SparseCategoricalAccuracy\",\n",
    "    \"TopKCategoricalAccuracy\", \"SparseTopKCategoricalAccuracy\",\n",
    "    \"F1Score\", \"FBetaScore\", \"IoU\", \"BinaryIoU\", \"MeanIoU\",\n",
    "    \"OneHotIoU\", \"OneHotMeanIoU\"\n",
    "]\n",
    "\n",
    "# Metric name mapping: (tensorflow_name, sklearn_name, abbreviation, method_name)\n",
    "# Initialize with common names\n",
    "metrics_mapping = [\n",
    "    (\"MeanSquaredError\", \"neg_mean_squared_error\", \"mse\", \"Mean Squared Error\"),\n",
    "    (\"RootMeanSquaredError\", \"neg_root_mean_squared_error\", \"rmse\", \"Root Mean Squared Error\"),\n",
    "    (\"MeanAbsoluteError\", \"neg_mean_absolute_error\", \"mae\", \"Mean Absolute Error\"),\n",
    "    (\"MeanAbsolutePercentageError\", \"neg_mean_absolute_percentage_error\", \"mape\", \"Mean Absolute Percentage Error\"),\n",
    "    (\"MeanSquaredLogarithmicError\", \"neg_mean_squared_log_error\", \"msle\", \"Mean Squared Logarithmic Error\"),\n",
    "    (\"CosineSimilarity\", None, \"cos_sim\", \"Cosine Similarity\"),\n",
    "    (\"LogCoshError\", None, \"log_cosh\", \"Log Cosh Error\"),\n",
    "    (\"R2Score\", \"r2\", \"r2\", \"R2 Score\"),\n",
    "    (\"AUC\", \"roc_auc\", \"auc\", \"Area Under the Curve\"),\n",
    "    (\"Precision\", None, \"prec\", \"Precision\"),\n",
    "    (\"Recall\", None, \"recall\", \"Recall\"),\n",
    "    (\"Accuracy\", \"accuracy\", \"acc\", \"Accuracy\"),\n",
    "    (\"TopKCategoricalAccuracy\", \"top_k_accuracy\", \"top_k_acc\", \"Top K Categorical Accuracy\"),\n",
    "    (\"BinaryCrossentropy\", None, \"bin_crossentropy\", \"Binary Crossentropy\"),\n",
    "    (\"CategoricalCrossentropy\", None, \"cat_crossentropy\", \"Categorical Crossentropy\"),\n",
    "    (\"SparseCategoricalCrossentropy\", None, \"sparse_cat_crossentropy\", \"Sparse Categorical Crossentropy\"),\n",
    "    (\"F1Score\", None, \"f1\", \"F1 Score\"),\n",
    "    (\"IoU\", None, \"iou\", \"Intersection over Union\")\n",
    "]\n",
    "\n",
    "# Add remaining metrics with None in the missing columns\n",
    "for metric in sklearn_metrics:\n",
    "    if not any(metric in row for row in metrics_mapping):\n",
    "        metrics_mapping.append((None, metric, None, None))\n",
    "\n",
    "for metric in tensorflow_metrics:\n",
    "    if not any(metric in row for row in metrics_mapping):\n",
    "        metrics_mapping.append((metric, None, None, None))\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame(metrics_mapping, columns=[\"tensorflow_name\", \"sklearn_name\", \"abbreviation\", \"method_name\"])\n",
    "\n",
    "# Display the dataframe to the user\n",
    "tools.display_dataframe_to_user(name=\"Metric Comparison\", dataframe=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Applying a Simple Data Transformation using Pinard\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pinard.transformations import StandardNormalVariate as SNV\n",
    "from pinard.core.runner import ExperimentRunner\n",
    "from pinard.core.config import Config\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "\n",
    "\n",
    "# Define the dataset path\n",
    "dataset_path = \"sample_data/mock_data\"\n",
    "\n",
    "# Define the data transformation pipeline\n",
    "# Here we apply Standard Normal Variate (SNV) transformation\n",
    "x_pipeline = [\n",
    "    SNV(),            # Apply SNV transformation\n",
    "    MinMaxScaler()    # Scale features to [0,1]\n",
    "]\n",
    "\n",
    "# No model is used in this example; we focus on data transformation\n",
    "config = Config(\n",
    "    dataset_path,\n",
    "    x_pipeline,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Run the experiment\n",
    "runner = ExperimentRunner(configs=[config])\n",
    "dataset, model_manager = runner.run()\n",
    "\n",
    "# Access the transformed data\n",
    "transformed_data = dataset.x_train\n",
    "\n",
    "print(\"Transformed data shape:\", transformed_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Applying a Preprocessing Pipeline and Training a Simple Model\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pinard.transformations import StandardNormalVariate as SNV, SavitzkyGolay as SG\n",
    "from pinard.core.runner import ExperimentRunner\n",
    "from pinard.core.config import Config\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "\n",
    "\n",
    "# Define the dataset path\n",
    "dataset_path = \"sample_data/mock_data\"\n",
    "\n",
    "# Define the data transformation pipeline\n",
    "x_pipeline = [\n",
    "    SG(window_length=11, polyorder=2),  # Apply Savitzky-Golay filter\n",
    "    SNV(),                              # Apply SNV transformation\n",
    "    MinMaxScaler()                      # Scale features to [0,1]\n",
    "]\n",
    "\n",
    "# Define the model\n",
    "model = {\n",
    "    \"class\": \"sklearn.linear_model.LinearRegression\",\n",
    "    \"model_params\": {}\n",
    "}\n",
    "\n",
    "# Training parameters\n",
    "train_params = {\n",
    "    \"action\": \"train\",\n",
    "    \"training_params\": {}\n",
    "}\n",
    "\n",
    "# Define the configuration\n",
    "config = Config(\n",
    "    dataset_path=dataset_path,\n",
    "    x_pipeline=x_pipeline,\n",
    "    y_pipeline=None,\n",
    "    model=model,\n",
    "    experiment_params=train_params,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Run the experiment\n",
    "runner = ExperimentRunner(configs=[config])\n",
    "dataset, model_manager = runner.run()\n",
    "\n",
    "# Access the trained model\n",
    "trained_model = model_manager.models[0].model\n",
    "\n",
    "# Print model coefficients\n",
    "print(\"Model coefficients:\", trained_model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Using Cross-Validation with Pinard\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pinard.transformations import StandardNormalVariate as SNV, SavitzkyGolay as SG\n",
    "from pinard.core.runner import ExperimentRunner\n",
    "from pinard.core.config import Config\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "\n",
    "\n",
    "# Define the dataset path\n",
    "dataset_path = \"sample_data/mock_data\"\n",
    "\n",
    "# Define the data transformation pipeline with cross-validation\n",
    "x_pipeline = [\n",
    "    {\"split\": RepeatedKFold(n_splits=5, n_repeats=2, random_state=42)},\n",
    "    SG(window_length=11, polyorder=2),\n",
    "    SNV(),\n",
    "    MinMaxScaler()\n",
    "]\n",
    "\n",
    "# Define the model\n",
    "model = {\n",
    "    \"class\": \"sklearn.linear_model.Ridge\",\n",
    "    \"model_params\": {\n",
    "        \"alpha\": 1.0\n",
    "    }\n",
    "}\n",
    "\n",
    "# Training parameters\n",
    "train_params = {\n",
    "    \"action\": \"train\",\n",
    "    \"training_params\": {}\n",
    "}\n",
    "\n",
    "# Define the configuration\n",
    "config = Config(\n",
    "    dataset_path=dataset_path,\n",
    "    x_pipeline=x_pipeline,\n",
    "    y_pipeline=None,\n",
    "    model=model,\n",
    "    experiment_params=train_params,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Run the experiment\n",
    "runner = ExperimentRunner(configs=[config])\n",
    "dataset, model_manager = runner.run()\n",
    "\n",
    "# Evaluate the model\n",
    "scores = model_manager.models[0].scores\n",
    "print(\"Cross-validation scores:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Fine-tuning a Model with Pinard\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pinard.transformations import StandardNormalVariate as SNV, SavitzkyGolay as SG\n",
    "from pinard.core.runner import ExperimentRunner\n",
    "from pinard.core.config import Config\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "\n",
    "\n",
    "# Define the dataset path\n",
    "dataset_path = \"sample_data/mock_data\"\n",
    "\n",
    "# Define the data transformation pipeline with cross-validation\n",
    "x_pipeline = [\n",
    "    {\"split\": RepeatedKFold(n_splits=5, n_repeats=2, random_state=42)},\n",
    "    SG(window_length=11, polyorder=2),\n",
    "    SNV(),\n",
    "    MinMaxScaler()\n",
    "]\n",
    "\n",
    "# Define the model\n",
    "model = {\n",
    "    \"class\": \"sklearn.linear_model.Ridge\",\n",
    "    \"model_params\": {}\n",
    "}\n",
    "\n",
    "# Define the finetune parameters\n",
    "finetune_params = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"finetune_params\": {\n",
    "        'model_params': {\n",
    "            'alpha': ('float', 0.1, 10.0)\n",
    "        },\n",
    "        'training_params': {},\n",
    "        'n_trials': 20,\n",
    "        'tuner': 'sklearn'  # Use scikit-learn's GridSearchCV\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define the configuration\n",
    "config = Config(\n",
    "    dataset_path=dataset_path,\n",
    "    x_pipeline=x_pipeline,\n",
    "    y_pipeline=None,\n",
    "    model=model,\n",
    "    experiment_params=finetune_params,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Run the experiment\n",
    "runner = ExperimentRunner(configs=[config])\n",
    "dataset, model_manager = runner.run()\n",
    "\n",
    "# Get the best model and its parameters\n",
    "best_model = model_manager.models[0].best_model\n",
    "best_params = model_manager.models[0].best_params\n",
    "\n",
    "print(\"Best model parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Fine-tuning a Model with Pinard\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pinard.transformations import StandardNormalVariate as SNV, SavitzkyGolay as SG\n",
    "from pinard.core.runner import ExperimentRunner\n",
    "from pinard.core.config import Config\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "\n",
    "\n",
    "# Define the dataset path\n",
    "dataset_path = \"sample_data/mock_data\"\n",
    "\n",
    "# Define the data transformation pipeline with cross-validation\n",
    "x_pipeline = [\n",
    "    {\"split\": RepeatedKFold(n_splits=5, n_repeats=2, random_state=42)},\n",
    "    SG(window_length=11, polyorder=2),\n",
    "    SNV(),\n",
    "    MinMaxScaler()\n",
    "]\n",
    "\n",
    "# Define the model\n",
    "model = {\n",
    "    \"class\": \"sklearn.linear_model.Ridge\",\n",
    "    \"model_params\": {}\n",
    "}\n",
    "\n",
    "# Define the finetune parameters\n",
    "finetune_params = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"finetune_params\": {\n",
    "        'model_params': {\n",
    "            'alpha': ('float', 0.1, 10.0)\n",
    "        },\n",
    "        'training_params': {},\n",
    "        'n_trials': 20,\n",
    "        'tuner': 'sklearn'  # Use scikit-learn's GridSearchCV\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define the configuration\n",
    "config = Config(\n",
    "    dataset_path=dataset_path,\n",
    "    x_pipeline=x_pipeline,\n",
    "    y_pipeline=None,\n",
    "    model=model,\n",
    "    experiment_params=finetune_params,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Run the experiment\n",
    "runner = ExperimentRunner(configs=[config])\n",
    "dataset, model_manager = runner.run()\n",
    "\n",
    "# Get the best model and its parameters\n",
    "best_model = model_manager.models[0].best_model\n",
    "best_params = model_manager.models[0].best_params\n",
    "\n",
    "print(\"Best model parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6: Fine-tuning a Custom TensorFlow Model with Pinard\n",
    "\n",
    "from kerastuner import HyperModel\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pinard.transformations import StandardNormalVariate as SNV, SavitzkyGolay as SG\n",
    "from pinard.core.runner import ExperimentRunner\n",
    "from pinard.core.config import Config\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "\n",
    "\n",
    "# Define the dataset path\n",
    "dataset_path = \"sample_data/mock_data\"\n",
    "\n",
    "# Placeholder for input shape (to be defined later)\n",
    "input_shape = None\n",
    "\n",
    "# Define a hypermodel for Keras Tuner\n",
    "\n",
    "\n",
    "class MyHyperModel(HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = tf.keras.Sequential()\n",
    "        units = hp.Int('units', min_value=32, max_value=128, step=32)\n",
    "        model.add(tf.keras.layers.Dense(units=units, activation='relu', input_shape=(input_shape,)))\n",
    "        model.add(tf.keras.layers.Dense(1))\n",
    "        optimizer = hp.Choice('optimizer', ['adam', 'sgd'])\n",
    "        model.compile(optimizer=optimizer, loss='mse')\n",
    "        return model\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = {\n",
    "    \"class\": \"tensorflow.keras.models.Sequential\",\n",
    "    \"model_params\": {\n",
    "        \"build_fn\": MyHyperModel()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define the finetune parameters\n",
    "finetune_params = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"finetune_params\": {\n",
    "        'model_params': {\n",
    "            'units': ('int', 32, 128, 32),\n",
    "            'optimizer': ['adam', 'sgd']\n",
    "        },\n",
    "        'training_params': {\n",
    "            'epochs': 50,\n",
    "            'batch_size': 32\n",
    "        },\n",
    "        'n_trials': 20,\n",
    "        'tuner': 'keras'  # Use Keras Tuner\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define the data transformation pipeline with cross-validation\n",
    "x_pipeline = [\n",
    "    {\"split\": RepeatedKFold(n_splits=5, n_repeats=2, random_state=42)},\n",
    "    SG(window_length=11, polyorder=2),\n",
    "    SNV(),\n",
    "    MinMaxScaler()\n",
    "]\n",
    "\n",
    "# Define the configuration\n",
    "config = Config(\n",
    "    dataset_path=dataset_path,\n",
    "    x_pipeline=x_pipeline,\n",
    "    y_pipeline=None,\n",
    "    model=model,\n",
    "    experiment_params=finetune_params,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Run the experiment\n",
    "runner = ExperimentRunner(configs=[config])\n",
    "\n",
    "# Set the input shape after data is loaded\n",
    "dataset, model_manager = runner.load_data_only()\n",
    "input_shape = dataset.x_train.shape[1]\n",
    "model['model_params']['build_fn'] = MyHyperModel()\n",
    "\n",
    "# Run the finetuning\n",
    "dataset, model_manager = runner.run()\n",
    "\n",
    "# Get the best model and its parameters\n",
    "best_model = model_manager.models[0].best_model\n",
    "best_params = model_manager.models[0].best_params\n",
    "\n",
    "print(\"Best model parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "\n",
    "class LogTransformer(TransformerMixin, BaseEstimator):\n",
    "    \"\"\"\n",
    "    Applies a logarithmic transformation to the data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, copy=True, offset=1e-6):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        copy : bool, default=True\n",
    "            Set to False to perform inplace computation.\n",
    "        offset : float, default=1e-6\n",
    "            A small constant to add to the data to avoid log(0).\n",
    "        \"\"\"\n",
    "        self.copy = copy\n",
    "        self.offset = offset\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, copy=None):\n",
    "        \"\"\"\n",
    "        Apply the logarithmic transformation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The data to transform.\n",
    "        copy : bool, default=None\n",
    "            Copy the input X or not.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array-like of shape (n_samples, n_features)\n",
    "            Transformed data.\n",
    "        \"\"\"\n",
    "        if copy is None:\n",
    "            copy = self.copy\n",
    "\n",
    "        X = np.asarray(X)\n",
    "        if copy:\n",
    "            X = X.copy()\n",
    "\n",
    "        X = np.log(X + self.offset)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import BaseCrossValidator\n",
    "from sklearn.utils import check_random_state\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class StratifiedFeatureSplitter(BaseCrossValidator):\n",
    "    \"\"\"\n",
    "    Stratified splitter based on a continuous feature.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_splits=5, feature_index=0, random_state=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_splits : int, default=5\n",
    "            Number of folds.\n",
    "        feature_index : int, default=0\n",
    "            Index of the feature to stratify on.\n",
    "        random_state : int or RandomState instance, default=None\n",
    "            Random state for reproducibility.\n",
    "        \"\"\"\n",
    "        self.n_splits = n_splits\n",
    "        self.feature_index = feature_index\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        X = np.asarray(X)\n",
    "        feature = X[:, self.feature_index]\n",
    "        percentiles = np.percentile(feature, np.linspace(0, 100, self.n_splits + 1))\n",
    "\n",
    "        indices = np.arange(len(X))\n",
    "        rng = check_random_state(self.random_state)\n",
    "        rng.shuffle(indices)\n",
    "\n",
    "        bins = np.digitize(feature[indices], percentiles[1:-1], right=True)\n",
    "\n",
    "        for fold in range(self.n_splits):\n",
    "            test_mask = bins == fold\n",
    "            train_mask = ~test_mask\n",
    "            yield indices[train_mask], indices[test_mask]\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using Custom LogTransformer and StratifiedFeatureSplitter in Pinard\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pinard.core.runner import ExperimentRunner\n",
    "from pinard.core.config import Config\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "\n",
    "\n",
    "# Assuming LogTransformer and StratifiedFeatureSplitter are defined as above\n",
    "\n",
    "# Define the dataset path\n",
    "dataset_path = \"sample_data/mock_data\"\n",
    "\n",
    "# Define the data transformation pipeline\n",
    "x_pipeline = [\n",
    "    {\"split\": StratifiedFeatureSplitter(n_splits=5, feature_index=0, random_state=42)},\n",
    "    LogTransformer(offset=1e-6),\n",
    "    MinMaxScaler()\n",
    "]\n",
    "\n",
    "# Define the model\n",
    "model = {\n",
    "    \"class\": \"sklearn.linear_model.LinearRegression\",\n",
    "    \"model_params\": {}\n",
    "}\n",
    "\n",
    "# Training parameters\n",
    "train_params = {\n",
    "    \"action\": \"train\",\n",
    "    \"training_params\": {}\n",
    "}\n",
    "\n",
    "# Define the configuration\n",
    "config = Config(\n",
    "    dataset_path=dataset_path,\n",
    "    x_pipeline=x_pipeline,\n",
    "    y_pipeline=None,\n",
    "    model=model,\n",
    "    experiment_params=train_params,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Run the experiment\n",
    "runner = ExperimentRunner(configs=[config])\n",
    "dataset, model_manager = runner.run()\n",
    "\n",
    "# Access the trained model\n",
    "trained_model = model_manager.models[0].model\n",
    "\n",
    "# Evaluate the model\n",
    "\n",
    "y_pred = trained_model.predict(dataset.x_test)\n",
    "mse = mean_squared_error(dataset.y_test, y_pred)\n",
    "print(\"Test MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RatioTransformer(TransformerMixin, BaseEstimator):\n",
    "    \"\"\"\n",
    "    Creates a new feature by taking the ratio of two features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, numerator_index=0, denominator_index=1, copy=True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        numerator_index : int, default=0\n",
    "            Index of the numerator feature.\n",
    "        denominator_index : int, default=1\n",
    "            Index of the denominator feature.\n",
    "        copy : bool, default=True\n",
    "            Set to False to perform inplace computation.\n",
    "        \"\"\"\n",
    "        self.numerator_index = numerator_index\n",
    "        self.denominator_index = denominator_index\n",
    "        self.copy = copy\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, copy=None):\n",
    "        \"\"\"\n",
    "        Create a new feature as the ratio of two existing features.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The data to transform.\n",
    "        copy : bool, default=None\n",
    "            Copy the input X or not.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array-like of shape (n_samples, n_features + 1)\n",
    "            Transformed data with the new ratio feature added.\n",
    "        \"\"\"\n",
    "        if copy is None:\n",
    "            copy = self.copy\n",
    "\n",
    "        X = np.asarray(X)\n",
    "        if copy:\n",
    "            X = X.copy()\n",
    "\n",
    "        numerator = X[:, self.numerator_index]\n",
    "        denominator = X[:, self.denominator_index] + 1e-6  # Avoid division by zero\n",
    "        ratio_feature = (numerator / denominator).reshape(-1, 1)\n",
    "\n",
    "        X_new = np.hstack((X, ratio_feature))\n",
    "        return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "\n",
    "\n",
    "class ClusterBasedSplitter(BaseCrossValidator):\n",
    "    \"\"\"\n",
    "    Splits data into training and testing sets based on clustering.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_splits=5, n_clusters=5, random_state=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_splits : int, default=5\n",
    "            Number of splits/folds.\n",
    "        n_clusters : int, default=5\n",
    "            Number of clusters to form.\n",
    "        random_state : int or RandomState instance, default=None\n",
    "            Random state for reproducibility.\n",
    "        \"\"\"\n",
    "        self.n_splits = n_splits\n",
    "        self.n_clusters = n_clusters\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        X = np.asarray(X)\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, random_state=self.random_state)\n",
    "        cluster_labels = kmeans.fit_predict(X)\n",
    "\n",
    "        unique_clusters = np.unique(cluster_labels)\n",
    "        rng = check_random_state(self.random_state)\n",
    "        rng.shuffle(unique_clusters)\n",
    "\n",
    "        clusters_per_split = np.array_split(unique_clusters, self.n_splits)\n",
    "\n",
    "        for cluster_group in clusters_per_split:\n",
    "            test_indices = np.where(np.isin(cluster_labels, cluster_group))[0]\n",
    "            train_indices = np.setdiff1d(np.arange(len(X)), test_indices)\n",
    "            yield train_indices, test_indices\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using Custom ClusterBasedSplitter in Pinard\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pinard.core.runner import ExperimentRunner\n",
    "from pinard.core.config import Config\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "\n",
    "\n",
    "# Assuming ClusterBasedSplitter is defined as above\n",
    "\n",
    "# Define the dataset path\n",
    "dataset_path = \"sample_data/mock_data\"\n",
    "\n",
    "# Define the data transformation pipeline\n",
    "x_pipeline = [\n",
    "    StandardScaler(),\n",
    "    {\"split\": ClusterBasedSplitter(n_splits=5, n_clusters=5, random_state=42)}\n",
    "]\n",
    "\n",
    "# Define the model\n",
    "model = {\n",
    "    \"class\": \"sklearn.svm.SVR\",\n",
    "    \"model_params\": {\n",
    "        \"kernel\": \"rbf\",\n",
    "        \"C\": 1.0,\n",
    "        \"epsilon\": 0.1\n",
    "    }\n",
    "}\n",
    "\n",
    "# Training parameters\n",
    "train_params = {\n",
    "    \"action\": \"train\",\n",
    "    \"training_params\": {}\n",
    "}\n",
    "\n",
    "# Define the configuration\n",
    "config = Config(\n",
    "    dataset_path=dataset_path,\n",
    "    x_pipeline=x_pipeline,\n",
    "    y_pipeline=None,\n",
    "    model=model,\n",
    "    experiment_params=train_params,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Run the experiment\n",
    "runner = ExperimentRunner(configs=[config])\n",
    "dataset, model_manager = runner.run()\n",
    "\n",
    "# Access the trained model\n",
    "trained_model = model_manager.models[0].model\n",
    "\n",
    "# Evaluate the model\n",
    "\n",
    "y_pred = trained_model.predict(dataset.x_test)\n",
    "mse = mean_squared_error(dataset.y_test, y_pred)\n",
    "print(\"Test MSE:\", mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 13:07:18,547 - INFO - ================================================================================\n",
      "2024-11-01 13:07:18,548 - INFO - ### PREPARING DATA ###\n",
      "2024-11-01 13:07:18,549 - INFO - ### LOADING DATASET ###\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Browsing sample_data/mock_data3_classif\n",
      "No train_group file found for sample_data/mock_data3_classif.\n",
      "No test_group file found for sample_data/mock_data3_classif.\n",
      "{'initial_shape': (3093, 1050), 'delimiter': ';', 'numeric_delimiter': '.', 'header_line': 0, 'final_shape': (3093, 1050), 'na_handling': {'strategy': 'abort', 'nb_removed_rows': None, 'removed_rows': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 13:07:20,844 - INFO - Dataset(x_train:(3093, 1050) - y_train:(3093, 1), x_test:(732, 1050) - y_test:(732, 1))\n",
      "2024-11-01 13:07:20,844 - INFO - ### PROCESSING DATASET ###\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'initial_shape': (732, 1050), 'delimiter': ';', 'numeric_delimiter': '.', 'header_line': 0, 'final_shape': (732, 1050), 'na_handling': {'strategy': 'abort', 'nb_removed_rows': None, 'removed_rows': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 13:07:21,094 - INFO - Dataset(x_train:(3093, 1050) - y_train:(3093, 1), x_test:(732, 1050) - y_test:(732, 1))\n",
      "Folds size: 2062-1031, 2062-1031, 2062-1031\n",
      "2024-11-01 13:07:21,094 - INFO - ### PREPARING MODEL ###\n",
      "2024-11-01 13:07:21,217 - INFO - Running config > {'dataset': 'sample_data/mock_data3_classif', 'x_pipeline': [{'class': 'sklearn.preprocessing.RobustScaler', 'params': {'copy': True, 'quantile_range': [25.0, 75.0], 'unit_variance': False, 'with_centering': True, 'with_scaling': True}}, {'split': {'class': 'sklearn.model_selection.RepeatedKFold', 'params': {'cv': {'class': 'sklearn.model_selection.KFold', 'params': None}, 'n_repeats': 1, 'random_state': None, 'cvargs': {'n_splits': 3}}}}, {'class': 'sklearn.preprocessing.MinMaxScaler', 'params': {'clip': False, 'copy': True, 'feature_range': [0, 1]}}], 'y_pipeline': None, 'model': {'function': 'pinard.presets.ref_models.bacon_classification'}, 'experiment': {'action': 'finetune', 'task': 'classification', 'finetune_params': {'n_trials': 5, 'model_params': {'filters_1': [8, 16, 32, 64], 'filters_2': [8, 16, 32, 64], 'filters_3': [8, 16, 32, 64]}}, 'training_params': {'epochs': 5, 'verbose': 0, 'loss': 'sparse_categorical_crossentropy'}, 'metrics': ['accuracy'], 'num_classes': 21}, 'seed': 246918912}\n",
      "2024-11-01 13:07:21,218 - INFO - Starting new experiment experiment_759c74af.\n",
      "2024-11-01 13:07:21,219 - INFO - Experiment prepared at results\\sample_datamock_data3_classif\\bacon_classification\\experiment_759c74af\n",
      "2024-11-01 13:07:21,220 - INFO - Finetuning the model\n",
      "[I 2024-11-01 13:07:21,328] A new study created in memory with name: no-name-307c7b51-d209-43d3-9b29-30f5ee1a93ba\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model cloned\n",
      "Model cloned\n",
      "Using framework: tensorflow\n",
      "Model cloned\n",
      "Model cloned\n",
      "Using framework: tensorflow\n",
      "Training fold with shapes: (3093, 1050, 1) (3093, 1) (732, 1050, 1) (732, 1)\n",
      "sparse_categorical_crossentropy ['accuracy']\n",
      "Training fold with shapes: (3093, 1050, 1) (3093, 1) (732, 1050, 1) (732, 1)\n",
      "sparse_categorical_crossentropy ['accuracy']\n",
      "Training fold with shapes: (3093, 1050, 1) (3093, 1) (732, 1050, 1) (732, 1)\n",
      "sparse_categorical_crossentropy ['accuracy']\n",
      "Training fold with shapes: (3093, 1050, 1) (3093, 1) (732, 1050, 1) (732, 1)\n",
      "sparse_categorical_crossentropy ['accuracy']\n",
      "Training with shapes: (3093, 1050, 1) (3093,) (732, 1050, 1) (732,)\n",
      "Training fold with shapes: (3093, 1050, 1) (3093, 1) (732, 1050, 1) (732, 1)\n",
      "sparse_categorical_crossentropy ['accuracy']\n",
      "Training with shapes: (3093, 1050, 1) (3093,) (732, 1050, 1) (732,)\n",
      "Training with shapes: (3093, 1050, 1) (3093,) (732, 1050, 1) (732,)\n",
      "Training with shapes: (3093, 1050, 1) (3093,) (732, 1050, 1) (732,)\n",
      "Training with shapes: (3093, 1050, 1) (3093,) (732, 1050, 1) (732,)\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-01 13:07:33,148] Trial 0 finished with value: 0.07103825136612021 and parameters: {'filters_1': 16, 'filters_2': 64, 'filters_3': 32}. Best is trial 0 with value: 0.07103825136612021.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-01 13:07:33,439] Trial 1 finished with value: 0.06830601092896176 and parameters: {'filters_1': 16, 'filters_2': 8, 'filters_3': 16}. Best is trial 1 with value: 0.06830601092896176.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-01 13:07:33,806] Trial 3 finished with value: 0.06420765027322405 and parameters: {'filters_1': 32, 'filters_2': 64, 'filters_3': 64}. Best is trial 3 with value: 0.06420765027322405.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-01 13:07:33,934] Trial 2 finished with value: 0.06967213114754098 and parameters: {'filters_1': 64, 'filters_2': 64, 'filters_3': 8}. Best is trial 3 with value: 0.06420765027322405.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-01 13:07:34,055] Trial 4 finished with value: 0.09289617486338798 and parameters: {'filters_1': 64, 'filters_2': 64, 'filters_3': 16}. Best is trial 3 with value: 0.06420765027322405.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model params: {'filters_1': 32, 'filters_2': 64, 'filters_3': 64}\n",
      "Model cloned\n",
      "Model cloned\n",
      "Training fold with shapes: (2062, 1050, 1) (2062, 1) (1031, 1050, 1) (1031, 1)\n",
      "sparse_categorical_crossentropy None\n",
      "Training with shapes: (2062, 1050, 1) (2062,) (1031, 1050, 1) (1031,)\n",
      "Training fold with shapes: (2062, 1050, 1) (2062, 1) (1031, 1050, 1) (1031, 1)\n",
      "sparse_categorical_crossentropy None\n",
      "Training with shapes: (2062, 1050, 1) (2062,) (1031, 1050, 1) (1031,)\n",
      "Training fold with shapes: (2062, 1050, 1) (2062, 1) (1031, 1050, 1) (1031, 1)\n",
      "sparse_categorical_crossentropy None\n",
      "Training with shapes: (2062, 1050, 1) (2062,) (1031, 1050, 1) (1031,)\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 13:07:45,849 - INFO - Evaluation Metrics fold_0: {'accuracy': 0.09016393442622951}\n",
      "2024-11-01 13:07:45,850 - INFO - Evaluation Metrics fold_1: {'accuracy': 0.13934426229508196}\n",
      "2024-11-01 13:07:45,851 - INFO - Evaluation Metrics fold_2: {'accuracy': 0.12021857923497267}\n",
      "2024-11-01 13:07:45,852 - INFO - Evaluation Metrics mean: {'accuracy': 0.10245901639344263}\n",
      "2024-11-01 13:07:45,853 - INFO - Evaluation Metrics best: {'accuracy': 0.13934426229508196}\n",
      "2024-11-01 13:07:45,853 - INFO - Evaluation Metrics weighted: {'accuracy': 0.1051912568306011}\n",
      "2024-11-01 13:07:45,855 - INFO - Metrics saved to results\\sample_datamock_data3_classif\\bacon_classification\\experiment_759c74af\\metrics.json\n",
      "2024-11-01 13:07:45,856 - INFO - Best parameters {'filters_1': 32, 'filters_2': 64, 'filters_3': 64} saved to results\\sample_datamock_data3_classif\\bacon_classification\\experiment_759c74af\\best_params.json\n",
      "2024-11-01 13:07:45,859 - INFO - Predictions saved to results\\sample_datamock_data3_classif\\bacon_classification\\experiment_759c74af\\predictions.csv\n",
      "2024-11-01 13:07:45,867 - INFO - Updated experiments at results\\sample_datamock_data3_classif\\bacon_classification\\experiments.json\n",
      "2024-11-01 13:07:45,873 - INFO - Updated experiments at results\\sample_datamock_data3_classif\\experiments.json\n",
      "2024-11-01 13:07:45,875 - INFO - Updated experiments at results\\sample_datamock_data3_classif\\bacon_classification\\experiments.json and results\\sample_datamock_data3_classif\\experiments.json\n",
      "2024-11-01 13:07:45,877 - INFO - ================================================================================\n",
      "2024-11-01 13:07:45,878 - INFO - ### PREPARING DATA ###\n",
      "2024-11-01 13:07:45,879 - INFO - ### LOADING DATASET ###\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [0.2578125 0.3984375 0.34375  ]\n",
      ">> Browsing sample_data/YamMould\n",
      "No train_group file found for sample_data/YamMould.\n",
      "No test_group file found for sample_data/YamMould.\n",
      "{'initial_shape': (359, 1050), 'delimiter': ';', 'numeric_delimiter': '.', 'header_line': 0, 'final_shape': (359, 1050), 'na_handling': {'strategy': 'abort', 'nb_removed_rows': None, 'removed_rows': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 13:07:48,244 - INFO - Dataset(x_train:(359, 1050) - y_train:(359, 1), x_test:(222, 1050) - y_test:(222, 1))\n",
      "2024-11-01 13:07:48,245 - INFO - ### PROCESSING DATASET ###\n",
      "2024-11-01 13:07:48,359 - INFO - Dataset(x_train:(359, 1050) - y_train:(359, 1), x_test:(222, 1050) - y_test:(222, 1))\n",
      "Folds size: 239-120, 239-120, 240-119\n",
      "2024-11-01 13:07:48,361 - INFO - ### PREPARING MODEL ###\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'initial_shape': (222, 1050), 'delimiter': ';', 'numeric_delimiter': '.', 'header_line': 0, 'final_shape': (222, 1050), 'na_handling': {'strategy': 'abort', 'nb_removed_rows': None, 'removed_rows': None}}\n",
      "Model cloned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 13:07:48,471 - INFO - Running config > {'dataset': 'sample_data/YamMould', 'x_pipeline': [{'class': 'sklearn.preprocessing.RobustScaler', 'params': {'copy': True, 'quantile_range': [25.0, 75.0], 'unit_variance': False, 'with_centering': True, 'with_scaling': True}}, {'split': {'class': 'sklearn.model_selection.RepeatedKFold', 'params': {'cv': {'class': 'sklearn.model_selection.KFold', 'params': None}, 'n_repeats': 1, 'random_state': None, 'cvargs': {'n_splits': 3}}}}, {'class': 'sklearn.preprocessing.MinMaxScaler', 'params': {'clip': False, 'copy': True, 'feature_range': [0, 1]}}], 'y_pipeline': None, 'model': {'function': 'pinard.presets.ref_models.bacon_classification'}, 'experiment': {'action': 'finetune', 'task': 'classification', 'finetune_params': {'n_trials': 5, 'model_params': {'filters_1': [8, 16, 32, 64], 'filters_2': [8, 16, 32, 64], 'filters_3': [8, 16, 32, 64]}}, 'training_params': {'epochs': 5, 'verbose': 0, 'loss': 'sparse_categorical_crossentropy'}, 'metrics': ['accuracy'], 'num_classes': 2}, 'seed': 246918912}\n",
      "2024-11-01 13:07:48,472 - INFO - Starting new experiment experiment_18de06fd.\n",
      "2024-11-01 13:07:48,473 - INFO - Experiment prepared at results\\sample_dataYamMould\\bacon_classification\\experiment_18de06fd\n",
      "2024-11-01 13:07:48,474 - INFO - Finetuning the model\n",
      "[I 2024-11-01 13:07:48,586] A new study created in memory with name: no-name-68fc8f80-f850-441b-8df3-a5d0d21583b9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model cloned\n",
      "Using framework: tensorflow\n",
      "Model cloned\n",
      "Model cloned\n",
      "Using framework: tensorflow\n",
      "Training fold with shapes: (359, 1050, 1) (359, 1) (222, 1050, 1) (222, 1)\n",
      "sparse_categorical_crossentropy ['accuracy']\n",
      "Training with shapes: (359, 1050, 1) (359,) (222, 1050, 1) (222,)\n",
      "Training fold with shapes: (359, 1050, 1) (359, 1) (222, 1050, 1) (222, 1)\n",
      "sparse_categorical_crossentropy ['accuracy']\n",
      "Training fold with shapes: (359, 1050, 1) (359, 1) (222, 1050, 1) (222, 1)\n",
      "sparse_categorical_crossentropy ['accuracy']\n",
      "Training fold with shapes: (359, 1050, 1) (359, 1) (222, 1050, 1) (222, 1)\n",
      "sparse_categorical_crossentropy ['accuracy']\n",
      "Training with shapes: (359, 1050, 1) (359,) (222, 1050, 1) (222,)\n",
      "Training with shapes: (359, 1050, 1) (359,) (222, 1050, 1) (222,)\n",
      "Training with shapes: (359, 1050, 1) (359,) (222, 1050, 1) (222,)\n",
      "Training fold with shapes: (359, 1050, 1) (359, 1) (222, 1050, 1) (222, 1)\n",
      "sparse_categorical_crossentropy ['accuracy']\n",
      "Training with shapes: (359, 1050, 1) (359,) (222, 1050, 1) (222,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-11-01 13:07:55,647] Trial 2 failed with parameters: {'filters_1': 16, 'filters_2': 8, 'filters_3': 8} because of the following error: InvalidArgumentError().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"d:\\workspace\\ml\\nirs\\pinard\\pinard\\core\\finetuner.py\", line 74, in objective\n",
      "    finetune_model_manager.train(dataset, training_params=training_params, metrics=metrics, no_folds=True)\n",
      "  File \"d:\\workspace\\ml\\nirs\\pinard\\pinard\\core\\model_manager.py\", line 451, in train\n",
      "    model.fit(x_train, y_train,\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 59, in quick_execute\n",
      "    except TypeError as e:\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:\n",
      "\n",
      "Detected at node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n",
      "  File \"c:\\ProgramData\\miniconda3\\Lib\\threading.py\", line 1030, in _bootstrap\n",
      "\n",
      "  File \"c:\\ProgramData\\miniconda3\\Lib\\threading.py\", line 1073, in _bootstrap_inner\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "\n",
      "  File \"c:\\ProgramData\\miniconda3\\Lib\\threading.py\", line 1010, in run\n",
      "\n",
      "  File \"c:\\ProgramData\\miniconda3\\Lib\\concurrent\\futures\\thread.py\", line 92, in _worker\n",
      "\n",
      "  File \"c:\\ProgramData\\miniconda3\\Lib\\concurrent\\futures\\thread.py\", line 58, in run\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py\", line 160, in _optimize_sequential\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "\n",
      "  File \"d:\\workspace\\ml\\nirs\\pinard\\pinard\\core\\finetuner.py\", line 74, in objective\n",
      "\n",
      "  File \"d:\\workspace\\ml\\nirs\\pinard\\pinard\\core\\model_manager.py\", line 451, in train\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 320, in fit\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 121, in one_step_on_iterator\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 108, in one_step_on_data\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 54, in train_step\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\trainer.py\", line 359, in _compute_loss\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\trainer.py\", line 327, in compute_loss\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\compile_utils.py\", line 611, in __call__\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\compile_utils.py\", line 652, in call\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\losses\\loss.py\", line 60, in __call__\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\losses\\losses.py\", line 27, in call\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\losses\\losses.py\", line 1870, in sparse_categorical_crossentropy\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\ops\\nn.py\", line 1559, in sparse_categorical_crossentropy\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py\", line 671, in sparse_categorical_crossentropy\n",
      "\n",
      "Received a label value of 1 which is outside the valid range of [0, 1).  Label values: 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0\n",
      "\t [[{{node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_one_step_on_iterator_591768]\n",
      "[W 2024-11-01 13:07:55,677] Trial 2 failed with value None.\n",
      "[W 2024-11-01 13:07:55,763] Trial 0 failed with parameters: {'filters_1': 64, 'filters_2': 64, 'filters_3': 8} because of the following error: InvalidArgumentError().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"d:\\workspace\\ml\\nirs\\pinard\\pinard\\core\\finetuner.py\", line 74, in objective\n",
      "    finetune_model_manager.train(dataset, training_params=training_params, metrics=metrics, no_folds=True)\n",
      "  File \"d:\\workspace\\ml\\nirs\\pinard\\pinard\\core\\model_manager.py\", line 451, in train\n",
      "    model.fit(x_train, y_train,\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 59, in quick_execute\n",
      "    except TypeError as e:\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:\n",
      "\n",
      "Detected at node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n",
      "  File \"c:\\ProgramData\\miniconda3\\Lib\\threading.py\", line 1030, in _bootstrap\n",
      "\n",
      "  File \"c:\\ProgramData\\miniconda3\\Lib\\threading.py\", line 1073, in _bootstrap_inner\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "\n",
      "  File \"c:\\ProgramData\\miniconda3\\Lib\\threading.py\", line 1010, in run\n",
      "\n",
      "  File \"c:\\ProgramData\\miniconda3\\Lib\\concurrent\\futures\\thread.py\", line 92, in _worker\n",
      "\n",
      "  File \"c:\\ProgramData\\miniconda3\\Lib\\concurrent\\futures\\thread.py\", line 58, in run\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py\", line 160, in _optimize_sequential\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "\n",
      "  File \"d:\\workspace\\ml\\nirs\\pinard\\pinard\\core\\finetuner.py\", line 74, in objective\n",
      "\n",
      "  File \"d:\\workspace\\ml\\nirs\\pinard\\pinard\\core\\model_manager.py\", line 451, in train\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 320, in fit\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 121, in one_step_on_iterator\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 108, in one_step_on_data\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 54, in train_step\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\trainer.py\", line 359, in _compute_loss\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\trainer.py\", line 327, in compute_loss\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\compile_utils.py\", line 611, in __call__\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\compile_utils.py\", line 652, in call\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\losses\\loss.py\", line 60, in __call__\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\losses\\losses.py\", line 27, in call\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\losses\\losses.py\", line 1870, in sparse_categorical_crossentropy\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\ops\\nn.py\", line 1559, in sparse_categorical_crossentropy\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py\", line 671, in sparse_categorical_crossentropy\n",
      "\n",
      "Received a label value of 1 which is outside the valid range of [0, 1).  Label values: 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 1 1 0\n",
      "\t [[{{node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_one_step_on_iterator_591841]\n",
      "[W 2024-11-01 13:07:55,776] Trial 0 failed with value None.\n",
      "[W 2024-11-01 13:07:55,853] Trial 1 failed with parameters: {'filters_1': 64, 'filters_2': 64, 'filters_3': 16} because of the following error: InvalidArgumentError().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"d:\\workspace\\ml\\nirs\\pinard\\pinard\\core\\finetuner.py\", line 74, in objective\n",
      "    finetune_model_manager.train(dataset, training_params=training_params, metrics=metrics, no_folds=True)\n",
      "  File \"d:\\workspace\\ml\\nirs\\pinard\\pinard\\core\\model_manager.py\", line 451, in train\n",
      "    model.fit(x_train, y_train,\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 59, in quick_execute\n",
      "    except TypeError as e:\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:\n",
      "\n",
      "Detected at node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n",
      "  File \"c:\\ProgramData\\miniconda3\\Lib\\threading.py\", line 1030, in _bootstrap\n",
      "\n",
      "  File \"c:\\ProgramData\\miniconda3\\Lib\\threading.py\", line 1073, in _bootstrap_inner\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "\n",
      "  File \"c:\\ProgramData\\miniconda3\\Lib\\threading.py\", line 1010, in run\n",
      "\n",
      "  File \"c:\\ProgramData\\miniconda3\\Lib\\concurrent\\futures\\thread.py\", line 92, in _worker\n",
      "\n",
      "  File \"c:\\ProgramData\\miniconda3\\Lib\\concurrent\\futures\\thread.py\", line 58, in run\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py\", line 160, in _optimize_sequential\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "\n",
      "  File \"d:\\workspace\\ml\\nirs\\pinard\\pinard\\core\\finetuner.py\", line 74, in objective\n",
      "\n",
      "  File \"d:\\workspace\\ml\\nirs\\pinard\\pinard\\core\\model_manager.py\", line 451, in train\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 320, in fit\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 121, in one_step_on_iterator\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 108, in one_step_on_data\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 54, in train_step\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\trainer.py\", line 359, in _compute_loss\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\trainer.py\", line 327, in compute_loss\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\compile_utils.py\", line 611, in __call__\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\compile_utils.py\", line 652, in call\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\losses\\loss.py\", line 60, in __call__\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\losses\\losses.py\", line 27, in call\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\losses\\losses.py\", line 1870, in sparse_categorical_crossentropy\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\ops\\nn.py\", line 1559, in sparse_categorical_crossentropy\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py\", line 671, in sparse_categorical_crossentropy\n",
      "\n",
      "Received a label value of 1 which is outside the valid range of [0, 1).  Label values: 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 1\n",
      "\t [[{{node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_one_step_on_iterator_592090]\n",
      "[W 2024-11-01 13:07:55,876] Trial 1 failed with value None.\n",
      "[W 2024-11-01 13:07:56,469] Trial 4 failed with parameters: {'filters_1': 8, 'filters_2': 32, 'filters_3': 64} because of the following error: InvalidArgumentError().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"d:\\workspace\\ml\\nirs\\pinard\\pinard\\core\\finetuner.py\", line 74, in objective\n",
      "    finetune_model_manager.train(dataset, training_params=training_params, metrics=metrics, no_folds=True)\n",
      "  File \"d:\\workspace\\ml\\nirs\\pinard\\pinard\\core\\model_manager.py\", line 451, in train\n",
      "    model.fit(x_train, y_train,\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 59, in quick_execute\n",
      "    except TypeError as e:\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:\n",
      "\n",
      "Detected at node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n",
      "  File \"c:\\ProgramData\\miniconda3\\Lib\\threading.py\", line 1030, in _bootstrap\n",
      "\n",
      "  File \"c:\\ProgramData\\miniconda3\\Lib\\threading.py\", line 1073, in _bootstrap_inner\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "\n",
      "  File \"c:\\ProgramData\\miniconda3\\Lib\\threading.py\", line 1010, in run\n",
      "\n",
      "  File \"c:\\ProgramData\\miniconda3\\Lib\\concurrent\\futures\\thread.py\", line 92, in _worker\n",
      "\n",
      "  File \"c:\\ProgramData\\miniconda3\\Lib\\concurrent\\futures\\thread.py\", line 58, in run\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py\", line 160, in _optimize_sequential\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "\n",
      "  File \"d:\\workspace\\ml\\nirs\\pinard\\pinard\\core\\finetuner.py\", line 74, in objective\n",
      "\n",
      "  File \"d:\\workspace\\ml\\nirs\\pinard\\pinard\\core\\model_manager.py\", line 451, in train\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 320, in fit\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 121, in one_step_on_iterator\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 108, in one_step_on_data\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 54, in train_step\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\trainer.py\", line 359, in _compute_loss\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\trainer.py\", line 327, in compute_loss\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\compile_utils.py\", line 611, in __call__\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\compile_utils.py\", line 652, in call\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\losses\\loss.py\", line 60, in __call__\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\losses\\losses.py\", line 27, in call\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\losses\\losses.py\", line 1870, in sparse_categorical_crossentropy\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\ops\\nn.py\", line 1559, in sparse_categorical_crossentropy\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py\", line 671, in sparse_categorical_crossentropy\n",
      "\n",
      "Received a label value of 1 which is outside the valid range of [0, 1).  Label values: 1 1 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1\n",
      "\t [[{{node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_one_step_on_iterator_592980]\n",
      "[W 2024-11-01 13:07:56,471] Trial 4 failed with value None.\n",
      "[W 2024-11-01 13:07:56,634] Trial 3 failed with parameters: {'filters_1': 32, 'filters_2': 64, 'filters_3': 8} because of the following error: InvalidArgumentError().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"d:\\workspace\\ml\\nirs\\pinard\\pinard\\core\\finetuner.py\", line 74, in objective\n",
      "    finetune_model_manager.train(dataset, training_params=training_params, metrics=metrics, no_folds=True)\n",
      "  File \"d:\\workspace\\ml\\nirs\\pinard\\pinard\\core\\model_manager.py\", line 451, in train\n",
      "    model.fit(x_train, y_train,\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 59, in quick_execute\n",
      "    except TypeError as e:\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:\n",
      "\n",
      "Detected at node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n",
      "  File \"c:\\ProgramData\\miniconda3\\Lib\\threading.py\", line 1030, in _bootstrap\n",
      "\n",
      "  File \"c:\\ProgramData\\miniconda3\\Lib\\threading.py\", line 1073, in _bootstrap_inner\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "\n",
      "  File \"c:\\ProgramData\\miniconda3\\Lib\\threading.py\", line 1010, in run\n",
      "\n",
      "  File \"c:\\ProgramData\\miniconda3\\Lib\\concurrent\\futures\\thread.py\", line 92, in _worker\n",
      "\n",
      "  File \"c:\\ProgramData\\miniconda3\\Lib\\concurrent\\futures\\thread.py\", line 58, in run\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py\", line 160, in _optimize_sequential\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "\n",
      "  File \"d:\\workspace\\ml\\nirs\\pinard\\pinard\\core\\finetuner.py\", line 74, in objective\n",
      "\n",
      "  File \"d:\\workspace\\ml\\nirs\\pinard\\pinard\\core\\model_manager.py\", line 451, in train\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 320, in fit\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 121, in one_step_on_iterator\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 108, in one_step_on_data\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 54, in train_step\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\trainer.py\", line 359, in _compute_loss\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\trainer.py\", line 327, in compute_loss\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\compile_utils.py\", line 611, in __call__\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\compile_utils.py\", line 652, in call\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\losses\\loss.py\", line 60, in __call__\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\losses\\losses.py\", line 27, in call\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\losses\\losses.py\", line 1870, in sparse_categorical_crossentropy\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\ops\\nn.py\", line 1559, in sparse_categorical_crossentropy\n",
      "\n",
      "  File \"C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py\", line 671, in sparse_categorical_crossentropy\n",
      "\n",
      "Received a label value of 1 which is outside the valid range of [0, 1).  Label values: 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1\n",
      "\t [[{{node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_one_step_on_iterator_593272]\n",
      "[W 2024-11-01 13:07:56,635] Trial 3 failed with value None.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No trials are completed yet.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 193\u001b[0m\n\u001b[0;32m    191\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    192\u001b[0m runner \u001b[38;5;241m=\u001b[39m ExperimentRunner(configs, resume_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrestart\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 193\u001b[0m dataset, model_manager \u001b[38;5;241m=\u001b[39m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime elapsed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;241m-\u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\workspace\\ml\\nirs\\pinard\\pinard\\core\\runner.py:113\u001b[0m, in \u001b[0;36mExperimentRunner.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train(model_manager, dataset, training_params, metrics, task)\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m action \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinetune\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 113\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fine_tune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinetune_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll experiments completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset, model_manager\n",
      "File \u001b[1;32md:\\workspace\\ml\\nirs\\pinard\\pinard\\core\\runner.py:327\u001b[0m, in \u001b[0;36mExperimentRunner._fine_tune\u001b[1;34m(self, model_manager, dataset, finetune_params, training_params, metrics, task)\u001b[0m\n\u001b[0;32m    325\u001b[0m finetuner_type \u001b[38;5;241m=\u001b[39m finetune_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtuner\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptuna\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    326\u001b[0m finetuner \u001b[38;5;241m=\u001b[39m FineTunerFactory\u001b[38;5;241m.\u001b[39mget_fine_tuner(finetuner_type, model_manager)\n\u001b[1;32m--> 327\u001b[0m best_params \u001b[38;5;241m=\u001b[39m \u001b[43mfinetuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinetune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinetune_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m model_manager\u001b[38;5;241m.\u001b[39msave_model(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanager\u001b[38;5;241m.\u001b[39mexperiment_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate_and_save_results(model_manager, dataset, metrics, best_params, task\u001b[38;5;241m=\u001b[39mtask)\n",
      "File \u001b[1;32md:\\workspace\\ml\\nirs\\pinard\\pinard\\core\\finetuner.py:97\u001b[0m, in \u001b[0;36mOptunaFineTuner.finetune\u001b[1;34m(self, dataset, finetune_params, src_training_params, metrics, task)\u001b[0m\n\u001b[0;32m     94\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     95\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(objective, n_trials\u001b[38;5;241m=\u001b[39mfinetune_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_trials\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m100\u001b[39m), n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 97\u001b[0m best_params \u001b[38;5;241m=\u001b[39m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_params\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# print(f\"Best hyperparameters: {best_params}\")\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Update the model_manager with the best model\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# best_trial = study.best_trial\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Rebuild the model with best parameters\u001b[39;00m\n\u001b[0;32m    103\u001b[0m best_model_params \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\study.py:119\u001b[0m, in \u001b[0;36mStudy.best_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbest_params\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m    109\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return parameters of the best trial in the study.\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    .. note::\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    116\u001b[0m \n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_trial\u001b[49m\u001b[38;5;241m.\u001b[39mparams\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\study.py:162\u001b[0m, in \u001b[0;36mStudy.best_trial\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_multi_objective():\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA single best trial cannot be retrieved from a multi-objective study. Consider \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing Study.best_trials to retrieve a list containing the best trials.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m     )\n\u001b[1;32m--> 162\u001b[0m best_trial \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_study_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# If the trial with the best value is infeasible, select the best trial from all feasible\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m# trials. Note that the behavior is undefined when constrained optimization without the\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# violation value in the best-valued trial.\u001b[39;00m\n\u001b[0;32m    167\u001b[0m constraints \u001b[38;5;241m=\u001b[39m best_trial\u001b[38;5;241m.\u001b[39msystem_attrs\u001b[38;5;241m.\u001b[39mget(_CONSTRAINTS_KEY)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\storages\\_in_memory.py:232\u001b[0m, in \u001b[0;36mInMemoryStorage.get_best_trial\u001b[1;34m(self, study_id)\u001b[0m\n\u001b[0;32m    229\u001b[0m best_trial_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_studies[study_id]\u001b[38;5;241m.\u001b[39mbest_trial_id\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_trial_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo trials are completed yet.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_studies[study_id]\u001b[38;5;241m.\u001b[39mdirections) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial can be obtained only for single-objective optimization.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    236\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: No trials are completed yet."
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "\n",
    "import time\n",
    "from pinard.presets.ref_models import decon, bacon, customizable_bacon, bacon_classification\n",
    "from pinard.presets.preprocessings import decon_set, bacon_set\n",
    "from pinard.data_splitters import KennardStoneSplitter\n",
    "from pinard.transformations import StandardNormalVariate as SNV, SavitzkyGolay as SG, Gaussian as GS, Derivate as  Dv\n",
    "from pinard.transformations import Rotate_Translate as RT, Spline_X_Simplification as SXS, Random_X_Operation as RXO\n",
    "from pinard.transformations import CropTransformer\n",
    "from pinard.core.runner import ExperimentRunner\n",
    "from pinard.core.config import Config\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, RepeatedKFold, StratifiedKFold, RepeatedStratifiedKFold, ShuffleSplit, GroupKFold, StratifiedShuffleSplit, BaseCrossValidator, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "model_sklearn = {\n",
    "    \"class\": \"sklearn.cross_decomposition.PLSRegression\",\n",
    "    \"model_params\": {\n",
    "        \"n_components\": 21,\n",
    "    }\n",
    "}\n",
    "    \n",
    "finetune_pls_experiment = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"finetune_params\": {\n",
    "        'model_params': {\n",
    "            'n_components': ('int', 5, 20),\n",
    "        },\n",
    "        'training_params': {},\n",
    "        'tuner': 'sklearn'\n",
    "    }\n",
    "}\n",
    "\n",
    "bacon_train = {\"action\": \"train\", \"training_params\": {\"epochs\": 2000, \"batch_size\": 500, \"patience\": 200, \"cyclic_lr\": True, \"base_lr\": 1e-6, \"max_lr\": 1e-3, \"step_size\": 400}}\n",
    "bacon_train_short = {\"action\": \"train\", \"training_params\": {\"epochs\": 10, \"batch_size\": 500, \"patience\": 20, \"cyclic_lr\": True, \"base_lr\": 1e-6, \"max_lr\": 1e-3, \"step_size\": 40}}\n",
    "bacon_finetune = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"finetune_params\": {\n",
    "        \"n_trials\": 5,\n",
    "        \"model_params\": {\n",
    "            \"filters_1\": [8, 16, 32, 64], \n",
    "            \"filters_2\": [8, 16, 32, 64], \n",
    "            \"filters_3\": [8, 16, 32, 64]\n",
    "        }\n",
    "    },\n",
    "    \"training_params\": {\n",
    "        \"epochs\": 10,\n",
    "        \"verbose\":0\n",
    "    }\n",
    "}\n",
    "\n",
    "full_bacon_finetune = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"training_params\": {\n",
    "        \"epochs\": 500,\n",
    "        \"patience\": 100,\n",
    "    },\n",
    "    \"finetune_params\": {\n",
    "        \"nb_trials\": 150,\n",
    "        \"model_params\": {\n",
    "            'spatial_dropout': (float, 0.01, 0.5),\n",
    "            'filters1': [4, 8, 16, 32, 64, 128, 256],\n",
    "            'kernel_size1': [3, 5, 7, 9, 11, 13, 15],\n",
    "            # 'strides1': [1, 2, 3, 4, 5],\n",
    "            # 'activation1': ['relu', 'selu', 'elu', 'swish'],\n",
    "            'dropout_rate': (float, 0.01, 0.5),\n",
    "            'filters2': [4, 8, 16, 32, 64, 128, 256],\n",
    "            # 'kernel_size2': [3, 5, 7, 9, 11, 13, 15],\n",
    "            # 'strides2': [1, 2, 3, 4, 5],\n",
    "            'activation2': ['relu', 'selu', 'elu', 'swish'],\n",
    "            'normalization_method1': ['BatchNormalization', 'LayerNormalization'],\n",
    "            'filters3': [4, 8, 16, 32, 64, 128, 256],\n",
    "            # 'kernel_size3': [3, 5, 7, 9, 11, 13, 15],\n",
    "            # 'strides3': [1, 2, 3, 4, 5],\n",
    "            'activation3': ['relu', 'selu', 'elu', 'swish'],\n",
    "            # 'normalization_method2': ['BatchNormalization', 'LayerNormalization'],\n",
    "            # 'dense_units': [4, 8, 16, 32, 64, 128, 256],\n",
    "            'dense_activation': ['relu', 'selu', 'elu', 'swish'],\n",
    "        },\n",
    "        # \"training_params\": {\n",
    "        #     \"batch_size\": [32, 64, 128, 256, 512],\n",
    "        #     \"cyclic_lr\": [True, False],\n",
    "        #     \"base_lr\": (float, 1e-6, 1e-2),\n",
    "        #     \"max_lr\": (float, 1e-3, 1e-1),\n",
    "        #     \"step_size\": (int, 500, 5000),\n",
    "        # },\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "x_pipeline_full = [\n",
    "    RobustScaler(),\n",
    "    {\"samples\": [None, None,None,None,SXS, RXO]},\n",
    "    {\"split\": RepeatedKFold(n_splits=3, n_repeats=1)},\n",
    "    {\"features\": [None, GS(2,1), SG, SNV, Dv, [GS, SNV], [GS, GS],[GS, SG],[SG, SNV], [GS, Dv], [SG, Dv]]},\n",
    "    MinMaxScaler()\n",
    "]\n",
    "\n",
    "\n",
    "bacon_finetune_classif = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"task\": \"classification\",\n",
    "    \"finetune_params\": {\n",
    "        \"n_trials\": 5,\n",
    "        \"model_params\": {\n",
    "            \"filters_1\": [8, 16, 32, 64], \n",
    "            \"filters_2\": [8, 16, 32, 64], \n",
    "            \"filters_3\": [8, 16, 32, 64]\n",
    "        }\n",
    "    },\n",
    "    \"training_params\": {\n",
    "        \"epochs\": 5,\n",
    "        \"verbose\":0\n",
    "    }\n",
    "}\n",
    "\n",
    "finetune_randomForestclassifier = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"task\": \"classification\",\n",
    "    \"finetune_params\": {\n",
    "        'model_params': {\n",
    "            'n_estimators': ('int', 5, 20),\n",
    "        },\n",
    "        'training_params': {},\n",
    "        'tuner': 'sklearn'\n",
    "    }\n",
    "}\n",
    "\n",
    "x_pipeline_PLS = [\n",
    "    RobustScaler(),\n",
    "    {\"split\": RepeatedKFold(n_splits=3, n_repeats=1)},\n",
    "    {\"features\": [None, GS(2,1), SG, SNV, Dv, [GS, SNV], [GS, GS],[GS, SG],[SG, SNV], [GS, Dv], [SG, Dv]]},\n",
    "    MinMaxScaler()\n",
    "]\n",
    "            \n",
    "            \n",
    "x_pipeline = [\n",
    "    RobustScaler(), \n",
    "    {\"split\": RepeatedKFold(n_splits=3, n_repeats=1)}, \n",
    "    # bacon_set(),\n",
    "    MinMaxScaler()\n",
    "]\n",
    "\n",
    "x_pipelineb = [\n",
    "    RobustScaler(), \n",
    "    {\"samples\": [RT(6)], \"balance\": True},\n",
    "    # {\"samples\": [None, RT]},\n",
    "    {\"split\": RepeatedKFold(n_splits=3, n_repeats=1)}, \n",
    "    bacon_set(),\n",
    "    MinMaxScaler()\n",
    "]\n",
    "\n",
    "\n",
    "seed = 123459456\n",
    "\n",
    "datasets = \"sample_data/mock_data3_classif\"\n",
    "y_pipeline = MinMaxScaler()\n",
    "# processing only\n",
    "config1 = Config(\"sample_data/Malaria2024\", x_pipeline_full, y_pipeline, None, None, seed)\n",
    "## TRAINING\n",
    "# regression\n",
    "config2 = Config(\"sample_data/mock_data2\", x_pipeline, y_pipeline, bacon, bacon_train_short, seed)\n",
    "config3 = Config(\"sample_data/mock_data3\", x_pipeline_PLS, y_pipeline, model_sklearn, None, seed)\n",
    "# classification\n",
    "config4 = Config(\"sample_data/mock_data3_classif\", x_pipeline, None, bacon_classification, {\"task\":\"classification\", \"training_params\":{\"epochs\":10, \"patience\": 100, \"verbose\":0}}, seed*2)\n",
    "config4b = Config(\"sample_data/YamMould\", x_pipelineb, None, bacon_classification, {\"task\":\"classification\", \"training_params\":{\"epochs\":10, \"patience\": 100, \"verbose\":0}}, seed*2)\n",
    "config5 = Config(\"sample_data/mock_data3_binary\", x_pipeline, None, bacon_classification, {\"task\":\"classification\", \"training_params\":{\"epochs\":5}, \"verbose\":0}, seed*2)\n",
    "config6 = Config(\"sample_data/WhiskyConcentration\", x_pipeline, None, RandomForestClassifier, {\"task\":\"classification\"}, seed*2)\n",
    "config7 = Config(\"sample_data/Malaria2024\", x_pipeline, None, RandomForestClassifier, {\"task\":\"classification\"}, seed*2)\n",
    "## FINETUNING\n",
    "# regression\n",
    "config8 = Config(\"sample_data/mock_data3\", x_pipeline, y_pipeline, bacon, bacon_finetune, seed)\n",
    "config9 = Config(\"sample_data/mock_data3\", x_pipeline, y_pipeline, model_sklearn, finetune_pls_experiment, seed)\n",
    "# classification\n",
    "config10 = Config(\"sample_data/mock_data3_classif\", x_pipeline, None, bacon_classification, bacon_finetune_classif, seed*2)\n",
    "config10b = Config(\"sample_data/YamMould\", x_pipeline, None, bacon_classification, bacon_finetune_classif, seed*2)\n",
    "config11 = Config(\"sample_data/mock_data3_classif\", x_pipeline, None, RandomForestClassifier, finetune_randomForestclassifier, seed*2)\n",
    "config11b = Config(\"sample_data/YamMould\", x_pipeline, None, RandomForestClassifier, finetune_randomForestclassifier, seed*2)\n",
    "\n",
    "\n",
    "configs = [config1, config2, config3, config4, config4b, config5, config6, config7, config8, config9, config10, config10b, config11, config11b]\n",
    "# configs = [config10, config10b]\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "runner = ExperimentRunner(configs, resume_mode=\"restart\")\n",
    "dataset, model_manager = runner.run()\n",
    "end = time.time()\n",
    "print(f\"Time elapsed: {end-start} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinard\n",
    "print(pinard.__version__)\n",
    "\n",
    "# load malaria manually and apply RT transformation manually and display chart of transformed samples\n",
    "\n",
    "config1 = Config(\"sample_data/Malaria2024\", x_pipeline, y_pipeline, None, None, seed)\n",
    "runner = ExperimentRunner([config1], resume_mode=\"restart\")\n",
    "dataset, model_manager = runner.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ace_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mace_tools\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtools\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Sklearn metrics list\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ace_tools'"
     ]
    }
   ],
   "source": [
    "import ace_tools as tools\n",
    "import pandas as pd\n",
    "\n",
    "# Sklearn metrics list\n",
    "sklearn_metrics = [\n",
    "    \"explained_variance\", \"r2\", \"max_error\", \"matthews_corrcoef\",\n",
    "    \"neg_median_absolute_error\", \"neg_mean_absolute_error\",\n",
    "    \"neg_mean_absolute_percentage_error\", \"neg_mean_squared_error\",\n",
    "    \"neg_mean_squared_log_error\", \"neg_root_mean_squared_error\",\n",
    "    \"neg_root_mean_squared_log_error\", \"neg_mean_poisson_deviance\",\n",
    "    \"neg_mean_gamma_deviance\", \"d2_absolute_error_score\", \"accuracy\",\n",
    "    \"top_k_accuracy\", \"roc_auc\", \"roc_auc_ovr\", \"roc_auc_ovo\",\n",
    "    \"roc_auc_ovr_weighted\", \"roc_auc_ovo_weighted\", \"balanced_accuracy\",\n",
    "    \"average_precision\", \"neg_log_loss\", \"neg_brier_score\",\n",
    "    \"positive_likelihood_ratio\", \"neg_negative_likelihood_ratio\",\n",
    "    \"adjusted_rand_score\", \"rand_score\", \"homogeneity_score\",\n",
    "    \"completeness_score\", \"v_measure_score\", \"mutual_info_score\",\n",
    "    \"adjusted_mutual_info_score\", \"normalized_mutual_info_score\",\n",
    "    \"fowlkes_mallows_score\"\n",
    "]\n",
    "\n",
    "# Tensorflow/keras metrics list\n",
    "tensorflow_metrics = [\n",
    "    \"MeanSquaredError\", \"RootMeanSquaredError\", \"MeanAbsoluteError\",\n",
    "    \"MeanAbsolutePercentageError\", \"MeanSquaredLogarithmicError\",\n",
    "    \"CosineSimilarity\", \"LogCoshError\", \"R2Score\", \"AUC\",\n",
    "    \"FalseNegatives\", \"FalsePositives\", \"Precision\", \"PrecisionAtRecall\",\n",
    "    \"Recall\", \"RecallAtPrecision\", \"SensitivityAtSpecificity\",\n",
    "    \"SpecificityAtSensitivity\", \"TrueNegatives\", \"TruePositives\",\n",
    "    \"Hinge\", \"SquaredHinge\", \"CategoricalHinge\", \"KLDivergence\",\n",
    "    \"Poisson\", \"BinaryCrossentropy\", \"CategoricalCrossentropy\",\n",
    "    \"SparseCategoricalCrossentropy\", \"Accuracy\", \"BinaryAccuracy\",\n",
    "    \"CategoricalAccuracy\", \"SparseCategoricalAccuracy\",\n",
    "    \"TopKCategoricalAccuracy\", \"SparseTopKCategoricalAccuracy\",\n",
    "    \"F1Score\", \"FBetaScore\", \"IoU\", \"BinaryIoU\", \"MeanIoU\",\n",
    "    \"OneHotIoU\", \"OneHotMeanIoU\"\n",
    "]\n",
    "\n",
    "# Metric name mapping: (tensorflow_name, sklearn_name, abbreviation, method_name)\n",
    "# Initialize with common names\n",
    "metrics_mapping = [\n",
    "    (\"MeanSquaredError\", \"neg_mean_squared_error\", \"mse\", \"Mean Squared Error\"),\n",
    "    (\"RootMeanSquaredError\", \"neg_root_mean_squared_error\", \"rmse\", \"Root Mean Squared Error\"),\n",
    "    (\"MeanAbsoluteError\", \"neg_mean_absolute_error\", \"mae\", \"Mean Absolute Error\"),\n",
    "    (\"MeanAbsolutePercentageError\", \"neg_mean_absolute_percentage_error\", \"mape\", \"Mean Absolute Percentage Error\"),\n",
    "    (\"MeanSquaredLogarithmicError\", \"neg_mean_squared_log_error\", \"msle\", \"Mean Squared Logarithmic Error\"),\n",
    "    (\"CosineSimilarity\", None, \"cos_sim\", \"Cosine Similarity\"),\n",
    "    (\"LogCoshError\", None, \"log_cosh\", \"Log Cosh Error\"),\n",
    "    (\"R2Score\", \"r2\", \"r2\", \"R2 Score\"),\n",
    "    (\"AUC\", \"roc_auc\", \"auc\", \"Area Under the Curve\"),\n",
    "    (\"Precision\", None, \"prec\", \"Precision\"),\n",
    "    (\"Recall\", None, \"recall\", \"Recall\"),\n",
    "    (\"Accuracy\", \"accuracy\", \"acc\", \"Accuracy\"),\n",
    "    (\"TopKCategoricalAccuracy\", \"top_k_accuracy\", \"top_k_acc\", \"Top K Categorical Accuracy\"),\n",
    "    (\"BinaryCrossentropy\", None, \"bin_crossentropy\", \"Binary Crossentropy\"),\n",
    "    (\"CategoricalCrossentropy\", None, \"cat_crossentropy\", \"Categorical Crossentropy\"),\n",
    "    (\"SparseCategoricalCrossentropy\", None, \"sparse_cat_crossentropy\", \"Sparse Categorical Crossentropy\"),\n",
    "    (\"F1Score\", None, \"f1\", \"F1 Score\"),\n",
    "    (\"IoU\", None, \"iou\", \"Intersection over Union\")\n",
    "]\n",
    "\n",
    "# Add remaining metrics with None in the missing columns\n",
    "for metric in sklearn_metrics:\n",
    "    if not any(metric in row for row in metrics_mapping):\n",
    "        metrics_mapping.append((None, metric, None, None))\n",
    "\n",
    "for metric in tensorflow_metrics:\n",
    "    if not any(metric in row for row in metrics_mapping):\n",
    "        metrics_mapping.append((metric, None, None, None))\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame(metrics_mapping, columns=[\"tensorflow_name\", \"sklearn_name\", \"abbreviation\", \"method_name\"])\n",
    "\n",
    "# Display the dataframe to the user\n",
    "tools.display_dataframe_to_user(name=\"Metric Comparison\", dataframe=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Applying a Simple Data Transformation using Pinard\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pinard.transformations import StandardNormalVariate as SNV\n",
    "from pinard.core.runner import ExperimentRunner\n",
    "from pinard.core.config import Config\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "\n",
    "\n",
    "# Define the dataset path\n",
    "dataset_path = \"sample_data/mock_data\"\n",
    "\n",
    "# Define the data transformation pipeline\n",
    "# Here we apply Standard Normal Variate (SNV) transformation\n",
    "x_pipeline = [\n",
    "    SNV(),            # Apply SNV transformation\n",
    "    MinMaxScaler()    # Scale features to [0,1]\n",
    "]\n",
    "\n",
    "# No model is used in this example; we focus on data transformation\n",
    "config = Config(\n",
    "    dataset_path,\n",
    "    x_pipeline,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Run the experiment\n",
    "runner = ExperimentRunner(configs=[config])\n",
    "dataset, model_manager = runner.run()\n",
    "\n",
    "# Access the transformed data\n",
    "transformed_data = dataset.x_train\n",
    "\n",
    "print(\"Transformed data shape:\", transformed_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Applying a Preprocessing Pipeline and Training a Simple Model\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pinard.transformations import StandardNormalVariate as SNV, SavitzkyGolay as SG\n",
    "from pinard.core.runner import ExperimentRunner\n",
    "from pinard.core.config import Config\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "\n",
    "\n",
    "# Define the dataset path\n",
    "dataset_path = \"sample_data/mock_data\"\n",
    "\n",
    "# Define the data transformation pipeline\n",
    "x_pipeline = [\n",
    "    SG(window_length=11, polyorder=2),  # Apply Savitzky-Golay filter\n",
    "    SNV(),                              # Apply SNV transformation\n",
    "    MinMaxScaler()                      # Scale features to [0,1]\n",
    "]\n",
    "\n",
    "# Define the model\n",
    "model = {\n",
    "    \"class\": \"sklearn.linear_model.LinearRegression\",\n",
    "    \"model_params\": {}\n",
    "}\n",
    "\n",
    "# Training parameters\n",
    "train_params = {\n",
    "    \"action\": \"train\",\n",
    "    \"training_params\": {}\n",
    "}\n",
    "\n",
    "# Define the configuration\n",
    "config = Config(\n",
    "    dataset_path=dataset_path,\n",
    "    x_pipeline=x_pipeline,\n",
    "    y_pipeline=None,\n",
    "    model=model,\n",
    "    experiment_params=train_params,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Run the experiment\n",
    "runner = ExperimentRunner(configs=[config])\n",
    "dataset, model_manager = runner.run()\n",
    "\n",
    "# Access the trained model\n",
    "trained_model = model_manager.models[0].model\n",
    "\n",
    "# Print model coefficients\n",
    "print(\"Model coefficients:\", trained_model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Using Cross-Validation with Pinard\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pinard.transformations import StandardNormalVariate as SNV, SavitzkyGolay as SG\n",
    "from pinard.core.runner import ExperimentRunner\n",
    "from pinard.core.config import Config\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "\n",
    "\n",
    "# Define the dataset path\n",
    "dataset_path = \"sample_data/mock_data\"\n",
    "\n",
    "# Define the data transformation pipeline with cross-validation\n",
    "x_pipeline = [\n",
    "    {\"split\": RepeatedKFold(n_splits=5, n_repeats=2, random_state=42)},\n",
    "    SG(window_length=11, polyorder=2),\n",
    "    SNV(),\n",
    "    MinMaxScaler()\n",
    "]\n",
    "\n",
    "# Define the model\n",
    "model = {\n",
    "    \"class\": \"sklearn.linear_model.Ridge\",\n",
    "    \"model_params\": {\n",
    "        \"alpha\": 1.0\n",
    "    }\n",
    "}\n",
    "\n",
    "# Training parameters\n",
    "train_params = {\n",
    "    \"action\": \"train\",\n",
    "    \"training_params\": {}\n",
    "}\n",
    "\n",
    "# Define the configuration\n",
    "config = Config(\n",
    "    dataset_path=dataset_path,\n",
    "    x_pipeline=x_pipeline,\n",
    "    y_pipeline=None,\n",
    "    model=model,\n",
    "    experiment_params=train_params,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Run the experiment\n",
    "runner = ExperimentRunner(configs=[config])\n",
    "dataset, model_manager = runner.run()\n",
    "\n",
    "# Evaluate the model\n",
    "scores = model_manager.models[0].scores\n",
    "print(\"Cross-validation scores:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Fine-tuning a Model with Pinard\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pinard.transformations import StandardNormalVariate as SNV, SavitzkyGolay as SG\n",
    "from pinard.core.runner import ExperimentRunner\n",
    "from pinard.core.config import Config\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "\n",
    "\n",
    "# Define the dataset path\n",
    "dataset_path = \"sample_data/mock_data\"\n",
    "\n",
    "# Define the data transformation pipeline with cross-validation\n",
    "x_pipeline = [\n",
    "    {\"split\": RepeatedKFold(n_splits=5, n_repeats=2, random_state=42)},\n",
    "    SG(window_length=11, polyorder=2),\n",
    "    SNV(),\n",
    "    MinMaxScaler()\n",
    "]\n",
    "\n",
    "# Define the model\n",
    "model = {\n",
    "    \"class\": \"sklearn.linear_model.Ridge\",\n",
    "    \"model_params\": {}\n",
    "}\n",
    "\n",
    "# Define the finetune parameters\n",
    "finetune_params = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"finetune_params\": {\n",
    "        'model_params': {\n",
    "            'alpha': ('float', 0.1, 10.0)\n",
    "        },\n",
    "        'training_params': {},\n",
    "        'n_trials': 20,\n",
    "        'tuner': 'sklearn'  # Use scikit-learn's GridSearchCV\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define the configuration\n",
    "config = Config(\n",
    "    dataset_path=dataset_path,\n",
    "    x_pipeline=x_pipeline,\n",
    "    y_pipeline=None,\n",
    "    model=model,\n",
    "    experiment_params=finetune_params,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Run the experiment\n",
    "runner = ExperimentRunner(configs=[config])\n",
    "dataset, model_manager = runner.run()\n",
    "\n",
    "# Get the best model and its parameters\n",
    "best_model = model_manager.models[0].best_model\n",
    "best_params = model_manager.models[0].best_params\n",
    "\n",
    "print(\"Best model parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Fine-tuning a Model with Pinard\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pinard.transformations import StandardNormalVariate as SNV, SavitzkyGolay as SG\n",
    "from pinard.core.runner import ExperimentRunner\n",
    "from pinard.core.config import Config\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "\n",
    "\n",
    "# Define the dataset path\n",
    "dataset_path = \"sample_data/mock_data\"\n",
    "\n",
    "# Define the data transformation pipeline with cross-validation\n",
    "x_pipeline = [\n",
    "    {\"split\": RepeatedKFold(n_splits=5, n_repeats=2, random_state=42)},\n",
    "    SG(window_length=11, polyorder=2),\n",
    "    SNV(),\n",
    "    MinMaxScaler()\n",
    "]\n",
    "\n",
    "# Define the model\n",
    "model = {\n",
    "    \"class\": \"sklearn.linear_model.Ridge\",\n",
    "    \"model_params\": {}\n",
    "}\n",
    "\n",
    "# Define the finetune parameters\n",
    "finetune_params = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"finetune_params\": {\n",
    "        'model_params': {\n",
    "            'alpha': ('float', 0.1, 10.0)\n",
    "        },\n",
    "        'training_params': {},\n",
    "        'n_trials': 20,\n",
    "        'tuner': 'sklearn'  # Use scikit-learn's GridSearchCV\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define the configuration\n",
    "config = Config(\n",
    "    dataset_path=dataset_path,\n",
    "    x_pipeline=x_pipeline,\n",
    "    y_pipeline=None,\n",
    "    model=model,\n",
    "    experiment_params=finetune_params,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Run the experiment\n",
    "runner = ExperimentRunner(configs=[config])\n",
    "dataset, model_manager = runner.run()\n",
    "\n",
    "# Get the best model and its parameters\n",
    "best_model = model_manager.models[0].best_model\n",
    "best_params = model_manager.models[0].best_params\n",
    "\n",
    "print(\"Best model parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6: Fine-tuning a Custom TensorFlow Model with Pinard\n",
    "\n",
    "from kerastuner import HyperModel\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pinard.transformations import StandardNormalVariate as SNV, SavitzkyGolay as SG\n",
    "from pinard.core.runner import ExperimentRunner\n",
    "from pinard.core.config import Config\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "\n",
    "\n",
    "# Define the dataset path\n",
    "dataset_path = \"sample_data/mock_data\"\n",
    "\n",
    "# Placeholder for input shape (to be defined later)\n",
    "input_shape = None\n",
    "\n",
    "# Define a hypermodel for Keras Tuner\n",
    "\n",
    "\n",
    "class MyHyperModel(HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = tf.keras.Sequential()\n",
    "        units = hp.Int('units', min_value=32, max_value=128, step=32)\n",
    "        model.add(tf.keras.layers.Dense(units=units, activation='relu', input_shape=(input_shape,)))\n",
    "        model.add(tf.keras.layers.Dense(1))\n",
    "        optimizer = hp.Choice('optimizer', ['adam', 'sgd'])\n",
    "        model.compile(optimizer=optimizer, loss='mse')\n",
    "        return model\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = {\n",
    "    \"class\": \"tensorflow.keras.models.Sequential\",\n",
    "    \"model_params\": {\n",
    "        \"build_fn\": MyHyperModel()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define the finetune parameters\n",
    "finetune_params = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"finetune_params\": {\n",
    "        'model_params': {\n",
    "            'units': ('int', 32, 128, 32),\n",
    "            'optimizer': ['adam', 'sgd']\n",
    "        },\n",
    "        'training_params': {\n",
    "            'epochs': 50,\n",
    "            'batch_size': 32\n",
    "        },\n",
    "        'n_trials': 20,\n",
    "        'tuner': 'keras'  # Use Keras Tuner\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define the data transformation pipeline with cross-validation\n",
    "x_pipeline = [\n",
    "    {\"split\": RepeatedKFold(n_splits=5, n_repeats=2, random_state=42)},\n",
    "    SG(window_length=11, polyorder=2),\n",
    "    SNV(),\n",
    "    MinMaxScaler()\n",
    "]\n",
    "\n",
    "# Define the configuration\n",
    "config = Config(\n",
    "    dataset_path=dataset_path,\n",
    "    x_pipeline=x_pipeline,\n",
    "    y_pipeline=None,\n",
    "    model=model,\n",
    "    experiment_params=finetune_params,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Run the experiment\n",
    "runner = ExperimentRunner(configs=[config])\n",
    "\n",
    "# Set the input shape after data is loaded\n",
    "dataset, model_manager = runner.load_data_only()\n",
    "input_shape = dataset.x_train.shape[1]\n",
    "model['model_params']['build_fn'] = MyHyperModel()\n",
    "\n",
    "# Run the finetuning\n",
    "dataset, model_manager = runner.run()\n",
    "\n",
    "# Get the best model and its parameters\n",
    "best_model = model_manager.models[0].best_model\n",
    "best_params = model_manager.models[0].best_params\n",
    "\n",
    "print(\"Best model parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "\n",
    "class LogTransformer(TransformerMixin, BaseEstimator):\n",
    "    \"\"\"\n",
    "    Applies a logarithmic transformation to the data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, copy=True, offset=1e-6):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        copy : bool, default=True\n",
    "            Set to False to perform inplace computation.\n",
    "        offset : float, default=1e-6\n",
    "            A small constant to add to the data to avoid log(0).\n",
    "        \"\"\"\n",
    "        self.copy = copy\n",
    "        self.offset = offset\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, copy=None):\n",
    "        \"\"\"\n",
    "        Apply the logarithmic transformation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The data to transform.\n",
    "        copy : bool, default=None\n",
    "            Copy the input X or not.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array-like of shape (n_samples, n_features)\n",
    "            Transformed data.\n",
    "        \"\"\"\n",
    "        if copy is None:\n",
    "            copy = self.copy\n",
    "\n",
    "        X = np.asarray(X)\n",
    "        if copy:\n",
    "            X = X.copy()\n",
    "\n",
    "        X = np.log(X + self.offset)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import BaseCrossValidator\n",
    "from sklearn.utils import check_random_state\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class StratifiedFeatureSplitter(BaseCrossValidator):\n",
    "    \"\"\"\n",
    "    Stratified splitter based on a continuous feature.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_splits=5, feature_index=0, random_state=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_splits : int, default=5\n",
    "            Number of folds.\n",
    "        feature_index : int, default=0\n",
    "            Index of the feature to stratify on.\n",
    "        random_state : int or RandomState instance, default=None\n",
    "            Random state for reproducibility.\n",
    "        \"\"\"\n",
    "        self.n_splits = n_splits\n",
    "        self.feature_index = feature_index\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        X = np.asarray(X)\n",
    "        feature = X[:, self.feature_index]\n",
    "        percentiles = np.percentile(feature, np.linspace(0, 100, self.n_splits + 1))\n",
    "\n",
    "        indices = np.arange(len(X))\n",
    "        rng = check_random_state(self.random_state)\n",
    "        rng.shuffle(indices)\n",
    "\n",
    "        bins = np.digitize(feature[indices], percentiles[1:-1], right=True)\n",
    "\n",
    "        for fold in range(self.n_splits):\n",
    "            test_mask = bins == fold\n",
    "            train_mask = ~test_mask\n",
    "            yield indices[train_mask], indices[test_mask]\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using Custom LogTransformer and StratifiedFeatureSplitter in Pinard\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pinard.core.runner import ExperimentRunner\n",
    "from pinard.core.config import Config\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "\n",
    "\n",
    "# Assuming LogTransformer and StratifiedFeatureSplitter are defined as above\n",
    "\n",
    "# Define the dataset path\n",
    "dataset_path = \"sample_data/mock_data\"\n",
    "\n",
    "# Define the data transformation pipeline\n",
    "x_pipeline = [\n",
    "    {\"split\": StratifiedFeatureSplitter(n_splits=5, feature_index=0, random_state=42)},\n",
    "    LogTransformer(offset=1e-6),\n",
    "    MinMaxScaler()\n",
    "]\n",
    "\n",
    "# Define the model\n",
    "model = {\n",
    "    \"class\": \"sklearn.linear_model.LinearRegression\",\n",
    "    \"model_params\": {}\n",
    "}\n",
    "\n",
    "# Training parameters\n",
    "train_params = {\n",
    "    \"action\": \"train\",\n",
    "    \"training_params\": {}\n",
    "}\n",
    "\n",
    "# Define the configuration\n",
    "config = Config(\n",
    "    dataset_path=dataset_path,\n",
    "    x_pipeline=x_pipeline,\n",
    "    y_pipeline=None,\n",
    "    model=model,\n",
    "    experiment_params=train_params,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Run the experiment\n",
    "runner = ExperimentRunner(configs=[config])\n",
    "dataset, model_manager = runner.run()\n",
    "\n",
    "# Access the trained model\n",
    "trained_model = model_manager.models[0].model\n",
    "\n",
    "# Evaluate the model\n",
    "\n",
    "y_pred = trained_model.predict(dataset.x_test)\n",
    "mse = mean_squared_error(dataset.y_test, y_pred)\n",
    "print(\"Test MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RatioTransformer(TransformerMixin, BaseEstimator):\n",
    "    \"\"\"\n",
    "    Creates a new feature by taking the ratio of two features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, numerator_index=0, denominator_index=1, copy=True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        numerator_index : int, default=0\n",
    "            Index of the numerator feature.\n",
    "        denominator_index : int, default=1\n",
    "            Index of the denominator feature.\n",
    "        copy : bool, default=True\n",
    "            Set to False to perform inplace computation.\n",
    "        \"\"\"\n",
    "        self.numerator_index = numerator_index\n",
    "        self.denominator_index = denominator_index\n",
    "        self.copy = copy\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, copy=None):\n",
    "        \"\"\"\n",
    "        Create a new feature as the ratio of two existing features.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The data to transform.\n",
    "        copy : bool, default=None\n",
    "            Copy the input X or not.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array-like of shape (n_samples, n_features + 1)\n",
    "            Transformed data with the new ratio feature added.\n",
    "        \"\"\"\n",
    "        if copy is None:\n",
    "            copy = self.copy\n",
    "\n",
    "        X = np.asarray(X)\n",
    "        if copy:\n",
    "            X = X.copy()\n",
    "\n",
    "        numerator = X[:, self.numerator_index]\n",
    "        denominator = X[:, self.denominator_index] + 1e-6  # Avoid division by zero\n",
    "        ratio_feature = (numerator / denominator).reshape(-1, 1)\n",
    "\n",
    "        X_new = np.hstack((X, ratio_feature))\n",
    "        return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "\n",
    "\n",
    "class ClusterBasedSplitter(BaseCrossValidator):\n",
    "    \"\"\"\n",
    "    Splits data into training and testing sets based on clustering.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_splits=5, n_clusters=5, random_state=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_splits : int, default=5\n",
    "            Number of splits/folds.\n",
    "        n_clusters : int, default=5\n",
    "            Number of clusters to form.\n",
    "        random_state : int or RandomState instance, default=None\n",
    "            Random state for reproducibility.\n",
    "        \"\"\"\n",
    "        self.n_splits = n_splits\n",
    "        self.n_clusters = n_clusters\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        X = np.asarray(X)\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, random_state=self.random_state)\n",
    "        cluster_labels = kmeans.fit_predict(X)\n",
    "\n",
    "        unique_clusters = np.unique(cluster_labels)\n",
    "        rng = check_random_state(self.random_state)\n",
    "        rng.shuffle(unique_clusters)\n",
    "\n",
    "        clusters_per_split = np.array_split(unique_clusters, self.n_splits)\n",
    "\n",
    "        for cluster_group in clusters_per_split:\n",
    "            test_indices = np.where(np.isin(cluster_labels, cluster_group))[0]\n",
    "            train_indices = np.setdiff1d(np.arange(len(X)), test_indices)\n",
    "            yield train_indices, test_indices\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using Custom ClusterBasedSplitter in Pinard\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pinard.core.runner import ExperimentRunner\n",
    "from pinard.core.config import Config\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "\n",
    "\n",
    "# Assuming ClusterBasedSplitter is defined as above\n",
    "\n",
    "# Define the dataset path\n",
    "dataset_path = \"sample_data/mock_data\"\n",
    "\n",
    "# Define the data transformation pipeline\n",
    "x_pipeline = [\n",
    "    StandardScaler(),\n",
    "    {\"split\": ClusterBasedSplitter(n_splits=5, n_clusters=5, random_state=42)}\n",
    "]\n",
    "\n",
    "# Define the model\n",
    "model = {\n",
    "    \"class\": \"sklearn.svm.SVR\",\n",
    "    \"model_params\": {\n",
    "        \"kernel\": \"rbf\",\n",
    "        \"C\": 1.0,\n",
    "        \"epsilon\": 0.1\n",
    "    }\n",
    "}\n",
    "\n",
    "# Training parameters\n",
    "train_params = {\n",
    "    \"action\": \"train\",\n",
    "    \"training_params\": {}\n",
    "}\n",
    "\n",
    "# Define the configuration\n",
    "config = Config(\n",
    "    dataset_path=dataset_path,\n",
    "    x_pipeline=x_pipeline,\n",
    "    y_pipeline=None,\n",
    "    model=model,\n",
    "    experiment_params=train_params,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Run the experiment\n",
    "runner = ExperimentRunner(configs=[config])\n",
    "dataset, model_manager = runner.run()\n",
    "\n",
    "# Access the trained model\n",
    "trained_model = model_manager.models[0].model\n",
    "\n",
    "# Evaluate the model\n",
    "\n",
    "y_pred = trained_model.predict(dataset.x_test)\n",
    "mse = mean_squared_error(dataset.y_test, y_pred)\n",
    "print(\"Test MSE:\", mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

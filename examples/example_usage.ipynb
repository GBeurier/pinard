{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-29 11:31:36,357 - INFO - ================================================================================\n",
      "2024-10-29 11:31:36,358 - INFO - ### PREPARING DATA ###\n",
      "2024-10-29 11:31:36,358 - INFO - ### LOADING DATASET ###\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Browsing sample_data/WhiskyConcentration\n",
      "No train_group file found for sample_data/WhiskyConcentration.\n",
      "No test_group file found for sample_data/WhiskyConcentration.\n",
      "{'initial_shape': (261, 5251), 'delimiter': ';', 'numeric_delimiter': '.', 'header_line': 0, 'final_shape': (261, 5248), 'na_handling': {'strategy': 'abort', 'nb_removed_rows': None, 'removed_rows': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-29 11:31:37,911 - INFO - Dataset(x_train:(261, 5248) - y_train:(261, 1), x_test:(263, 5248) - y_test:(263, 1))\n",
      "2024-10-29 11:31:37,912 - INFO - ### PROCESSING DATASET ###\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'initial_shape': (263, 5251), 'delimiter': ';', 'numeric_delimiter': '.', 'header_line': 0, 'final_shape': (263, 5248), 'na_handling': {'strategy': 'abort', 'nb_removed_rows': None, 'removed_rows': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-29 11:31:38,289 - INFO - Dataset(x_train:(261, 20992) - y_train:(261, 1), x_test:(263, 20992) - y_test:(263, 1))\n",
      "2024-10-29 11:31:38,290 - INFO - ### PREPARING MODEL ###\n",
      "2024-10-29 11:31:38,292 - INFO - Running config > {'dataset': 'sample_data/WhiskyConcentration', 'x_pipeline': [{'class': 'sklearn.preprocessing.RobustScaler', 'params': {'copy': True, 'quantile_range': [25.0, 75.0], 'unit_variance': False, 'with_centering': True, 'with_scaling': True}}, {'features': [None, [{'class': 'pinard.transformations.Gaussian', 'params': {'copy': True, 'order': 2, 'sigma': 1}}, {'class': 'sklearn.preprocessing.StandardScaler', 'params': {'copy': True, 'with_mean': True, 'with_std': True}}], {'class': 'pinard.transformations.SavitzkyGolay', 'params': {'copy': True, 'delta': 1.0, 'deriv': 0, 'polyorder': 3, 'window_length': 11}}, {'class': 'pinard.transformations.Gaussian', 'params': {'copy': True, 'order': 2, 'sigma': 1}}]}, {'class': 'sklearn.preprocessing.MinMaxScaler', 'params': {'clip': False, 'copy': True, 'feature_range': [0, 1]}}], 'y_pipeline': None, 'model': {'class': 'sklearn.ensemble.RandomForestClassifier', 'params': None}, 'experiment': {'task': 'classification', 'metrics': ['accuracy'], 'num_classes': 4}, 'seed': 246918912}\n",
      "2024-10-29 11:31:38,294 - INFO - Starting new experiment experiment_8146bc0d.\n",
      "2024-10-29 11:31:38,295 - INFO - Experiment prepared at results\\sample_dataWhiskyConcentration\\RandomForestClassifier\\experiment_8146bc0d\n",
      "2024-10-29 11:31:38,296 - INFO - Training the model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using framework: sklearn\n",
      "Training fold 1, with shapes: (261, 20992) (261, 1) (263, 20992) (263, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-29 11:31:39,616 - INFO - Saved model to results\\sample_dataWhiskyConcentration\\RandomForestClassifier\\experiment_8146bc0d\n",
      "2024-10-29 11:31:39,625 - INFO - Metrics saved to results\\sample_dataWhiskyConcentration\\RandomForestClassifier\\experiment_8146bc0d\\metrics.json\n",
      "2024-10-29 11:31:39,626 - INFO - Evaluation Metrics: {'accuracy': 0.39923954372623577}\n",
      "2024-10-29 11:31:39,628 - INFO - Predictions saved to results\\sample_dataWhiskyConcentration\\RandomForestClassifier\\experiment_8146bc0d\\predictions.csv\n",
      "2024-10-29 11:31:39,629 - INFO - Updated experiments at results\\sample_dataWhiskyConcentration\\RandomForestClassifier\\experiments.json\n",
      "2024-10-29 11:31:39,631 - INFO - Updated experiments at results\\sample_dataWhiskyConcentration\\experiments.json\n",
      "2024-10-29 11:31:39,631 - INFO - Updated experiments at results\\sample_dataWhiskyConcentration\\RandomForestClassifier\\experiments.json and results\\sample_dataWhiskyConcentration\\experiments.json\n",
      "2024-10-29 11:31:39,632 - INFO - All experiments completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 3.284183979034424 seconds\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "\n",
    "import time\n",
    "from pinard.presets.ref_models import decon, bacon, customizable_bacon, bacon_classification\n",
    "from pinard.presets.preprocessings import decon_set, bacon_set\n",
    "from pinard.data_splitters import KennardStoneSplitter\n",
    "from pinard.transformations import StandardNormalVariate as SNV, SavitzkyGolay as SG, Gaussian as GS, Derivate as  Dv\n",
    "from pinard.transformations import Rotate_Translate as RT, Spline_X_Simplification as SXS, Random_X_Operation as RXO\n",
    "from pinard.transformations import CropTransformer\n",
    "from pinard.core.runner import ExperimentRunner\n",
    "from pinard.core.config import Config\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, RepeatedKFold, StratifiedKFold, RepeatedStratifiedKFold, ShuffleSplit, GroupKFold, StratifiedShuffleSplit, BaseCrossValidator, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "model_sklearn = {\n",
    "    \"class\": \"sklearn.cross_decomposition.PLSRegression\",\n",
    "    \"model_params\": {\n",
    "        \"n_components\": 21,\n",
    "    }\n",
    "}\n",
    "    \n",
    "finetune_pls_experiment = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"finetune_params\": {\n",
    "        'model_params': {\n",
    "            'n_components': ('int', 5, 20),\n",
    "        },\n",
    "        'training_params': {},\n",
    "        'tuner': 'sklearn'\n",
    "    }\n",
    "}\n",
    "\n",
    "bacon_train = {\"action\": \"train\", \"training_params\": {\"epochs\": 2000, \"batch_size\": 500, \"patience\": 200, \"cyclic_lr\": True, \"base_lr\": 1e-6, \"max_lr\": 1e-3, \"step_size\": 400}}\n",
    "bacon_train_short = {\"action\": \"train\", \"training_params\": {\"epochs\": 10, \"batch_size\": 500, \"patience\": 20, \"cyclic_lr\": True, \"base_lr\": 1e-6, \"max_lr\": 1e-3, \"step_size\": 40}}\n",
    "bacon_finetune = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"finetune_params\": {\n",
    "        \"n_trials\": 5,\n",
    "        \"model_params\": {\n",
    "            \"filters_1\": [8, 16, 32, 64], \n",
    "            \"filters_2\": [8, 16, 32, 64], \n",
    "            \"filters_3\": [8, 16, 32, 64]\n",
    "        }\n",
    "    },\n",
    "    \"training_params\": {\n",
    "        \"epochs\": 10,\n",
    "    }\n",
    "}\n",
    "\n",
    "full_bacon_finetune = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"training_params\": {\n",
    "        \"epochs\": 500,\n",
    "        \"patience\": 100,\n",
    "    },\n",
    "    \"finetune_params\": {\n",
    "        \"nb_trials\": 150,\n",
    "        \"model_params\": {\n",
    "            'spatial_dropout': (float, 0.01, 0.5),\n",
    "            'filters1': [4, 8, 16, 32, 64, 128, 256],\n",
    "            'kernel_size1': [3, 5, 7, 9, 11, 13, 15],\n",
    "            # 'strides1': [1, 2, 3, 4, 5],\n",
    "            # 'activation1': ['relu', 'selu', 'elu', 'swish'],\n",
    "            'dropout_rate': (float, 0.01, 0.5),\n",
    "            'filters2': [4, 8, 16, 32, 64, 128, 256],\n",
    "            # 'kernel_size2': [3, 5, 7, 9, 11, 13, 15],\n",
    "            # 'strides2': [1, 2, 3, 4, 5],\n",
    "            'activation2': ['relu', 'selu', 'elu', 'swish'],\n",
    "            'normalization_method1': ['BatchNormalization', 'LayerNormalization'],\n",
    "            'filters3': [4, 8, 16, 32, 64, 128, 256],\n",
    "            # 'kernel_size3': [3, 5, 7, 9, 11, 13, 15],\n",
    "            # 'strides3': [1, 2, 3, 4, 5],\n",
    "            'activation3': ['relu', 'selu', 'elu', 'swish'],\n",
    "            # 'normalization_method2': ['BatchNormalization', 'LayerNormalization'],\n",
    "            # 'dense_units': [4, 8, 16, 32, 64, 128, 256],\n",
    "            'dense_activation': ['relu', 'selu', 'elu', 'swish'],\n",
    "        },\n",
    "        # \"training_params\": {\n",
    "        #     \"batch_size\": [32, 64, 128, 256, 512],\n",
    "        #     \"cyclic_lr\": [True, False],\n",
    "        #     \"base_lr\": (float, 1e-6, 1e-2),\n",
    "        #     \"max_lr\": (float, 1e-3, 1e-1),\n",
    "        #     \"step_size\": (int, 500, 5000),\n",
    "        # },\n",
    "    }\n",
    "}\n",
    "\n",
    "x_pipeline_PLS = [\n",
    "    RobustScaler(),\n",
    "    # {\"samples\": [None, SXS, RXO]},\n",
    "    # {\"split\": RepeatedKFold(n_splits=3, n_repeats=1)},\n",
    "    {\"features\": [None, GS(2,1), SG, SNV, Dv, [GS, SNV], [GS, GS],[GS, SG],[SG, SNV], [GS, Dv], [SG, Dv]]},\n",
    "    MinMaxScaler()\n",
    "]\n",
    "                    \n",
    "\n",
    "\n",
    "x_pipeline_full = [\n",
    "    RobustScaler(),\n",
    "    {\"samples\": [None, None,None,None,SXS, RXO]},\n",
    "    {\"split\": RepeatedKFold(n_splits=3, n_repeats=1)},\n",
    "    {\"features\": [None, GS(2,1), SG, SNV, Dv, [GS, SNV], [GS, GS],[GS, SG],[SG, SNV], [GS, Dv], [SG, Dv]]},\n",
    "    MinMaxScaler()\n",
    "]\n",
    "\n",
    "x_pipeline_full2 = [\n",
    "    RobustScaler(),\n",
    "    {\"samples\": [None, None,None,None,SXS, RXO]},\n",
    "    {\"split\": RepeatedKFold(n_splits=3, n_repeats=1)},\n",
    "    {\"features\": [None, GS(2,1), SG, SNV, Dv, [GS, SNV], [GS, GS],[GS, SG],[SG, SNV], [GS, Dv], [SG, Dv]]},\n",
    "    MinMaxScaler()\n",
    "]\n",
    "\n",
    "x_pipeline = [\n",
    "    RobustScaler(), \n",
    "    # {\"samples\": [None, SXS]}, \n",
    "    {\"split\": RepeatedKFold(n_splits=3, n_repeats=1)}, \n",
    "    {\"features\": [None, [GS(), SNV()], SG(), GS()]}, \n",
    "    # {\"features\": [None, GS]}, \n",
    "    # {\"features\": [None, GS, SG, SNV, Dv, [GS, SNV], [GS, GS],[GS, SG],[SG, SNV], [GS, Dv], [SG, Dv]]},\n",
    "    # {\"features\": [None, SG, GS, SNV, [SG, SNV], [GS, SNV], [SG, GS]]}, \n",
    "    # bacon_set(),\n",
    "    MinMaxScaler()\n",
    "]\n",
    "bacon_finetune_classif = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"task\": \"classification\",\n",
    "    \"finetune_params\": {\n",
    "        \"n_trials\": 5,\n",
    "        \"model_params\": {\n",
    "            \"filters_1\": [8, 16, 32, 64], \n",
    "            \"filters_2\": [8, 16, 32, 64], \n",
    "            \"filters_3\": [8, 16, 32, 64]\n",
    "        }\n",
    "    },\n",
    "    \"training_params\": {\n",
    "        \"epochs\": 5,\n",
    "    }\n",
    "}\n",
    "\n",
    "finetune_randomForestclassifier = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"task\": \"classification\",\n",
    "    \"finetune_params\": {\n",
    "        'model_params': {\n",
    "            'n_estimators': ('int', 5, 20),\n",
    "        },\n",
    "        'training_params': {},\n",
    "        'tuner': 'sklearn'\n",
    "    }\n",
    "}\n",
    "\n",
    "seed = 123459456\n",
    "\n",
    "datasets = \"sample_data/mock_data3_classif\"\n",
    "y_pipeline = MinMaxScaler()\n",
    "# processing only\n",
    "config2 = Config(\"sample_data/mock_data3\", x_pipeline_full, y_pipeline, None, None, seed)\n",
    "## TRAINING\n",
    "# regression\n",
    "config1 = Config(\"sample_data/mock_data2\", x_pipeline, y_pipeline, bacon, bacon_train_short, seed)\n",
    "config4 = Config(\"sample_data/mock_data3\", x_pipeline_PLS, y_pipeline, model_sklearn, None, seed)\n",
    "# classification\n",
    "config3 = Config(\"sample_data/mock_data3_classif\", x_pipeline, None, bacon_classification, {\"task\":\"classification\", \"training_params\":{\"epochs\":5}}, seed*2)\n",
    "config11 = Config(\"sample_data/mock_data3_binary\", x_pipeline, None, bacon_classification, {\"task\":\"classification\", \"training_params\":{\"epochs\":5}}, seed*2)\n",
    "config5 = Config(\"sample_data/WhiskyConcentration\", x_pipeline, None, RandomForestClassifier, {\"task\":\"classification\"}, seed*2)\n",
    "config10 = Config(\"sample_data/Malaria2024\", x_pipeline, None, RandomForestClassifier, {\"task\":\"classification\"}, seed*2)\n",
    "## FINETUNING\n",
    "# regression\n",
    "config6 = Config(\"sample_data/mock_data3\", x_pipeline, y_pipeline, bacon, bacon_finetune, seed)\n",
    "config7 = Config(\"sample_data/mock_data3\", x_pipeline, y_pipeline, model_sklearn, finetune_pls_experiment, seed)\n",
    "# classification\n",
    "config8 = Config(\"sample_data/mock_data3_classif\", x_pipeline, None, bacon_classification, bacon_finetune_classif, seed*2)\n",
    "config9 = Config(\"sample_data/mock_data3_classif\", x_pipeline, None, RandomForestClassifier, finetune_randomForestclassifier, seed*2)\n",
    "\n",
    "\n",
    "# configs = [config1, config2, config3, config4, config5, config6, config7, config8, config9]\n",
    "configs = [config5]\n",
    "\n",
    "start = time.time()\n",
    "runner = ExperimentRunner(configs, resume_mode=\"restart\")\n",
    "dataset, model_manager = runner.run()\n",
    "end = time.time()\n",
    "print(f\"Time elapsed: {end-start} seconds\")\n",
    "\n",
    "\n",
    "# print(dataset)\n",
    "# print(dataset.raw_x_train.shape)\n",
    "# print(dataset.to_str(\"union\"))\n",
    "\n",
    "\n",
    "# # chart all sample transformations\n",
    "# sample_0 = dataset.x_train[0][0]\n",
    "# print(sample_0.shape)\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# fig, axs = plt.subplots(5, 5, figsize=(15, 5))\n",
    "# for i, ax in enumerate(axs.flat):\n",
    "#     ax.plot(sample_0[i])\n",
    "#     ax.set_title(f\"Sample {i}\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# class AddVal(TransformerMixin, BaseEstimator):\n",
    "#     def __init__(self, val):\n",
    "#         self.val = val\n",
    "\n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         return X + self.val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinard\n",
    "print(pinard.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ace_tools as tools\n",
    "import pandas as pd\n",
    "\n",
    "# Sklearn metrics list\n",
    "sklearn_metrics = [\n",
    "    \"explained_variance\", \"r2\", \"max_error\", \"matthews_corrcoef\",\n",
    "    \"neg_median_absolute_error\", \"neg_mean_absolute_error\",\n",
    "    \"neg_mean_absolute_percentage_error\", \"neg_mean_squared_error\",\n",
    "    \"neg_mean_squared_log_error\", \"neg_root_mean_squared_error\",\n",
    "    \"neg_root_mean_squared_log_error\", \"neg_mean_poisson_deviance\",\n",
    "    \"neg_mean_gamma_deviance\", \"d2_absolute_error_score\", \"accuracy\",\n",
    "    \"top_k_accuracy\", \"roc_auc\", \"roc_auc_ovr\", \"roc_auc_ovo\",\n",
    "    \"roc_auc_ovr_weighted\", \"roc_auc_ovo_weighted\", \"balanced_accuracy\",\n",
    "    \"average_precision\", \"neg_log_loss\", \"neg_brier_score\",\n",
    "    \"positive_likelihood_ratio\", \"neg_negative_likelihood_ratio\",\n",
    "    \"adjusted_rand_score\", \"rand_score\", \"homogeneity_score\",\n",
    "    \"completeness_score\", \"v_measure_score\", \"mutual_info_score\",\n",
    "    \"adjusted_mutual_info_score\", \"normalized_mutual_info_score\",\n",
    "    \"fowlkes_mallows_score\"\n",
    "]\n",
    "\n",
    "# Tensorflow/keras metrics list\n",
    "tensorflow_metrics = [\n",
    "    \"MeanSquaredError\", \"RootMeanSquaredError\", \"MeanAbsoluteError\",\n",
    "    \"MeanAbsolutePercentageError\", \"MeanSquaredLogarithmicError\",\n",
    "    \"CosineSimilarity\", \"LogCoshError\", \"R2Score\", \"AUC\",\n",
    "    \"FalseNegatives\", \"FalsePositives\", \"Precision\", \"PrecisionAtRecall\",\n",
    "    \"Recall\", \"RecallAtPrecision\", \"SensitivityAtSpecificity\",\n",
    "    \"SpecificityAtSensitivity\", \"TrueNegatives\", \"TruePositives\",\n",
    "    \"Hinge\", \"SquaredHinge\", \"CategoricalHinge\", \"KLDivergence\",\n",
    "    \"Poisson\", \"BinaryCrossentropy\", \"CategoricalCrossentropy\",\n",
    "    \"SparseCategoricalCrossentropy\", \"Accuracy\", \"BinaryAccuracy\",\n",
    "    \"CategoricalAccuracy\", \"SparseCategoricalAccuracy\",\n",
    "    \"TopKCategoricalAccuracy\", \"SparseTopKCategoricalAccuracy\",\n",
    "    \"F1Score\", \"FBetaScore\", \"IoU\", \"BinaryIoU\", \"MeanIoU\",\n",
    "    \"OneHotIoU\", \"OneHotMeanIoU\"\n",
    "]\n",
    "\n",
    "# Metric name mapping: (tensorflow_name, sklearn_name, abbreviation, method_name)\n",
    "# Initialize with common names\n",
    "metrics_mapping = [\n",
    "    (\"MeanSquaredError\", \"neg_mean_squared_error\", \"mse\", \"Mean Squared Error\"),\n",
    "    (\"RootMeanSquaredError\", \"neg_root_mean_squared_error\", \"rmse\", \"Root Mean Squared Error\"),\n",
    "    (\"MeanAbsoluteError\", \"neg_mean_absolute_error\", \"mae\", \"Mean Absolute Error\"),\n",
    "    (\"MeanAbsolutePercentageError\", \"neg_mean_absolute_percentage_error\", \"mape\", \"Mean Absolute Percentage Error\"),\n",
    "    (\"MeanSquaredLogarithmicError\", \"neg_mean_squared_log_error\", \"msle\", \"Mean Squared Logarithmic Error\"),\n",
    "    (\"CosineSimilarity\", None, \"cos_sim\", \"Cosine Similarity\"),\n",
    "    (\"LogCoshError\", None, \"log_cosh\", \"Log Cosh Error\"),\n",
    "    (\"R2Score\", \"r2\", \"r2\", \"R2 Score\"),\n",
    "    (\"AUC\", \"roc_auc\", \"auc\", \"Area Under the Curve\"),\n",
    "    (\"Precision\", None, \"prec\", \"Precision\"),\n",
    "    (\"Recall\", None, \"recall\", \"Recall\"),\n",
    "    (\"Accuracy\", \"accuracy\", \"acc\", \"Accuracy\"),\n",
    "    (\"TopKCategoricalAccuracy\", \"top_k_accuracy\", \"top_k_acc\", \"Top K Categorical Accuracy\"),\n",
    "    (\"BinaryCrossentropy\", None, \"bin_crossentropy\", \"Binary Crossentropy\"),\n",
    "    (\"CategoricalCrossentropy\", None, \"cat_crossentropy\", \"Categorical Crossentropy\"),\n",
    "    (\"SparseCategoricalCrossentropy\", None, \"sparse_cat_crossentropy\", \"Sparse Categorical Crossentropy\"),\n",
    "    (\"F1Score\", None, \"f1\", \"F1 Score\"),\n",
    "    (\"IoU\", None, \"iou\", \"Intersection over Union\")\n",
    "]\n",
    "\n",
    "# Add remaining metrics with None in the missing columns\n",
    "for metric in sklearn_metrics:\n",
    "    if not any(metric in row for row in metrics_mapping):\n",
    "        metrics_mapping.append((None, metric, None, None))\n",
    "\n",
    "for metric in tensorflow_metrics:\n",
    "    if not any(metric in row for row in metrics_mapping):\n",
    "        metrics_mapping.append((metric, None, None, None))\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame(metrics_mapping, columns=[\"tensorflow_name\", \"sklearn_name\", \"abbreviation\", \"method_name\"])\n",
    "\n",
    "# Display the dataframe to the user\n",
    "tools.display_dataframe_to_user(name=\"Metric Comparison\", dataframe=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-28 17:37:15,064 - INFO - ================================================================================\n",
      "2024-10-28 17:37:15,065 - INFO - ### PREPARING DATA ###\n",
      "2024-10-28 17:37:15,065 - INFO - ### LOADING DATASET ###\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Browsing sample_data/mock_data\n",
      "{'initial_shape': (130, 2151), 'delimiter': ';', 'numeric_delimiter': '.', 'header_line': 0, 'final_shape': (130, 2151), 'na_handling': {'strategy': 'abort', 'nb_removed_rows': None, 'removed_rows': None}}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid data: x and y have different number of rows (130 != 131)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Run the experiment\u001b[39;00m\n\u001b[0;32m     33\u001b[0m runner \u001b[38;5;241m=\u001b[39m ExperimentRunner(configs\u001b[38;5;241m=\u001b[39m[config])\n\u001b[1;32m---> 34\u001b[0m dataset, model_manager \u001b[38;5;241m=\u001b[39m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Access the transformed data\u001b[39;00m\n\u001b[0;32m     37\u001b[0m transformed_data \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mx_train\n",
      "File \u001b[1;32md:\\workspace\\ml\\nirs\\pinard\\pinard\\core\\runner.py:90\u001b[0m, in \u001b[0;36mExperimentRunner.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### PREPARING DATA ###\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 90\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m config, action, metrics, training_params, finetune_params, task \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_experiment(config, dataset)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### PREPARING MODEL ###\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\workspace\\ml\\nirs\\pinard\\pinard\\core\\runner.py:122\u001b[0m, in \u001b[0;36mExperimentRunner._prepare_data\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    119\u001b[0m y_pipeline_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39my_pipeline\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### LOADING DATASET ###\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 122\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(dataset)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### PROCESSING DATASET ###\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\workspace\\ml\\nirs\\pinard\\pinard\\data\\dataset_loader.py:158\u001b[0m, in \u001b[0;36mget_dataset\u001b[1;34m(data_config)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03mLoad dataset based on the data configuration.\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;124;03m- Dataset: Dataset object with loaded data IDs.\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m config \u001b[38;5;241m=\u001b[39m parse_config(data_config)\n\u001b[1;32m--> 158\u001b[0m x_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[43mhandle_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m x_test, y_test \u001b[38;5;241m=\u001b[39m handle_data(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    160\u001b[0m dataset \u001b[38;5;241m=\u001b[39m Dataset()\n",
      "File \u001b[1;32md:\\workspace\\ml\\nirs\\pinard\\pinard\\data\\dataset_loader.py:142\u001b[0m, in \u001b[0;36mhandle_data\u001b[1;34m(config, t_set)\u001b[0m\n\u001b[0;32m    140\u001b[0m x_params \u001b[38;5;241m=\u001b[39m _merge_params(config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt_set\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_x_params\u001b[39m\u001b[38;5;124m'\u001b[39m), config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt_set\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_params\u001b[39m\u001b[38;5;124m'\u001b[39m), config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal_params\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    141\u001b[0m y_params \u001b[38;5;241m=\u001b[39m _merge_params(config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt_set\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_y_params\u001b[39m\u001b[38;5;124m'\u001b[39m), config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt_set\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_params\u001b[39m\u001b[38;5;124m'\u001b[39m), config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal_params\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m--> 142\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[43mload_XY\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mt_set\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_x\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mt_set\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_x_filter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m               \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mt_set\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_y\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mt_set\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_y_filter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, y\n",
      "File \u001b[1;32md:\\workspace\\ml\\nirs\\pinard\\pinard\\data\\dataset_loader.py:93\u001b[0m, in \u001b[0;36mload_XY\u001b[1;34m(x_path, x_filter, x_params, y_path, y_filter, y_params)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuto-filtering not implemented yet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid data: x and y have different number of rows (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, y\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid data: x and y have different number of rows (130 != 131)"
     ]
    }
   ],
   "source": [
    "# Example 1: Applying a Simple Data Transformation using Pinard\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pinard.transformations import StandardNormalVariate as SNV\n",
    "from pinard.core.runner import ExperimentRunner\n",
    "from pinard.core.config import Config\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "\n",
    "\n",
    "# Define the dataset path\n",
    "dataset_path = \"sample_data/mock_data\"\n",
    "\n",
    "# Define the data transformation pipeline\n",
    "# Here we apply Standard Normal Variate (SNV) transformation\n",
    "x_pipeline = [\n",
    "    SNV(),            # Apply SNV transformation\n",
    "    MinMaxScaler()    # Scale features to [0,1]\n",
    "]\n",
    "\n",
    "# No model is used in this example; we focus on data transformation\n",
    "config = Config(\n",
    "    dataset_path,\n",
    "    x_pipeline,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Run the experiment\n",
    "runner = ExperimentRunner(configs=[config])\n",
    "dataset, model_manager = runner.run()\n",
    "\n",
    "# Access the transformed data\n",
    "transformed_data = dataset.x_train\n",
    "\n",
    "print(\"Transformed data shape:\", transformed_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Applying a Preprocessing Pipeline and Training a Simple Model\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pinard.transformations import StandardNormalVariate as SNV, SavitzkyGolay as SG\n",
    "from pinard.core.runner import ExperimentRunner\n",
    "from pinard.core.config import Config\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "\n",
    "\n",
    "# Define the dataset path\n",
    "dataset_path = \"sample_data/mock_data\"\n",
    "\n",
    "# Define the data transformation pipeline\n",
    "x_pipeline = [\n",
    "    SG(window_length=11, polyorder=2),  # Apply Savitzky-Golay filter\n",
    "    SNV(),                              # Apply SNV transformation\n",
    "    MinMaxScaler()                      # Scale features to [0,1]\n",
    "]\n",
    "\n",
    "# Define the model\n",
    "model = {\n",
    "    \"class\": \"sklearn.linear_model.LinearRegression\",\n",
    "    \"model_params\": {}\n",
    "}\n",
    "\n",
    "# Training parameters\n",
    "train_params = {\n",
    "    \"action\": \"train\",\n",
    "    \"training_params\": {}\n",
    "}\n",
    "\n",
    "# Define the configuration\n",
    "config = Config(\n",
    "    dataset_path=dataset_path,\n",
    "    x_pipeline=x_pipeline,\n",
    "    y_pipeline=None,\n",
    "    model=model,\n",
    "    experiment_params=train_params,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Run the experiment\n",
    "runner = ExperimentRunner(configs=[config])\n",
    "dataset, model_manager = runner.run()\n",
    "\n",
    "# Access the trained model\n",
    "trained_model = model_manager.models[0].model\n",
    "\n",
    "# Print model coefficients\n",
    "print(\"Model coefficients:\", trained_model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Using Cross-Validation with Pinard\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pinard.transformations import StandardNormalVariate as SNV, SavitzkyGolay as SG\n",
    "from pinard.core.runner import ExperimentRunner\n",
    "from pinard.core.config import Config\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "\n",
    "\n",
    "# Define the dataset path\n",
    "dataset_path = \"sample_data/mock_data\"\n",
    "\n",
    "# Define the data transformation pipeline with cross-validation\n",
    "x_pipeline = [\n",
    "    {\"split\": RepeatedKFold(n_splits=5, n_repeats=2, random_state=42)},\n",
    "    SG(window_length=11, polyorder=2),\n",
    "    SNV(),\n",
    "    MinMaxScaler()\n",
    "]\n",
    "\n",
    "# Define the model\n",
    "model = {\n",
    "    \"class\": \"sklearn.linear_model.Ridge\",\n",
    "    \"model_params\": {\n",
    "        \"alpha\": 1.0\n",
    "    }\n",
    "}\n",
    "\n",
    "# Training parameters\n",
    "train_params = {\n",
    "    \"action\": \"train\",\n",
    "    \"training_params\": {}\n",
    "}\n",
    "\n",
    "# Define the configuration\n",
    "config = Config(\n",
    "    dataset_path=dataset_path,\n",
    "    x_pipeline=x_pipeline,\n",
    "    y_pipeline=None,\n",
    "    model=model,\n",
    "    experiment_params=train_params,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Run the experiment\n",
    "runner = ExperimentRunner(configs=[config])\n",
    "dataset, model_manager = runner.run()\n",
    "\n",
    "# Evaluate the model\n",
    "scores = model_manager.models[0].scores\n",
    "print(\"Cross-validation scores:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Fine-tuning a Model with Pinard\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pinard.transformations import StandardNormalVariate as SNV, SavitzkyGolay as SG\n",
    "from pinard.core.runner import ExperimentRunner\n",
    "from pinard.core.config import Config\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "\n",
    "\n",
    "# Define the dataset path\n",
    "dataset_path = \"sample_data/mock_data\"\n",
    "\n",
    "# Define the data transformation pipeline with cross-validation\n",
    "x_pipeline = [\n",
    "    {\"split\": RepeatedKFold(n_splits=5, n_repeats=2, random_state=42)},\n",
    "    SG(window_length=11, polyorder=2),\n",
    "    SNV(),\n",
    "    MinMaxScaler()\n",
    "]\n",
    "\n",
    "# Define the model\n",
    "model = {\n",
    "    \"class\": \"sklearn.linear_model.Ridge\",\n",
    "    \"model_params\": {}\n",
    "}\n",
    "\n",
    "# Define the finetune parameters\n",
    "finetune_params = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"finetune_params\": {\n",
    "        'model_params': {\n",
    "            'alpha': ('float', 0.1, 10.0)\n",
    "        },\n",
    "        'training_params': {},\n",
    "        'n_trials': 20,\n",
    "        'tuner': 'sklearn'  # Use scikit-learn's GridSearchCV\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define the configuration\n",
    "config = Config(\n",
    "    dataset_path=dataset_path,\n",
    "    x_pipeline=x_pipeline,\n",
    "    y_pipeline=None,\n",
    "    model=model,\n",
    "    experiment_params=finetune_params,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Run the experiment\n",
    "runner = ExperimentRunner(configs=[config])\n",
    "dataset, model_manager = runner.run()\n",
    "\n",
    "# Get the best model and its parameters\n",
    "best_model = model_manager.models[0].best_model\n",
    "best_params = model_manager.models[0].best_params\n",
    "\n",
    "print(\"Best model parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Fine-tuning a Model with Pinard\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pinard.transformations import StandardNormalVariate as SNV, SavitzkyGolay as SG\n",
    "from pinard.core.runner import ExperimentRunner\n",
    "from pinard.core.config import Config\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "\n",
    "\n",
    "# Define the dataset path\n",
    "dataset_path = \"sample_data/mock_data\"\n",
    "\n",
    "# Define the data transformation pipeline with cross-validation\n",
    "x_pipeline = [\n",
    "    {\"split\": RepeatedKFold(n_splits=5, n_repeats=2, random_state=42)},\n",
    "    SG(window_length=11, polyorder=2),\n",
    "    SNV(),\n",
    "    MinMaxScaler()\n",
    "]\n",
    "\n",
    "# Define the model\n",
    "model = {\n",
    "    \"class\": \"sklearn.linear_model.Ridge\",\n",
    "    \"model_params\": {}\n",
    "}\n",
    "\n",
    "# Define the finetune parameters\n",
    "finetune_params = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"finetune_params\": {\n",
    "        'model_params': {\n",
    "            'alpha': ('float', 0.1, 10.0)\n",
    "        },\n",
    "        'training_params': {},\n",
    "        'n_trials': 20,\n",
    "        'tuner': 'sklearn'  # Use scikit-learn's GridSearchCV\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define the configuration\n",
    "config = Config(\n",
    "    dataset_path=dataset_path,\n",
    "    x_pipeline=x_pipeline,\n",
    "    y_pipeline=None,\n",
    "    model=model,\n",
    "    experiment_params=finetune_params,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Run the experiment\n",
    "runner = ExperimentRunner(configs=[config])\n",
    "dataset, model_manager = runner.run()\n",
    "\n",
    "# Get the best model and its parameters\n",
    "best_model = model_manager.models[0].best_model\n",
    "best_params = model_manager.models[0].best_params\n",
    "\n",
    "print(\"Best model parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6: Fine-tuning a Custom TensorFlow Model with Pinard\n",
    "\n",
    "from kerastuner import HyperModel\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pinard.transformations import StandardNormalVariate as SNV, SavitzkyGolay as SG\n",
    "from pinard.core.runner import ExperimentRunner\n",
    "from pinard.core.config import Config\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "\n",
    "\n",
    "# Define the dataset path\n",
    "dataset_path = \"sample_data/mock_data\"\n",
    "\n",
    "# Placeholder for input shape (to be defined later)\n",
    "input_shape = None\n",
    "\n",
    "# Define a hypermodel for Keras Tuner\n",
    "\n",
    "\n",
    "class MyHyperModel(HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = tf.keras.Sequential()\n",
    "        units = hp.Int('units', min_value=32, max_value=128, step=32)\n",
    "        model.add(tf.keras.layers.Dense(units=units, activation='relu', input_shape=(input_shape,)))\n",
    "        model.add(tf.keras.layers.Dense(1))\n",
    "        optimizer = hp.Choice('optimizer', ['adam', 'sgd'])\n",
    "        model.compile(optimizer=optimizer, loss='mse')\n",
    "        return model\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = {\n",
    "    \"class\": \"tensorflow.keras.models.Sequential\",\n",
    "    \"model_params\": {\n",
    "        \"build_fn\": MyHyperModel()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define the finetune parameters\n",
    "finetune_params = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"finetune_params\": {\n",
    "        'model_params': {\n",
    "            'units': ('int', 32, 128, 32),\n",
    "            'optimizer': ['adam', 'sgd']\n",
    "        },\n",
    "        'training_params': {\n",
    "            'epochs': 50,\n",
    "            'batch_size': 32\n",
    "        },\n",
    "        'n_trials': 20,\n",
    "        'tuner': 'keras'  # Use Keras Tuner\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define the data transformation pipeline with cross-validation\n",
    "x_pipeline = [\n",
    "    {\"split\": RepeatedKFold(n_splits=5, n_repeats=2, random_state=42)},\n",
    "    SG(window_length=11, polyorder=2),\n",
    "    SNV(),\n",
    "    MinMaxScaler()\n",
    "]\n",
    "\n",
    "# Define the configuration\n",
    "config = Config(\n",
    "    dataset_path=dataset_path,\n",
    "    x_pipeline=x_pipeline,\n",
    "    y_pipeline=None,\n",
    "    model=model,\n",
    "    experiment_params=finetune_params,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Run the experiment\n",
    "runner = ExperimentRunner(configs=[config])\n",
    "\n",
    "# Set the input shape after data is loaded\n",
    "dataset, model_manager = runner.load_data_only()\n",
    "input_shape = dataset.x_train.shape[1]\n",
    "model['model_params']['build_fn'] = MyHyperModel()\n",
    "\n",
    "# Run the finetuning\n",
    "dataset, model_manager = runner.run()\n",
    "\n",
    "# Get the best model and its parameters\n",
    "best_model = model_manager.models[0].best_model\n",
    "best_params = model_manager.models[0].best_params\n",
    "\n",
    "print(\"Best model parameters:\", best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

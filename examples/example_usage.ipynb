{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "\n",
    "import time\n",
    "from pinard.presets.ref_models import decon, bacon, customizable_bacon, bacon_classification\n",
    "from pinard.presets.preprocessings import decon_set, bacon_set\n",
    "from pinard.data_splitters import KennardStoneSplitter\n",
    "from pinard.transformations import StandardNormalVariate as SNV, SavitzkyGolay as SG, Gaussian as GS, Derivate as  Dv\n",
    "from pinard.transformations import Rotate_Translate as RT, Spline_X_Simplification as SXS, Random_X_Operation as RXO\n",
    "from pinard.transformations import CropTransformer\n",
    "from pinard.core.runner import ExperimentRunner\n",
    "from pinard.core.config import Config\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, RepeatedKFold, StratifiedKFold, RepeatedStratifiedKFold, ShuffleSplit, GroupKFold, StratifiedShuffleSplit, BaseCrossValidator, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "model_sklearn = {\n",
    "    \"class\": \"sklearn.cross_decomposition.PLSRegression\",\n",
    "    \"model_params\": {\n",
    "        \"n_components\": 21,\n",
    "    }\n",
    "}\n",
    "    \n",
    "finetune_pls_experiment = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"finetune_params\": {\n",
    "        'model_params': {\n",
    "            'n_components': ('int', 5, 20),\n",
    "        },\n",
    "        'training_params': {},\n",
    "        'tuner': 'sklearn'\n",
    "    }\n",
    "}\n",
    "\n",
    "bacon_train = {\"action\": \"train\", \"training_params\": {\"epochs\": 2000, \"batch_size\": 500, \"patience\": 200, \"cyclic_lr\": True, \"base_lr\": 1e-6, \"max_lr\": 1e-3, \"step_size\": 400}}\n",
    "bacon_train_short = {\"action\": \"train\", \"training_params\": {\"epochs\": 10, \"batch_size\": 500, \"patience\": 20, \"cyclic_lr\": True, \"base_lr\": 1e-6, \"max_lr\": 1e-3, \"step_size\": 40}}\n",
    "bacon_finetune = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"finetune_params\": {\n",
    "        \"n_trials\": 5,\n",
    "        \"model_params\": {\n",
    "            \"filters_1\": [8, 16, 32, 64], \n",
    "            \"filters_2\": [8, 16, 32, 64], \n",
    "            \"filters_3\": [8, 16, 32, 64]\n",
    "        }\n",
    "    },\n",
    "    \"training_params\": {\n",
    "        \"epochs\": 10,\n",
    "    }\n",
    "}\n",
    "\n",
    "full_bacon_finetune = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"training_params\": {\n",
    "        \"epochs\": 500,\n",
    "        \"patience\": 100,\n",
    "    },\n",
    "    \"finetune_params\": {\n",
    "        \"nb_trials\": 150,\n",
    "        \"model_params\": {\n",
    "            'spatial_dropout': (float, 0.01, 0.5),\n",
    "            'filters1': [4, 8, 16, 32, 64, 128, 256],\n",
    "            'kernel_size1': [3, 5, 7, 9, 11, 13, 15],\n",
    "            # 'strides1': [1, 2, 3, 4, 5],\n",
    "            # 'activation1': ['relu', 'selu', 'elu', 'swish'],\n",
    "            'dropout_rate': (float, 0.01, 0.5),\n",
    "            'filters2': [4, 8, 16, 32, 64, 128, 256],\n",
    "            # 'kernel_size2': [3, 5, 7, 9, 11, 13, 15],\n",
    "            # 'strides2': [1, 2, 3, 4, 5],\n",
    "            'activation2': ['relu', 'selu', 'elu', 'swish'],\n",
    "            'normalization_method1': ['BatchNormalization', 'LayerNormalization'],\n",
    "            'filters3': [4, 8, 16, 32, 64, 128, 256],\n",
    "            # 'kernel_size3': [3, 5, 7, 9, 11, 13, 15],\n",
    "            # 'strides3': [1, 2, 3, 4, 5],\n",
    "            'activation3': ['relu', 'selu', 'elu', 'swish'],\n",
    "            # 'normalization_method2': ['BatchNormalization', 'LayerNormalization'],\n",
    "            # 'dense_units': [4, 8, 16, 32, 64, 128, 256],\n",
    "            'dense_activation': ['relu', 'selu', 'elu', 'swish'],\n",
    "        },\n",
    "        # \"training_params\": {\n",
    "        #     \"batch_size\": [32, 64, 128, 256, 512],\n",
    "        #     \"cyclic_lr\": [True, False],\n",
    "        #     \"base_lr\": (float, 1e-6, 1e-2),\n",
    "        #     \"max_lr\": (float, 1e-3, 1e-1),\n",
    "        #     \"step_size\": (int, 500, 5000),\n",
    "        # },\n",
    "    }\n",
    "}\n",
    "\n",
    "x_pipeline_PLS = [\n",
    "    RobustScaler(),\n",
    "    # {\"samples\": [None, SXS, RXO]},\n",
    "    # {\"split\": RepeatedKFold(n_splits=3, n_repeats=1)},\n",
    "    {\"features\": [None, GS(2,1), SG, SNV, Dv, [GS, SNV], [GS, GS],[GS, SG],[SG, SNV], [GS, Dv], [SG, Dv]]},\n",
    "    MinMaxScaler()\n",
    "]\n",
    "                    \n",
    "\n",
    "\n",
    "x_pipeline_full = [\n",
    "    RobustScaler(),\n",
    "    {\"samples\": [None, None,None,None,SXS, RXO]},\n",
    "    {\"split\": RepeatedKFold(n_splits=3, n_repeats=1)},\n",
    "    {\"features\": [None, GS(2,1), SG, SNV, Dv, [GS, SNV], [GS, GS],[GS, SG],[SG, SNV], [GS, Dv], [SG, Dv]]},\n",
    "    MinMaxScaler()\n",
    "]\n",
    "\n",
    "x_pipeline_full2 = [\n",
    "    RobustScaler(),\n",
    "    {\"samples\": [None, None,None,None,SXS, RXO]},\n",
    "    {\"split\": RepeatedKFold(n_splits=3, n_repeats=1)},\n",
    "    {\"features\": [None, GS(2,1), SG, SNV, Dv, [GS, SNV], [GS, GS],[GS, SG],[SG, SNV], [GS, Dv], [SG, Dv]]},\n",
    "    MinMaxScaler()\n",
    "]\n",
    "\n",
    "x_pipeline = [\n",
    "    RobustScaler(), \n",
    "    # {\"samples\": [None, SXS]}, \n",
    "    # {\"split\": RepeatedKFold(n_splits=3, n_repeats=1)}, \n",
    "    {\"features\": [None, [GS(), SNV()], SG(), GS()]}, \n",
    "    # {\"features\": [None, GS]}, \n",
    "    # {\"features\": [None, GS, SG, SNV, Dv, [GS, SNV], [GS, GS],[GS, SG],[SG, SNV], [GS, Dv], [SG, Dv]]},\n",
    "    # {\"features\": [None, SG, GS, SNV, [SG, SNV], [GS, SNV], [SG, GS]]}, \n",
    "    # bacon_set(),\n",
    "    MinMaxScaler()\n",
    "]\n",
    "bacon_finetune_classif = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"task\": \"classification\",\n",
    "    \"finetune_params\": {\n",
    "        \"n_trials\": 5,\n",
    "        \"model_params\": {\n",
    "            \"filters_1\": [8, 16, 32, 64], \n",
    "            \"filters_2\": [8, 16, 32, 64], \n",
    "            \"filters_3\": [8, 16, 32, 64]\n",
    "        }\n",
    "    },\n",
    "    \"training_params\": {\n",
    "        \"epochs\": 5,\n",
    "    }\n",
    "}\n",
    "\n",
    "finetune_randomForestclassifier = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"task\": \"classification\",\n",
    "    \"finetune_params\": {\n",
    "        'model_params': {\n",
    "            'n_estimators': ('int', 5, 20),\n",
    "        },\n",
    "        'training_params': {},\n",
    "        'tuner': 'sklearn'\n",
    "    }\n",
    "}\n",
    "\n",
    "seed = 123459456\n",
    "\n",
    "datasets = \"sample_data/mock_data3_classif\"\n",
    "y_pipeline = MinMaxScaler()\n",
    "# processing only\n",
    "config2 = Config(\"sample_data/mock_data3\", x_pipeline_full, y_pipeline, None, None, seed)\n",
    "## TRAINING\n",
    "# regression\n",
    "config1 = Config(\"sample_data/mock_data2\", x_pipeline, y_pipeline, bacon, bacon_train_short, seed)\n",
    "config4 = Config(\"sample_data/mock_data3\", x_pipeline_PLS, y_pipeline, model_sklearn, None, seed)\n",
    "# classification\n",
    "config3 = Config(\"sample_data/mock_data3_classif\", x_pipeline, None, bacon_classification, {\"task\":\"classification\", \"training_params\":{\"epochs\":5}}, seed*2)\n",
    "config11 = Config(\"sample_data/mock_data3_binary\", x_pipeline, None, bacon_classification, {\"task\":\"classification\", \"training_params\":{\"epochs\":5}}, seed*2)\n",
    "config5 = Config(\"sample_data/mock_data3_classif\", x_pipeline, None, RandomForestClassifier, {\"task\":\"classification\"}, seed*2)\n",
    "config10 = Config(\"sample_data/mock_data3_binary\", x_pipeline, None, RandomForestClassifier, {\"task\":\"classification\"}, seed*2)\n",
    "## FINETUNING\n",
    "# regression\n",
    "config6 = Config(\"sample_data/mock_data3\", x_pipeline, y_pipeline, bacon, bacon_finetune, seed)\n",
    "config7 = Config(\"sample_data/mock_data3\", x_pipeline, y_pipeline, model_sklearn, finetune_pls_experiment, seed)\n",
    "# classification\n",
    "config8 = Config(\"sample_data/mock_data3_classif\", x_pipeline, None, bacon_classification, bacon_finetune_classif, seed*2)\n",
    "config9 = Config(\"sample_data/mock_data3_classif\", x_pipeline, None, RandomForestClassifier, finetune_randomForestclassifier, seed*2)\n",
    "\n",
    "\n",
    "# configs = [config1, config2, config3, config4, config5, config6, config7, config8, config9]\n",
    "configs = [config3, config5]\n",
    "\n",
    "start = time.time()\n",
    "runner = ExperimentRunner(configs, resume_mode=\"restart\")\n",
    "dataset, model_manager = runner.run()\n",
    "end = time.time()\n",
    "print(f\"Time elapsed: {end-start} seconds\")\n",
    "\n",
    "\n",
    "# print(dataset)\n",
    "# print(dataset.raw_x_train.shape)\n",
    "# print(dataset.to_str(\"union\"))\n",
    "\n",
    "\n",
    "# # chart all sample transformations\n",
    "# sample_0 = dataset.x_train[0][0]\n",
    "# print(sample_0.shape)\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# fig, axs = plt.subplots(5, 5, figsize=(15, 5))\n",
    "# for i, ax in enumerate(axs.flat):\n",
    "#     ax.plot(sample_0[i])\n",
    "#     ax.set_title(f\"Sample {i}\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# class AddVal(TransformerMixin, BaseEstimator):\n",
    "#     def __init__(self, val):\n",
    "#         self.val = val\n",
    "\n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         return X + self.val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinard\n",
    "print(pinard.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ace_tools as tools\n",
    "import pandas as pd\n",
    "\n",
    "# Sklearn metrics list\n",
    "sklearn_metrics = [\n",
    "    \"explained_variance\", \"r2\", \"max_error\", \"matthews_corrcoef\",\n",
    "    \"neg_median_absolute_error\", \"neg_mean_absolute_error\",\n",
    "    \"neg_mean_absolute_percentage_error\", \"neg_mean_squared_error\",\n",
    "    \"neg_mean_squared_log_error\", \"neg_root_mean_squared_error\",\n",
    "    \"neg_root_mean_squared_log_error\", \"neg_mean_poisson_deviance\",\n",
    "    \"neg_mean_gamma_deviance\", \"d2_absolute_error_score\", \"accuracy\",\n",
    "    \"top_k_accuracy\", \"roc_auc\", \"roc_auc_ovr\", \"roc_auc_ovo\",\n",
    "    \"roc_auc_ovr_weighted\", \"roc_auc_ovo_weighted\", \"balanced_accuracy\",\n",
    "    \"average_precision\", \"neg_log_loss\", \"neg_brier_score\",\n",
    "    \"positive_likelihood_ratio\", \"neg_negative_likelihood_ratio\",\n",
    "    \"adjusted_rand_score\", \"rand_score\", \"homogeneity_score\",\n",
    "    \"completeness_score\", \"v_measure_score\", \"mutual_info_score\",\n",
    "    \"adjusted_mutual_info_score\", \"normalized_mutual_info_score\",\n",
    "    \"fowlkes_mallows_score\"\n",
    "]\n",
    "\n",
    "# Tensorflow/keras metrics list\n",
    "tensorflow_metrics = [\n",
    "    \"MeanSquaredError\", \"RootMeanSquaredError\", \"MeanAbsoluteError\",\n",
    "    \"MeanAbsolutePercentageError\", \"MeanSquaredLogarithmicError\",\n",
    "    \"CosineSimilarity\", \"LogCoshError\", \"R2Score\", \"AUC\",\n",
    "    \"FalseNegatives\", \"FalsePositives\", \"Precision\", \"PrecisionAtRecall\",\n",
    "    \"Recall\", \"RecallAtPrecision\", \"SensitivityAtSpecificity\",\n",
    "    \"SpecificityAtSensitivity\", \"TrueNegatives\", \"TruePositives\",\n",
    "    \"Hinge\", \"SquaredHinge\", \"CategoricalHinge\", \"KLDivergence\",\n",
    "    \"Poisson\", \"BinaryCrossentropy\", \"CategoricalCrossentropy\",\n",
    "    \"SparseCategoricalCrossentropy\", \"Accuracy\", \"BinaryAccuracy\",\n",
    "    \"CategoricalAccuracy\", \"SparseCategoricalAccuracy\",\n",
    "    \"TopKCategoricalAccuracy\", \"SparseTopKCategoricalAccuracy\",\n",
    "    \"F1Score\", \"FBetaScore\", \"IoU\", \"BinaryIoU\", \"MeanIoU\",\n",
    "    \"OneHotIoU\", \"OneHotMeanIoU\"\n",
    "]\n",
    "\n",
    "# Metric name mapping: (tensorflow_name, sklearn_name, abbreviation, method_name)\n",
    "# Initialize with common names\n",
    "metrics_mapping = [\n",
    "    (\"MeanSquaredError\", \"neg_mean_squared_error\", \"mse\", \"Mean Squared Error\"),\n",
    "    (\"RootMeanSquaredError\", \"neg_root_mean_squared_error\", \"rmse\", \"Root Mean Squared Error\"),\n",
    "    (\"MeanAbsoluteError\", \"neg_mean_absolute_error\", \"mae\", \"Mean Absolute Error\"),\n",
    "    (\"MeanAbsolutePercentageError\", \"neg_mean_absolute_percentage_error\", \"mape\", \"Mean Absolute Percentage Error\"),\n",
    "    (\"MeanSquaredLogarithmicError\", \"neg_mean_squared_log_error\", \"msle\", \"Mean Squared Logarithmic Error\"),\n",
    "    (\"CosineSimilarity\", None, \"cos_sim\", \"Cosine Similarity\"),\n",
    "    (\"LogCoshError\", None, \"log_cosh\", \"Log Cosh Error\"),\n",
    "    (\"R2Score\", \"r2\", \"r2\", \"R2 Score\"),\n",
    "    (\"AUC\", \"roc_auc\", \"auc\", \"Area Under the Curve\"),\n",
    "    (\"Precision\", None, \"prec\", \"Precision\"),\n",
    "    (\"Recall\", None, \"recall\", \"Recall\"),\n",
    "    (\"Accuracy\", \"accuracy\", \"acc\", \"Accuracy\"),\n",
    "    (\"TopKCategoricalAccuracy\", \"top_k_accuracy\", \"top_k_acc\", \"Top K Categorical Accuracy\"),\n",
    "    (\"BinaryCrossentropy\", None, \"bin_crossentropy\", \"Binary Crossentropy\"),\n",
    "    (\"CategoricalCrossentropy\", None, \"cat_crossentropy\", \"Categorical Crossentropy\"),\n",
    "    (\"SparseCategoricalCrossentropy\", None, \"sparse_cat_crossentropy\", \"Sparse Categorical Crossentropy\"),\n",
    "    (\"F1Score\", None, \"f1\", \"F1 Score\"),\n",
    "    (\"IoU\", None, \"iou\", \"Intersection over Union\")\n",
    "]\n",
    "\n",
    "# Add remaining metrics with None in the missing columns\n",
    "for metric in sklearn_metrics:\n",
    "    if not any(metric in row for row in metrics_mapping):\n",
    "        metrics_mapping.append((None, metric, None, None))\n",
    "\n",
    "for metric in tensorflow_metrics:\n",
    "    if not any(metric in row for row in metrics_mapping):\n",
    "        metrics_mapping.append((metric, None, None, None))\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame(metrics_mapping, columns=[\"tensorflow_name\", \"sklearn_name\", \"abbreviation\", \"method_name\"])\n",
    "\n",
    "# Display the dataframe to the user\n",
    "tools.display_dataframe_to_user(name=\"Metric Comparison\", dataframe=df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

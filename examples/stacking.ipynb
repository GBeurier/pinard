{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation\n",
        "The code below is installing the pinard and scikeras packages via pip, the package installer for Python. pinard is a package designed to perform preprocessing and modeling of spectral data, and scikeras is a package that allows to use Keras models in scikit-learn. By running these commands, the packages will be installed and can be imported and used in the current notebook or environment"
      ],
      "metadata": {
        "id": "k68WS2_bUMbq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0pl0TqfTUmA"
      },
      "outputs": [],
      "source": [
        "!pip install pinard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXHWoU0zTUmB"
      },
      "source": [
        "# Combine predictors using Stacking\n",
        "\n",
        "\n",
        "## Loading data\n",
        "\n",
        "The code above is using the pinard package to load and preprocess data from the provided CSV files. The data is loaded into the x and y variables and then split into training and test sets using the train_test_split_idx function. The preprocessing pipeline is then declared with a list of preprocessing steps, each step is a tuple containing a name and an instance of a preprocessing class. The preprocessing steps include : 'id' for identity transformer, 'savgol' for Savitzky-Golay smoothing filter, 'gaussian1' and 'gaussian2' for Gaussian filter with different parameter, 'haar' for Haar wavelet filter, and several steps that are composed of different preprocessing steps using the pipeline class from scikit-learn. These steps will be applied in sequence to the data in the final pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTpea5lbTUmD",
        "outputId": "450752ca-51bd-473a-81e7-48c95c212239"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(270, 2151) (270,) (91, 2151) (91,)\n"
          ]
        }
      ],
      "source": [
        "# Standard loading and preprocessing code\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from pinard import utils\n",
        "from pinard import preprocessing as pp\n",
        "from pinard.model_selection import train_test_split_idx\n",
        "\n",
        "# Init basic random\n",
        "rd_seed = 42\n",
        "np.random.seed(rd_seed)\n",
        "\n",
        "xcal_csv = \"https://raw.githubusercontent.com/GBeurier/pinard/main/examples/Xcal.csv\"\n",
        "ycal_csv = \"https://raw.githubusercontent.com/GBeurier/pinard/main/examples/Ycal.csv\"\n",
        "\n",
        "# Create a set named data\n",
        "x, y = utils.load_csv(xcal_csv, ycal_csv, x_hdr=0, y_hdr=0, remove_na=True)\n",
        "train_index, test_index = train_test_split_idx(x, y=y, method=\"random\", test_size=0.25, random_state=rd_seed)\n",
        "X_train, y_train, X_test, y_test = x[train_index], y[train_index], x[test_index], y[test_index]\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "### Declare preprocessing pipeline components\n",
        "preprocessing = [   ('id', pp.IdentityTransformer()),\n",
        "                    ('savgol', pp.SavitzkyGolay()),\n",
        "                    ('gaussian1', pp.Gaussian(order = 1, sigma = 2)),\n",
        "                    ('gaussian2', pp.Gaussian(order = 2, sigma = 1)),\n",
        "                    ('haar', pp.Wavelet('haar')),\n",
        "                    ('savgol*savgol', Pipeline([('_sg1',pp.SavitzkyGolay()),('_sg2',pp.SavitzkyGolay())])),\n",
        "                    ('gaussian1*savgol', Pipeline([('_g1',pp.Gaussian(order = 1, sigma = 2)),('_sg3',pp.SavitzkyGolay())])),\n",
        "                    ('gaussian2*savgol', Pipeline([('_g2',pp.Gaussian(order = 1, sigma = 2)),('_sg4',pp.SavitzkyGolay())])),\n",
        "                    ('haar*savgol', Pipeline([('_haar2',pp.Wavelet('haar')),('_sg5',pp.SavitzkyGolay())]))\n",
        "                ]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lh_OkcQ9TUmE"
      },
      "source": [
        "## Pipelines creation\n",
        "\n",
        "This code is setting up a pipeline for a stacking regressor. The pipeline starts with the union_pipeline which is a pipeline that contains the preprocessing steps that were defined earlier and the MinMaxScaler which scales the data between 0 and 1. The FeatureUnion class is used to combine the results of all the preprocessing steps into a single array. The get_estimator function is a helper function that creates a new pipeline by adding a regressor to the union_pipeline. This new pipeline is wrapped with a TransformedTargetRegressor object, which applies the same scaling on both the input and output data. The estimators list contains the different models that will be used in the stacking regressor. Each model is represented by a name and an instance of the TransformedTargetRegressor class. The final estimator is the RidgeCV, a Ridge regression model with built-in cross-validation. The StackingRegressor class is then initialized with the list of estimators and the final estimator. This will train the different models in parallel and then use the RidgeCV model to make the final predictions using the outputs of the other models as input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wconKQATUmF"
      },
      "outputs": [],
      "source": [
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.pipeline import FeatureUnion, make_pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "from sklearn.linear_model import RidgeCV\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "union_pipeline = make_pipeline(MinMaxScaler(), FeatureUnion(preprocessing))\n",
        "\n",
        "def get_estimator(regressor):\n",
        "    pipeline = make_pipeline(union_pipeline, regressor)\n",
        "    return TransformedTargetRegressor(regressor = pipeline, transformer = MinMaxScaler())\n",
        "\n",
        "estimators = [\n",
        "    # (\"Random Forest\", get_estimator( RandomForestRegressor(random_state=rd_seed) ) ),\n",
        "    (\"PLS\", get_estimator( PLSRegression(n_components=10) ) ),\n",
        "    (\"PLS_small\", get_estimator( PLSRegression(n_components=3) ) ),\n",
        "    # (\"XGBoost\", get_estimator( XGBRegressor() ) )\n",
        "]\n",
        "\n",
        "\n",
        "stacking_regressor = StackingRegressor(estimators=estimators, final_estimator=RidgeCV())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction and Visualization\n",
        "\n",
        "The code below is visualizing the results of the different models and the stacked regressor. It uses the cross_validate and cross_val_predict functions from scikit-learn to evaluate the models. The cross_validate function performs cross-validation on the models using the defined scoring metric (in this case R-squared and mean absolute error) and returns the results as a dictionary. The cross_val_predict function is used to generate predictions for all the samples in the dataset using the k-fold cross-validation. The plot_regression_results is a helper function that creates a scatter plot of the predicted vs true targets. The plot shows the true values on the x-axis and the predicted values on the y-axis. A perfect model will have all the points on the y=x line. A line and a scatter plot are plotted, the line y=x represents the perfect predictions. The performance of the models is measured by the R-squared metric and the mean absolute error. The function also displays the elapsed time for training and evaluating the model. At the end, the code is creating a figure with 3 rows and 2 columns of subplots to show the results of the different models, and the stacked regressor. The figure has a title, and it is showing the results using plt.show()."
      ],
      "metadata": {
        "id": "L8WU5fBgVa8H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJq-7Z-MTUmG",
        "outputId": "be5f5ea4-1add-4147-ebb3-7a98d40d3a63"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   4 out of   4 | elapsed:   10.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done   4 out of   4 | elapsed:   10.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   4 out of   4 | elapsed:    4.6s finished\n",
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   4 out of   4 | elapsed:    2.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done   4 out of   4 | elapsed:    2.3s finished\n",
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   4 out of   4 | elapsed:    2.5s finished\n",
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import cross_validate, cross_val_predict\n",
        "\n",
        "\n",
        "def plot_regression_results(ax, y_true, y_pred, title, scores, elapsed_time):\n",
        "    \"\"\"Scatter plot of the predicted vs true targets.\"\"\"\n",
        "    ax.plot(\n",
        "        [y_true.min(), y_true.max()], [y_true.min(), y_true.max()], \"--r\", linewidth=2\n",
        "    )\n",
        "    ax.scatter(y_true, y_pred, alpha=0.2)\n",
        "\n",
        "    ax.spines[\"top\"].set_visible(False)\n",
        "    ax.spines[\"right\"].set_visible(False)\n",
        "    ax.get_xaxis().tick_bottom()\n",
        "    ax.get_yaxis().tick_left()\n",
        "    ax.spines[\"left\"].set_position((\"outward\", 10))\n",
        "    ax.spines[\"bottom\"].set_position((\"outward\", 10))\n",
        "    ax.set_xlim([y_true.min(), y_true.max()])\n",
        "    ax.set_ylim([y_true.min(), y_true.max()])\n",
        "    ax.set_xlabel(\"Measured\")\n",
        "    ax.set_ylabel(\"Predicted\")\n",
        "    extra = plt.Rectangle(\n",
        "        (0, 0), 0, 0, fc=\"w\", fill=False, edgecolor=\"none\", linewidth=0\n",
        "    )\n",
        "    ax.legend([extra], [scores], loc=\"upper left\")\n",
        "    title = title + \"\\n Evaluation in {:.2f} seconds\".format(elapsed_time)\n",
        "    ax.set_title(title)\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(3, 2, figsize=(9, 7))\n",
        "axs = np.ravel(axs)\n",
        "\n",
        "for ax, (name, est) in zip(\n",
        "    axs, estimators + [(\"Stacking Regressor\", stacking_regressor)]\n",
        "):\n",
        "    start_time = time.time()\n",
        "    score = cross_validate(\n",
        "        est, x, y, cv=4, scoring=[\"r2\", \"neg_mean_absolute_error\"], n_jobs=2, verbose=2\n",
        "    )\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    y_pred = cross_val_predict(est, x, y, cv=4, n_jobs=2, verbose=1)\n",
        "\n",
        "    plot_regression_results(\n",
        "        ax,\n",
        "        y,\n",
        "        y_pred,\n",
        "        name,\n",
        "        (r\"$R^2={:.2f} \\pm {:.2f}$\" + \"\\n\" + r\"$MAE={:.2f} \\pm {:.2f}$\").format(\n",
        "            np.mean(score[\"test_r2\"]),\n",
        "            np.std(score[\"test_r2\"]),\n",
        "            -np.mean(score[\"test_neg_mean_absolute_error\"]),\n",
        "            np.std(score[\"test_neg_mean_absolute_error\"]),\n",
        "        ),\n",
        "        elapsed_time,\n",
        "    )\n",
        "\n",
        "plt.suptitle(\"Single predictors versus stacked predictors\")\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(top=0.9)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7.6 ('PyNIRS_env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "c3c3249a294db370905b327c25644ce18610bfebb370223cf3414a9c437db486"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
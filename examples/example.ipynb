{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "### init random seeds\n",
    "rd_seed = 42\n",
    "np.random.seed(rd_seed)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pinard import preprocessor as pp\n",
    "\n",
    "### Declare preprocessing pipeline components\n",
    "preprocessing = [   ('id', pp.IdentityTransformer()),\n",
    "                    ('savgol', pp.SavitzkyGolay()),\n",
    "                    # ('derivate', pp.Derivate()), \n",
    "                    ('gaussian1', pp.Gaussian(order = 1, sigma = 2)),\n",
    "                    ('gaussian2', pp.Gaussian(order = 2, sigma = 1)),\n",
    "                    ('haar', pp.Wavelet('haar')),\n",
    "                    ('savgol*savgol', Pipeline([('_sg1',pp.SavitzkyGolay()),('_sg2',pp.SavitzkyGolay())])),\n",
    "                    ('gaussian1*savgol', Pipeline([('_g1',pp.Gaussian(order = 1, sigma = 2)),('_sg3',pp.SavitzkyGolay())])),\n",
    "                    ('gaussian2*savgol', Pipeline([('_g2',pp.Gaussian(order = 1, sigma = 2)),('_sg4',pp.SavitzkyGolay())])),\n",
    "                    ('haar*savgol', Pipeline([('_haar2',pp.Wavelet('haar')),('_sg5',pp.SavitzkyGolay())]))\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinard import nirs_set as n_set\n",
    "\n",
    "### Load data\n",
    "n = n_set.NIRS_Set('data')\n",
    "\n",
    "### 1 set\n",
    "# X, y = n.load('Xcal.csv', 'Ycal.csv', x_hdr=0, y_hdr=0, y_cols=0)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = rd_seed)\n",
    "\n",
    "\n",
    "## 2 sets\n",
    "name = 'ArabifPlant_Growth_rate'\n",
    "X_train, y_train = n.load(name + '_Xcal.csv', name + '_Ycal.csv', y_cols=0)\n",
    "X_test, y_test = n.load(name + '_Xval.csv', name + '_Yval.csv', y_cols=0)\n",
    "\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple xgboost pipeline\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "xgb =  XGBRegressor()\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler()), \n",
    "    ('preprocessing', FeatureUnion(preprocessing)), \n",
    "    ('XGB', xgb)\n",
    "    # ('SVM', svm.SVR())\n",
    "])\n",
    "\n",
    "estimator = TransformedTargetRegressor(regressor = pipeline, transformer = MinMaxScaler())\n",
    "\n",
    "estimator.fit(X_train, y_train)\n",
    "Y_preds = estimator.predict(X_test)\n",
    "print(\"MAE\", mean_absolute_error(y_test, Y_preds))\n",
    "print(\"MSE\", mean_squared_error(y_test, Y_preds))\n",
    "print(\"MAPE\", mean_absolute_percentage_error(y_test, Y_preds))\n",
    "print(\"R²\", r2_score(y_test, Y_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "## Simple PLS pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler()), \n",
    "    ('preprocessing', FeatureUnion(preprocessing)), \n",
    "    ('pls', PLSRegression(n_components=7))\n",
    "])\n",
    "\n",
    "estimator = TransformedTargetRegressor(regressor = pipeline, transformer = MinMaxScaler())\n",
    "\n",
    "estimator.fit(X_train, y_train)\n",
    "Y_preds = estimator.predict(X_test)\n",
    "print(\"MAE\", mean_absolute_error(y_test, Y_preds))\n",
    "print(\"MSE\", mean_squared_error(y_test, Y_preds))\n",
    "print(\"MAPE\", mean_absolute_percentage_error(y_test, Y_preds))\n",
    "print(\"R²\", r2_score(y_test, Y_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KERAS Model\n",
    "##TODO > remove keraswrapper for sciKeras wrapper\n",
    "\n",
    "from pinard.nirs_pipelines import FeatureAugmentation\n",
    "\n",
    "\n",
    "import tensorflow\n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, SpatialDropout1D,BatchNormalization,Flatten, Dropout, Input, MaxPool1D\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger, Callback\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "from typing import Dict, Iterable, Any\n",
    "import scikeras\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tensorflow.random.set_seed(rd_seed)\n",
    "tensorflow.keras.backend.clear_session()\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler, QuantileTransformer\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "EPOCHS = 4000\n",
    "MIN_LR = 5e-5\n",
    "MAX_LR = 1e-3\n",
    "CYCLE_EPOCHS = 200\n",
    "# initializer = initializers.RandomNormal(stddev=0.01)\n",
    "\n",
    "steps_per_epoch = len(X_train) // BATCH_SIZE\n",
    "\n",
    "clr = tfa.optimizers.CyclicalLearningRate(\n",
    "    initial_learning_rate=MIN_LR,\n",
    "    maximal_learning_rate=MAX_LR,\n",
    "    scale_fn=lambda x: 1/(2.**(x-1)),\n",
    "    step_size= CYCLE_EPOCHS * steps_per_epoch\n",
    ")\n",
    "\n",
    "class Auto_Save(Callback):\n",
    "    best_weights = []\n",
    "    def __init__(self, filepath):\n",
    "        super(Auto_Save, self).__init__()\n",
    "        self.best = np.Inf\n",
    "        self.filepath = filepath\n",
    "        \n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_loss = logs.get('val_loss')\n",
    "        # print('\\nLearning rate: {}\\n'.format(self.model.optimizer.lr))\n",
    "        print(\"Learning rate:\", clr(self.model.optimizer.iterations.numpy()).numpy())\n",
    "        if np.less(current_loss, self.best):\n",
    "            self.best = current_loss            \n",
    "            Auto_Save.best_weights = self.model.get_weights()\n",
    "            # print('Saving weights validation loss= {0:6.4f}'.format(current_loss))\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        # print('\\nSaved best {0:6.4f} to {} \\n'.format(self.best, self.filepath))\n",
    "        print('\\nSaved best {0:6.4f}\\n'.format(self.best))\n",
    "        \n",
    "\n",
    "def keras_model(meta: Dict[str, Any]):\n",
    "\n",
    "    \n",
    "    step = np.arange(0, steps_per_epoch * EPOCHS)\n",
    "    lr = clr(step)\n",
    "    plt.plot(step, lr)\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Learning Rate\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    input_shape = meta[\"X_shape_\"][1:]\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(Conv1D (filters=64, kernel_size=3, padding=\"same\", activation='swish'))\n",
    "    model.add(Conv1D (filters=64, kernel_size=3, padding=\"same\", activation='swish'))\n",
    "    model.add(MaxPool1D(pool_size=7,strides=5))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(Conv1D (filters=128, kernel_size=3, padding=\"same\", activation='swish'))\n",
    "    model.add(Conv1D (filters=128, kernel_size=3, padding=\"same\", activation='swish'))\n",
    "    model.add(MaxPool1D(pool_size=7,strides=5))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=2048, activation=\"swish\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=2048, activation=\"swish\"))\n",
    "    model.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "    model.compile(loss = 'mean_squared_error', metrics=['mse'], optimizer = Adam(clr))\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "    \n",
    "    \n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "y_scaler.fit(y_train.reshape((-1,1)))\n",
    "y_valid = y_scaler.transform(y_test.reshape((-1,1)))\n",
    "\n",
    "transformer_pipe = Pipeline([\n",
    "    ('scaler', MinMaxScaler()), \n",
    "    ('preprocessing', FeatureAugmentation(preprocessing)), \n",
    "])\n",
    "\n",
    "transformer_pipe.fit(X_train)\n",
    "X_valid = transformer_pipe.transform(X_test)\n",
    "print(X_valid.shape)\n",
    "\n",
    "model_filepath = 'model_test.hdf5'\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=300, verbose=0, mode='min') \n",
    "# reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=200, verbose=1, min_delta=0.001, mode='min')\n",
    "# mcp_save = ModelCheckpoint(model_filepath, initial_value_threshold=0.008, save_best_only=True, monitor='val_loss', mode='min')#, save_freq=50) \n",
    "# logger = CSVLogger(\"log.csv\", append=True)\n",
    "auto_save = Auto_Save(model_filepath)\n",
    "\n",
    "kregressor = KerasRegressor(model = keras_model,\n",
    "                            callbacks=[earlyStopping, auto_save],\n",
    "                            # callbacks=[reduce_lr_loss, changer],\n",
    "                            epochs=EPOCHS, \n",
    "                            fit__batch_size=BATCH_SIZE,\n",
    "                            # fit__validation_split=0.2,\n",
    "                            fit__validation_data = (X_valid, y_valid),\n",
    "                            verbose = 2)\n",
    "# #  metrics=[KerasRegressor.r_squared])\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('trans', transformer_pipe), \n",
    "    ('KerasNN', kregressor)\n",
    "])\n",
    "\n",
    "\n",
    "estimator = TransformedTargetRegressor(regressor = pipeline, transformer = y_scaler)\n",
    "estimator.fit(X_train, y_train)\n",
    "\n",
    "# # estimator.fit(X, y)\n",
    "# estimator.regressor_[1].model_.load_weights(model_filepath)\n",
    "estimator.regressor_[1].model_.set_weights(Auto_Save.best_weights)\n",
    "Y_preds = estimator.predict(X_test)\n",
    "print(\"MAE\", mean_absolute_error(y_test, Y_preds))\n",
    "print(\"MSE\", mean_squared_error(y_test, Y_preds))\n",
    "print(\"MAPE\", mean_absolute_percentage_error(y_test, Y_preds))\n",
    "print(\"R²\", r2_score(y_test, Y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda \n",
    "cuda.select_device(0) \n",
    "cuda.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_model(meta: Dict[str, Any]):\n",
    "    input_shape = meta[\"X_shape_\"][1:]\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(SpatialDropout1D(0.08))\n",
    "    model.add(Conv1D (filters=8, kernel_size=15, strides=5, activation='selu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D (filters=64, kernel_size=21, strides=3, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D (filters=32, kernel_size=5, strides=3, activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(16, activation='sigmoid'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss = 'mean_squared_error', optimizer = 'adam')\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Explainer\n",
    "\n",
    "import shap\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "from matplotlib import pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain how the input to the 7th layer of the model explains the top two classes\n",
    "# def map2layer(x, layer):\n",
    "#     feed_dict = dict(zip([model.layers[0].input], [preprocess_input(x.copy())]))\n",
    "#     return K.get_session().run(model.layers[layer].input, feed_dict)\n",
    "\n",
    "\n",
    "# layer_in = estimator.regressor_[2].model_.layers[3]\n",
    "# layer_out = estimator.regressor_[2].model_.layers[-1]\n",
    "# layer_out\n",
    "\n",
    "# e = shap.GradientExplainer((layer_in.input, layer_out.output), map2layer(preprocess_input(X.copy()), 7))\n",
    "\n",
    "# kmodel = estimator.regressor_[2].model_\n",
    "# explainer = shap.GradientExplainer(kmodel, X_train)\n",
    "\n",
    "xtt = estimator.regressor_[:-1].transform(X_test)\n",
    "print(X_test.shape, xtt.shape)\n",
    "shap_values = explainer.shap_values(xtt[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_summary = shap.kmeans(X_train, 10)\n",
    "explainer = shap.KernelExplainer(estimator.predict, X_train_summary)\n",
    "shap_values = explainer.shap_values(X_test[0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value, shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global summary\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\", max_display=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local summary\n",
    "shap.summary_plot(shap_values, X_test[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (30,8)\n",
    "\n",
    "X_ = np.arange(350, 350+shap_values[0].shape[0])\n",
    "for s in shap_values:\n",
    "    plt.plot(X_, s, alpha=0.3)\n",
    "plt.title('SHAP values along the spectrum')\n",
    "# plt.show()\n",
    "\n",
    "for sx in  X_test[0:shap_values.shape[0]]:\n",
    "    plt.plot(X_, sx)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# X_train_summary = shap.kmeans(X_train, 10)\n",
    "\n",
    "shap_values2 = explainer2.shap_values(X_test[0:5])\n",
    "\n",
    "shap.plots.waterfall(shap_values2, max_display=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer2 = shap.Explainer(estimator.predict, X_train[0:30])\n",
    "shap_values2 = explainer2(X_test[0:10], max_evals=4400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.waterfall(shap_values2[2], shap_values2[0][0], X_test[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (10,5)\n",
    "print(shap_values[0].shape)\n",
    "\n",
    "X_ = np.arange(0, shap_values[0].shape[0])\n",
    "for s in X_test:\n",
    "    plt.plot(X_, s)\n",
    "plt.title('SHAP values along the spectrum')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b09f6e5407ec4329146609a0cb08cbbe4720f97bb26598a93c421b663bd10d2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('pynirsENV': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

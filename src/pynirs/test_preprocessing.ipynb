{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "(10, 10)\n",
      "(10, 10)\n",
      "(30, 10)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from preprocessor import SavitzkyGolay, Derivate, MultiplicativeScatterCorrection, RobustNormalVariate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pipeline_tools import FeatureUnionOnAxis\n",
    "\n",
    "X, y = make_regression(n_samples=10, n_features=10, random_state=0)\n",
    "print(X.shape)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "print(X.shape)\n",
    "union = FeatureUnionOnAxis([('id', FunctionTransformer()), ('derivate', Derivate()), ('spl', RobustNormalVariate())], axis = 1)\n",
    "a = union.fit_transform(X, y)\n",
    "print(a.shape)\n",
    "# pipe = Pipeline()\n",
    "# # The pipeline can be used as any other estimator\n",
    "# # and avoids leaking the test set into the train set\n",
    "# print(X[0,0:10])\n",
    "# T = pipe.fit_transform(X_train, y_train)\n",
    "# print(pipe)\n",
    "# print(T[0,0:10])\n",
    "\n",
    "\n",
    "# plt.rcParams['figure.figsize'] = [30,30]\n",
    "# l = np.arange(0, 1000, 1)\n",
    "# fig, AX = plt.subplots( 3 )\n",
    "# for i in range(3):\n",
    "#     AX[i].plot(l, X[i])\n",
    "#     AX[i].plot(l, T[i])\n",
    "    \n",
    "\n",
    "# pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING sample_data\\ALPINE_C_424_Murguzur\n",
      "mean 0.2502768\n",
      "std 0.22829306\n",
      "median 0.18485376\n",
      "min -0.104465626\n",
      "max 1.3915194\n",
      "percentile [0.06791676 0.40602568]\n",
      "linalg 298.51172\n",
      "polyfit [1.3631628  0.46054249]\n",
      "polyfit [[1.3631628  1.3960305  1.36012011 ... 1.1916488  1.19263494 1.19377185]\n",
      " [0.46054249 0.46635726 0.48146435 ... 0.1880057  0.187743   0.18729798]]\n",
      "polyfit (2, 2151)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10924/3336331152.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sample_data'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m \u001b[1;31m# with open(\"scores.json\", \"w\") as write_file:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;31m#     json.dump(scores, write_file, indent=4)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10924/3336331152.py\u001b[0m in \u001b[0;36mtraverse\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LOADING\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m         \u001b[1;31m# score = benchmark_folder(x,y,f)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[1;31m# scores[f] = score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "import json\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "sys.path.append(os.path.abspath(\"\") + \"/../src/pynirs\")\n",
    "from nirs_set import NIRS_Set as NSet\n",
    "import preprocessor as pp\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Conv1D, Flatten, BatchNormalization, SpatialDropout1D, Dropout\n",
    "# from tensorflow.keras.optimizers import SGD\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "# from kennard_stone import train_test_split, KFold\n",
    "# from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from matplotlib.pyplot import plot\n",
    "\n",
    "SEED = 12485\n",
    "def set_seed(sd):\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['PYTHONHASHSEED']=str(sd)\n",
    "    random.seed(sd)\n",
    "    np.random.seed(sd)\n",
    "    tf.random.set_seed(sd)\n",
    "set_seed(SEED)\n",
    "\n",
    "hello = tf.constant(\"hello TensorFlow!\")\n",
    "\n",
    "def load_path(folder):\n",
    "    n = NSet(\"example\")\n",
    "    n.load(folder+\"/XCal.csv\", folder+\"/YCal.csv\", 0, 0, 0)\n",
    "    x_src = n.get_raw_x()\n",
    "\n",
    "    print(\"mean\", np.mean(x_src))\n",
    "    print(\"std\", np.std(x_src))\n",
    "    print(\"median\", np.median(x_src))\n",
    "    print(\"min\", np.min(x_src))\n",
    "    print(\"max\", np.max(x_src))\n",
    "    print(\"percentile\", np.percentile(x_src,[20, 80]))\n",
    "    print(\"linalg\", np.linalg.norm(x_src))\n",
    "    reference = np.mean(x_src, axis = 1)\n",
    "    print(\"polyfit\", np.polyfit(reference, x_src[:, 0], deg = 1))\n",
    "    p = np.polyfit(reference, x_src, deg = 1)\n",
    "    print(\"polyfit\", p)\n",
    "    print(\"polyfit\", p.shape)\n",
    "    return\n",
    "    x = x_src.copy()\n",
    "    # PROCESSING = ['x','x*detrend','x*snv','x*savgol1','x*msc','x*derivate','x*gaussian1','x*gaussian2','x*wv_haar','x*detrend*snv','x*detrend*rnv','x*detrend*savgol1','x*detrend*msc','x*detrend*derivate','x*detrend*wv_haar','x*wv_bior2.2','x*wv_db2','x*wv_dmey','x*wv_rbio6.8','x*wv_sym4','x*snv*savgol1','x*savgol1*savgol1','x*msc*savgol1','x*derivate*savgol1','x*gaussian1*savgol1','x*gaussian2*savgol1','x*wv_haar*savgol1','x*snv*msc','x*rnv*msc','x*savgol1*msc','x*msc*msc','x*derivate*msc','x*gaussian1*msc','x*gaussian2*msc','x*wv_haar*msc','x*snv*derivate','x*savgol1*derivate','x*msc*derivate','x*derivate*derivate','x*gaussian1*derivate','x*gaussian2*derivate','x*wv_haar*derivate']\n",
    "    PROCESSING = ['x'\n",
    "                # ,'x*detrend'\n",
    "                ,'x*savgol1'\n",
    "                # ,'x*derivate'\n",
    "                ,'x*gaussian1'\n",
    "                ,'x*gaussian2'\n",
    "                ,'x*wv_haar'\n",
    "                # ,'x*detrend*savgol1'\n",
    "                # ,'x*detrend*derivate'\n",
    "                # ,'x*detrend*wv_haar'\n",
    "                # # ,'x*detrend*gaussian1'\n",
    "                # # ,'x*detrend*gaussian2'\n",
    "                # ,'x*wv_bior2.2'\n",
    "                # ,'x*wv_db2'\n",
    "                # ,'x*wv_dmey'\n",
    "                # ,'x*wv_rbio6.8'\n",
    "                # ,'x*wv_sym4'\n",
    "                # ,'x*savgol1*savgol1'\n",
    "                # ,'x*derivate*savgol1'\n",
    "                # ,'x*gaussian1*savgol1'\n",
    "                # ,'x*gaussian2*savgol1'\n",
    "                # ,'x*wv_haar*savgol1'\n",
    "                # ,'x*savgol1*derivate'\n",
    "                # ,'x*derivate*derivate'\n",
    "                # ,'x*gaussian1*derivate'\n",
    "                # ,'x*gaussian2*derivate'\n",
    "                # ,'x*wv_haar*derivate'\n",
    "                ]\n",
    "    pp_spectra = pp.process(x, PROCESSING)\n",
    "    x = np.array(list(pp_spectra.values()))\n",
    "    x = np.swapaxes(x, 0, 1)\n",
    "    x = np.swapaxes(x, 1, 2)\n",
    "    print(x[0,:,0])\n",
    "    print(x[0,:,1])\n",
    "    print(x[0,:,2])\n",
    "    \n",
    "    x = x_src.copy()\n",
    "    pp_spectra = pp.process(x[0:2], PROCESSING)\n",
    "    x = np.array(list(pp_spectra.values()))\n",
    "    x = np.swapaxes(x, 0, 1)\n",
    "    x = np.swapaxes(x, 1, 2)\n",
    "    print(x[0,:,0])\n",
    "    print(x[0,:,1])\n",
    "    print(x[0,:,2])\n",
    "    \n",
    "    y_src = n.get_raw_y()\n",
    "    scaler_y = MinMaxScaler(feature_range=(0.1,0.9))\n",
    "    y = scaler_y.fit_transform(y_src)\n",
    "    \n",
    "    return x, y, scaler_y\n",
    "\n",
    "# def learn(X_train, X_test, y_train, y_test, input_shape, name, folder):\n",
    "#     model = Sequential()\n",
    "#     model.add(SpatialDropout1D(0.08, input_shape=input_shape))\n",
    "#     model.add(Conv1D (filters=8, kernel_size=15, strides=5, activation='selu'))\n",
    "#     # model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.3))\n",
    "#     model.add(Conv1D (filters=64, kernel_size=21, strides=3, activation='relu'))\n",
    "#     # model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Conv1D (filters=32, kernel_size=5, strides=3, activation='elu'))\n",
    "#     # model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(16, activation='sigmoid'))\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#     # model.summary()\n",
    "\n",
    "#     reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=50, verbose=1, min_delta=0.5e-5, mode='min')\n",
    "#     earlyStopping = EarlyStopping(monitor='val_loss', patience=90, verbose=0, mode='min') \n",
    "#     mcp_save = ModelCheckpoint(\"tmp.model\", save_best_only=True, monitor='val_loss', mode='min') \n",
    "\n",
    "#     model.compile(loss='mean_squared_error', metrics=['mae','mse'], optimizer='rmsprop')\n",
    "\n",
    "#     history = model.fit(X_train, y_train, \n",
    "#                 epochs=600, \n",
    "#                 batch_size=500, \n",
    "#                 shuffle=True, \n",
    "#                 validation_data = (X_test, y_test),\n",
    "#                 verbose=0, \n",
    "#                 callbacks=[earlyStopping])\n",
    "\n",
    "#     best_score = min(history.history['val_mse'])\n",
    "#     best_epoch = np.argmin(history.history['val_mse'])\n",
    "#     print(folder, input_shape, name, best_score, best_epoch)\n",
    "#     return best_score\n",
    "    \n",
    "    \n",
    "# def benchmark_folder(x,y,folder):\n",
    "#     print(\"***** BENCHMARKING\", folder, \"*****\")\n",
    "#     s = x.shape\n",
    "    \n",
    "#     # X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.3)\n",
    "    \n",
    "#     scores = {}\n",
    "    \n",
    "#     kf = KFold(n_splits=4)#, random_state=SEED, shuffle=True)\n",
    "#     fold_index = 0\n",
    "#     # for train_index, test_index in kf.split(x): #sklearn split\n",
    "#     for train_index, test_index in kf.split(x[:,:,0]): #kennard stone split\n",
    "        \n",
    "#         X_train, X_test = x[train_index], x[test_index]\n",
    "#         y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "#         score = learn(X_train, X_test, y_train, y_test, (s[1], s[2]), fold_index, folder)\n",
    "#         name = str(fold_index)\n",
    "#         scores[name] = score\n",
    "            \n",
    "#         # X_train_src = X_train[:,:,0:2]\n",
    "#         # X_test_src = X_test[:,:,0:2]\n",
    "        \n",
    "#         # for i in range(15, s[2], 15):\n",
    "#         #     X_train_sub = X_train[:,:,0:i]\n",
    "#         #     X_test_sub = X_test[:,:,0:i]\n",
    "#         #     score = learn(X_train_sub, X_test_sub, y_train, y_test, (s[1], i), str(i), folder)\n",
    "#         #     name = str(fold_index) + '>0-' + str(i)\n",
    "#         #     scores[name] = score\n",
    "            \n",
    "#         # for i in range(15, s[2], 15):\n",
    "#         #     X_train_sub = X_train[:,:,i-15:i]\n",
    "#         #     X_train_sub = np.concatenate((X_train_src, X_train_sub), axis = 2)\n",
    "#         #     X_test_sub = X_test[:,:,i-15:i]\n",
    "#         #     X_test_sub = np.concatenate((X_test_src, X_test_sub), axis = 2)\n",
    "#         #     score = learn(X_train_sub, X_test_sub, y_train, y_test, (s[1], 17), str(i), folder)\n",
    "#         #     name = str(fold_index) + '>' + str(i-15) + \"-\" + str(i)\n",
    "#         #     scores[name] = score\n",
    "            \n",
    "#         fold_index += 1\n",
    "        \n",
    "#     return scores\n",
    "    \n",
    "    \n",
    "def traverse(directory):\n",
    "    scores = {}\n",
    "    folders = [x[0] for x in os.walk(directory)]\n",
    "    for f in folders:\n",
    "        if f == directory:\n",
    "            continue\n",
    "        print(\"LOADING\", f)\n",
    "        x,y,_ = load_path(f)\n",
    "        # score = benchmark_folder(x,y,f)\n",
    "        # scores[f] = score\n",
    "        break\n",
    "    return scores\n",
    "\n",
    "scores = traverse('sample_data')\n",
    "# with open(\"scores.json\", \"w\") as write_file:\n",
    "#     json.dump(scores, write_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 1. ]\n",
      " [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0. ]\n",
      " [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]]\n",
      "[[2. 2. 2. 2. 2. 2. 2. 2. 4.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 0.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = [[2,2,2,2,2,2,2,2,4],[2,2,2,2,2,2,2,2,0],[2,2,2,2,2,2,2,2,2]]\n",
    "X = np.array(X)\n",
    "\n",
    "imax = 1\n",
    "imin = 0\n",
    "f = (imax - imin)/(np.max(X) - np.min(X))\n",
    "n = X.shape\n",
    "arr = np.empty((0, n[0]), dtype = float) #create empty array for spectra\n",
    "for i in range(0, n[1]):\n",
    "    d = X[:, i]\n",
    "    dnorm = imin + f*d\n",
    "    arr = np.append(arr, [dnorm], axis = 0)\n",
    "    \n",
    "X = np.transpose(arr)\n",
    "print(X)\n",
    "\n",
    "n = X.shape\n",
    "arr = np.empty((0, n[0]), dtype = float) #create empty array for spectra\n",
    "for i in range(0, n[1]):\n",
    "    d = X[:, i]\n",
    "    dnorm = d/f - imin\n",
    "    arr = np.append(arr, [dnorm], axis = 0)\n",
    "\n",
    "X = np.transpose(arr)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11852/135898606.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Compute model terms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwave\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mwave\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mwave\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mp2\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwave\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mp1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = [[2,2,2,2,2,2,2,2,4],[2,2,2,2,2,2,2,2,0],[2,2,2,2,2,2,2,2,2]]\n",
    "spectra = X\n",
    "\n",
    "wave = np.array([1, 2, 1, 2])\n",
    "\n",
    "p1 = .5 * (wave[0] + wave[-1])\n",
    "p2 = 2 / (wave[0] - wave[-1])\n",
    "\n",
    "# Compute model terms\n",
    "model = np.ones((wave.size(), 4))\n",
    "model[:, 1] = p2 * (wave[0] - wave) - 1\n",
    "model[:, 2] = (p2 ** 2) * ((wave - p1) ** 2)\n",
    "model[:, 3] = np.mean(spectra, axis = 1)\n",
    "\n",
    "# Solve correction parameters\n",
    "params = np.linalg.lstsq(model, spectra)[0].T\n",
    "\n",
    "# Apply correction\n",
    "spectra = spectra - np.dot(params[:, :-1], model[:, :-1].T).T\n",
    "spectra = np.multiply(spectra, 1 / np.repeat(params[:, -1].reshape(1, -1), spectra.shape[0], axis = 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.   0.   0.5  0.  -0.5  0.   0.   1.   2. ]\n",
      " [ 0.   0.   3.   0.  -3.   0.   0.  -1.  -2. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0. ]]\n",
      "[2. 2. 2. 3. 2. 2. 2. 2. 4.]\n",
      "[2. 2. 2. 8. 2. 2. 2. 2. 0.]\n",
      "[2. 2. 2. 2. 2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "from scipy import signal\n",
    "import scipy.ndimage as nd\n",
    "X = [[2,2,2,3,2,2,2,2,4],[2,2,2,8,2,2,2,2,0],[2,2,2,2,2,2,2,2,2]]\n",
    "X = np.array(X)\n",
    "\n",
    "A = np.gradient(X, axis=1)\n",
    "print(A)\n",
    "for i in range(len(X)):\n",
    "    a = A[i]\n",
    "    x = X[i]\n",
    "    b = x[0] + 2 * np.c_[np.r_[0, a[1: -1: 2].cumsum()], a[::2].cumsum() - a[0] / 2].ravel()[: len(a)]\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.22474487 -1.22474487 -1.22474487 -1.22474487 -1.22474487 -1.22474487\n",
      "  -1.22474487 -1.22474487  1.22474487]\n",
      " [ 0.          0.          0.          1.22474487  0.          0.\n",
      "   0.          0.         -1.22474487]\n",
      " [ 1.22474487  1.22474487  1.22474487  0.          1.22474487  1.22474487\n",
      "   1.22474487  1.22474487  0.        ]]\n",
      "[[-1.22474487 -1.22474487 -1.22474487 -1.22474487 -1.22474487 -1.22474487\n",
      "  -1.22474487 -1.22474487  1.22474487]\n",
      " [ 0.          0.          0.          1.22474487  0.          0.\n",
      "   0.          0.         -1.22474487]\n",
      " [ 1.22474487  1.22474487  1.22474487  0.          1.22474487  1.22474487\n",
      "   1.22474487  1.22474487  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import preprocessor as pp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "b = pp.StandardNormalVariate()\n",
    "X = [[1,1,1,0,1,1,1,1,4],[2,2,2,4,2,2,2,2,0],[3,3,3,2,3,3,3,3,2]]\n",
    "b.fit(X)\n",
    "# print(b.std_)\n",
    "x = b.transform(X)\n",
    "print(x)\n",
    "\n",
    "print(StandardScaler().fit_transform(X))\n",
    "# print(b.inverse_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.20143051  0.1097772  -0.61450787  1.4963892  -0.28725745 -1.51958579\n",
      "  -0.01017149  0.34425589  0.37918193  1.40525922 -1.22575484 -0.39632028\n",
      "  -0.17702553  0.6605248   1.39023186  0.04719278  0.3466409  -0.67954721\n",
      "  -0.67897811  1.32887549]\n",
      " [-1.82490933 -0.22867789 -1.81905118  0.23149312  0.03240633  0.73447346\n",
      "  -0.26554663  0.26920605  0.7099471  -0.80341893  0.46642547  1.2069917\n",
      "   0.26057525  0.84975242 -0.7164621   0.43009793 -1.46607815 -0.74871409\n",
      "   0.01645979 -1.20541019]\n",
      " [ 0.47786435  0.45886516  1.24086626  0.26425757  0.00950691  1.046114\n",
      "   0.05481998 -1.45264818  0.76295341 -0.09989035  0.65398573 -0.50767265\n",
      "  -0.66375955  0.07180835 -1.26407854 -0.11613216  1.37404346 -1.24752758\n",
      "   0.23331549 -0.5699728 ]]\n",
      "(3, 80)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import *\n",
    "r = np.random.randn(3,20)\n",
    "print(r)\n",
    "t = KernelCenterer().fit_transform(r)\n",
    "print(t.shape)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a027d9ccaa4161f7bb2a6671f63bb76a78cb94a27161911cf16ac7661ccf92de"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('pynirsENV': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

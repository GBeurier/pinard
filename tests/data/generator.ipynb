{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pinard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate preprocessing test data - Snapshot (22-10-20 - commit:5544c8f119baea5f8a3d99f7ba67c1e914b98b9c)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "\n",
    "from pinard import preprocessor as pp\n",
    "from pinard.nirs_pipelines import FeatureAugmentation\n",
    "\n",
    "x_fname = 'test_preprocessing_src.csv'\n",
    "x_df = pd.read_csv(x_fname, sep=';', header=None)\n",
    "x = x_df.astype(np.float32).values\n",
    "\n",
    "preprocessing = [   ('Id', pp.IdentityTransformer()),\n",
    "                    ('Baseline', pp.Baseline()),\n",
    "                    ('StandardNormalVariate', pp.StandardNormalVariate()), \n",
    "                    ('RobustNormalVariate', pp.RobustNormalVariate()),\n",
    "                    ('SavitzkyGolay', pp.SavitzkyGolay()),\n",
    "                    ('Normalize', pp.Normalize()),\n",
    "                    ('Detrend', pp.Detrend()),\n",
    "                    ('MultiplicativeScatterCorrection', pp.MultiplicativeScatterCorrection()),\n",
    "                    ('Derivate', pp.Derivate()),\n",
    "                    ('Gaussian', pp.Gaussian(order = 2, sigma = 1)),\n",
    "                    ('Wavelet', pp.Wavelet()),\n",
    "                    ('SimpleScale', pp.SimpleScale()),\n",
    "                ]\n",
    "\n",
    "pipeline = FeatureAugmentation(preprocessing)\n",
    "xt = pipeline.fit_transform(x)\n",
    "xtt = np.swapaxes(xt, 1, 2)\n",
    "a = pp.baseline(x)[0]\n",
    "xtt = np.concatenate(xtt)\n",
    "b = xtt[0:12,:]\n",
    "np.savetxt(\"test_preprocessing_validation.csv\", b, delimiter=\";\")\n",
    "np.savetxt(\"test_data.csv\", x, delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14948/3419856023.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mShuffleSplit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpinard\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mx_fname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'test_split.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "## Generate model selection test data - Snapshot (22-10-20 - commit:5544c8f119baea5f8a3d99f7ba67c1e914b98b9c)\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from pinard import model_selection\n",
    "\n",
    "x_fname = 'test_split.csv'\n",
    "x_df = pd.read_csv(x_fname, sep=';', header=None)\n",
    "x = x_df.astype(np.float32).values\n",
    "y = np.reshape(x[:,0], (-1,1))\n",
    "x = x[:,1:]\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "train_index, test_index = model_selection.train_test_split_idx(x, method=\"random\", test_size=0.25, random_state=42)\n",
    "test_data = train_index\n",
    "\n",
    "train_index, test_index = model_selection.train_test_split_idx(x, method=\"k_mean\", test_size=0.25, random_state=42, metric= \"canberra\")\n",
    "test_data = np.column_stack((test_data, train_index))\n",
    "\n",
    "train_index, test_index = model_selection.train_test_split_idx(x, method=\"k_mean\", test_size=0.25, random_state=42, pca_components=4, metric= \"canberra\")\n",
    "test_data = np.column_stack((test_data, train_index))\n",
    "\n",
    "train_index, test_index = model_selection.train_test_split_idx(x, method=\"kennard_stone\", test_size=0.25, random_state=42)\n",
    "test_data = np.column_stack((test_data, train_index))\n",
    "\n",
    "train_index, test_index = model_selection.train_test_split_idx(x, method=\"kennard_stone\", test_size=0.25, random_state=42, metric='correlation', pca_components=8)\n",
    "test_data = np.column_stack((test_data, train_index))\n",
    "\n",
    "train_index, test_index = model_selection.train_test_split_idx(x, method=\"kennard_stone\", test_size=0.25, random_state=42, metric='correlation')\n",
    "test_data = np.column_stack((test_data, train_index))\n",
    "\n",
    "train_index, test_index = model_selection.train_test_split_idx(x, y=y, method=\"spxy\", test_size=0.25, random_state=42)\n",
    "test_data = np.column_stack((test_data, train_index))\n",
    "\n",
    "train_index, test_index = model_selection.train_test_split_idx(x, y=y, method=\"spxy\", test_size=0.25, random_state=42, pca_components=2)\n",
    "test_data = np.column_stack((test_data, train_index))\n",
    "\n",
    "train_index, test_index = model_selection.train_test_split_idx(x, y=y, method=\"spxy\", test_size=0.25, random_state=42, metric='canberra')\n",
    "test_data = np.column_stack((test_data, train_index))\n",
    "\n",
    "train_index, test_index = model_selection.train_test_split_idx(x, y=y, method=\"stratified\", test_size=0.25, random_state=42)\n",
    "test_data = np.column_stack((test_data, train_index))\n",
    "\n",
    "train_index, test_index = model_selection.train_test_split_idx(x, y=y, method=\"stratified\", test_size=0.25, random_state=42, n_bins=4)\n",
    "test_data = np.column_stack((test_data, train_index))\n",
    "\n",
    "train_index, test_index = model_selection.train_test_split_idx(x, y=y, method=\"circular\", test_size=0.25, random_state=42)\n",
    "test_data = np.column_stack((test_data, train_index))\n",
    "\n",
    "# train_index, test_index = model_selection.train_test_split_idx(x, method=\"SPlit\", test_size=0.25, random_state=42)\n",
    "# test_data = np.column_stack((test_data, train_index))\n",
    "\n",
    "np.savetxt(\"test_split_validation.csv\", test_data.astype(np.int32), delimiter=\";\", fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "[[0.72180432 0.72554117 0.72992826 0.7402032  0.75501108 0.76992512\n",
      "  0.77215958 0.76993704 0.79868561 0.80254292]\n",
      " [0.81684974 0.81926377 0.82232804 0.83128017 0.84476523 0.85835645\n",
      "  0.85926809 0.85572273 0.88314848 0.88568298]\n",
      " [0.86521149 0.86832756 0.87789011 0.89377493 0.90516847 0.90261436\n",
      "  0.90387791 0.9171114  0.93558824 0.93859202]\n",
      " [0.76445829 0.7680819  0.778152   0.79454435 0.80644544 0.80439886\n",
      "  0.80616996 0.81991098 0.83889536 0.84240668]\n",
      " [0.87479496 0.87715763 0.87726188 0.88246244 0.89566934 0.90164775\n",
      "  0.91074002 0.92526674 0.93393207 0.93441856]\n",
      " [0.97002107 0.9710499  0.96982031 0.97368704 0.9855601  0.99020467\n",
      "  0.99796311 1.011156   1.01848748 1.01764014]\n",
      " [0.89307523 0.89902884 0.90493369 0.91273469 0.9155581  0.91560477\n",
      "  0.92193139 0.93618143 0.94984376 0.95042825]\n",
      " [1.04906841 1.05418637 1.05925558 1.06622093 1.06820869 1.06741972\n",
      "  1.07291069 1.08632508 1.09915177 1.09890061]]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "## Generate augmentation test data - Snapshot (22-10-20 - commit:-----)\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from pinard import augmentation as aug\n",
    "from pinard.sklearn import SampleAugmentation\n",
    "\n",
    "x_fname = \"test_augmentation.csv\"\n",
    "x_df = pd.read_csv(x_fname, sep=\";\", header=None)\n",
    "x = x_df.astype(np.float32).values\n",
    "y = np.reshape(x[:, 0], (-1, 1))\n",
    "x = x[:, 1:]\n",
    "\n",
    "augmentations = [\n",
    "    (\"Id\", aug.IdentityAugmenter()),\n",
    "    (\"Rotate_Translate\", aug.Rotate_Translate(random_state=42, per_sample=False)),\n",
    "    # (\"Random_Y_Shift\", aug.Random_Y_Shift(random_state=42, per_sample=False)),\n",
    "    # (\"Random_Multiplicative_Shift\", aug.Random_Multiplicative_Shift(random_state=42, per_sample=False)),\n",
    "    # (\"Random_Spline_Addition\", aug.Random_Spline_Addition(random_state=42)),\n",
    "    # (\"Random_X_Spline_Deformation\", aug.Random_X_Spline_Deformation(random_state=42)),\n",
    "    # (\"Random_X_Spline_Shift\", aug.Random_X_Spline_Shift(random_state=42)),\n",
    "    # (\"Monotonous_Spline_Simplification\", aug.Monotonous_Spline_Simplification(random_state=42)),\n",
    "    # (\"Dependent_Spline_Simplification\", aug.Dependent_Spline_Simplification(random_state=42)),\n",
    "]\n",
    "\n",
    "\n",
    "# for augment in augmentations:\n",
    "#     print(augment[0])\n",
    "#     augment[1].fit_transform(x,y)\n",
    "\n",
    "aug = SampleAugmentation(augmentations)\n",
    "X_train, y_train = aug.transform(x, y)\n",
    "print(X_train[:,:10])\n",
    "\n",
    "# print(X_train.shape, y_train.shape)\n",
    "# print(np.concatenate(y_train))\n",
    "# print(X_train[2], x[1])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b09f6e5407ec4329146609a0cb08cbbe4720f97bb26598a93c421b663bd10d2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('pynirsENV': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
